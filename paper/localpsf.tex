% SIAM Article Template
\documentclass[review,onefignum,onetabnum]{siamart190516}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{localpsf_shared}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={Fast matrix-free approximation of smoothly varying blur operators, with application to Hessians in PDE-constrained inverse problems with highly informative data},
  pdfauthor={N. Alger, N. Petra, T. Hartland, and O. Ghattas}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

\externaldocument{localpsf_supplement}

% FundRef data to be entered by SIAM
%<funding-group specific-use="FundRef">
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
  We present an efficient matrix-free method for approximating locally translation invariant operators that have locally supported non-negative integral kernels. Such operators arise, for example, as Schur complements in Schur complements methods for solving partial differential equations (PDEs), Poincare-Steklov operators in domain decomposition methods, covariance operators in spatial statistics, blurring operators in imaging, and Hessians in distributed parameter PDE-constrained optimization and inverse problems. The method computes impulse responses of the operator at a collection of scattered points, then interpolates these computed impulse responses to form a product-convolution approximation of the operator. The product-convolution approximation is converted to hierarchical matrix format in order to solve linear systems and perform other matrix operations efficiently. Impulse responses are computed by applying the operator to a small number of Dirac comb ``batches'' of point sources. The key innovation of our method is a matrix-free procedure for choosing as many points as possible per batch, while ensuring that the supports of the impulse responses within each batch do not overlap. We apply the method to approximate Hessians in large-scale PDE-constrained inverse problem with highly informative data. Numerical results demonstrate that our method substantially outperforms existing state-of-the-art Hessian approximation methods which are based on low-rank approximation. Our method is able to form high quality approximations of high rank Hessians using only a small number of Hessian matrix-vector products.
\end{abstract}

% REQUIRED
\begin{keywords}
  example, \LaTeX
\end{keywords}

% REQUIRED
\begin{AMS}
  68Q25, 68R10, 68U05
\end{AMS}

\section{Introduction}
\label{sec:intro}

We present a fast matrix free method for approximating locally translation-invariant operators, $\Aop$, that have locally supported non-negative integral kernels. Such operators arise throughout computational science. Examples include Schur complements in Schur complements methods for solving partial differential equations (PDEs) \cite{TOBYPAPER}, Poincare-Steklov operators in domain decomposition methods (e.g., Dirichlet-to-Neumann maps), covariance operators in spatial statistics, blurring operators in imaging, and Hessians in distributed parameter PDE-constrained optimization and inverse problems.

Let $\Omega \subset \mathbb{R}^d$ be a bounded domain in dimension $d=1$, $2$, or $3$. We consider integral operators $\Aop:L^2(\Omega)\rightarrow L^2(\Omega)'$ of the form
\begin{equation}
\label{eq:kernel_representation}
(\Aop u)(v) := \int_\Omega \int_\Omega v(y) \Aker(y,x) u(x) dx dy,
\end{equation}
where $\Aker:\Omega \times \Omega \rightarrow \mathbb{R}$ is the integral kernel. Here $\Aop u \in L^2(\Omega)'$ is the linear functional that results from applying $\Aop$ to $u\in L^2(\Omega)$, and $\left(\Aop u\right)(v)$ is the scalar that results from applying that linear functional to $v \in L^2(\Omega)$. We further narrow our focus to operators which possess the following four properties:
\begin{description}
\item[1) Matrix-free:] One cannot easily evaluate kernel entries $\Phi(y,x)$. Instead, access to $\Aop$ is available through ``operator actions.'' That is, we have a black-box computational procedure (which is typically expensive) through which we may evaluate the maps
\begin{equation*}
	u \mapsto\Aop u \quad \text{and} \quad v \mapsto\Aop^Tv
\end{equation*} 
for arbitrary functions $u$ and $v$. After discretization (see Appendix \ref{app:discretized_operations}), $\Aop$ becomes a dense matrix, $\mathbf{A}$, that is typically too large to be built and stored. This property is called ``matrix-free'' because at the discrete level it means that one can perform matrix-vector products of $\mathbf{A}$ and $\mathbf{A}^T$ with arbitrary vectors, but one cannot easily\footnote{One could compute $\mathbf{v} = \mathbf{A}\mathbf{e}_j$, where $\mathbf{e}_j=(0,\dots,0,1,0,\dots,0)$ is the unit vector which has $j^\text{th}$ component equal to one and all other components equal to zero, then extract $\mathbf{A}_{ij} = \mathbf{v}_i$. However, this process is wasteful because one computes the entire vector $\mathbf{v}$, then discards all but one component.} access matrix entries $\mathbf{A}_{ij}$. In applications, applying $\mathbf{A}$ to a vector may involve iteratively solving a large linear system, timestepping, or performing some other nontrivial computational procedure. For example, in reduced space approaches to PDE-constrained optimization and inverse problems, Hessian information is available only through the application of the Hessian to vectors, and one such Hessian application requires solving two PDEs \cite{HESSIANADJOINT}. 
\item[2) Local support:] Let $\phi_x$ be the following \emph{impulse response}:
\begin{equation}
\label{eq:impulse_response_defn}
\phi_x(y) := \Phi(y, x).
\end{equation} 
We say that $\Aop$ has local support if, for all $x\in \Omega$, the support of $\phi_x$ is contained (or approximately contained) in a neighborhood of $x$. The smaller these supports are, the better our algorithm will perform.  Let $\delta_x$ denote the point source (delta distribution) centered at $x$. The function $\phi_x$ is called the impulse response because straightforward analysis shows that it may be expressed as follows:
\begin{equation}
\label{eq:impulse_response_delta_action}
\phi_x = \left\langle \Aop, \delta_x \right\rangle^*,
\end{equation}
where $\left\langle \Aop, \delta_x \right\rangle \in L^2(\Omega)'$ denotes the result of applying the operator $\Aop$ to the distribution $\delta_x$, and $\left\langle \Aop, \delta_x \right\rangle^* \in L^2(\Omega)$ denotes the Riesz representation of $\langle\Aop, \delta_x\rangle$ with respect to the $L^2$ inner product.\footnote{Recall that the Riesz representative of a functional $\psi \in L^2(\Omega)'$ with respect to the $L^2$ inner product is the unique function $\psi^* \in L^2(\Omega)$ such that $\psi(v) = \left(\psi^*,v\right)_{L^2(\Omega)}$ for all $v \in L^2(\Omega)$.} Note that while the domain of $\Aop$ is defined as $L^2(\Omega)$, which does not contain $\delta_x$, the action of $\Aop$ may be extended to distributions in a natural way, as described in Appendix \ref{app:distributions}. 
%The action of $\mathcal{A}$ on a distribution $\mu$ is written as $\langle \mathcal{A}, \mu \rangle$. We use this angle bracket notation to distinguish the action of $\mathcal{A}$ on distributions from the action of $\mathcal{A}$ on functions.
%We take the Riesz representative of the linear functional $\left\langle \mathcal{A}, \delta_x \right\rangle \in L^2(\Omega)'$ to convert it into a function $\left\langle \mathcal{A}, \delta_x \right\rangle^* \in L^2(\Omega)$. 
%After discretization, the process of applying $\Aop$ to the distribution $\delta_x$, then forming the Riesz representative of the resulting linear functional, amounts to computing $\boldsymbol{\phi}_x = \mathbf{M}^{-1}\mathbf{A} \mathbf{M}^{-1} \boldsymbol{\delta}_x$, where $\boldsymbol{\phi}_x$, $\mathbf{A}$, and $\boldsymbol{\delta}_x$ are discretized versions of $\phi_x$, $A$ and $\delta_x$, respectively, and $\mathbf{M}$ is a discretization of the $L^2$ inner product bilinear form (e.g., a mass matrix in the finite element context). 
From \eqref{eq:impulse_response_delta_action}, we see that this local support property means that the response of $\Aop$ to a point source at $x$ is zero (or small) at points $y$ that are far from $x$.
\item[3) Local translation invariance:] We say that $\Aop$ is locally translation invariant if
\begin{equation}
	\label{eq:local_translation_invariance}
	\Phi(y+h, x+h) \approx \Phi(y,x)
\end{equation}
when $h$ is not too large. One can imagine the kernel entry $\Phi(y,x)$ as representing the strength of an interaction between a source at point $x$ and a target at point $y$. Local translation invariance means that the interaction strength remains roughly the same if the source and target points are both translated by the same vector. This is illustrated in Figure FIG.
\item[4) Non-negative kernel:] For all $(y,x) \in \Omega \times \Omega$, we have
\begin{equation*}
\Phi(y,x) \ge 0.
\end{equation*}
\end{description}

Our overall strategy is to form a product-convolution approximation, $\AopPc$, of $\Aop$ using only operator actions, then convert $\AopPc$ into a hierarchical matrix (H-matrix), which we denote by $\AmatPc$. Schematically,
\begin{equation*}
\Aop \longrightarrow \underbrace{\AopPc}_{\substack{\text{product}\\ \text{convolution}}} \longrightarrow \underbrace{\AmatPc}_{\text{H-matrix}}.
\end{equation*}
We will define product-convolution approximations in Section \ref{sec:prodconv_intro}. H-matrices are discussed in Appendix \ref{app:h_matrix}. Our method is summarized in Section \ref{sec:overview_intro}, and described in detail in Section \ref{sec:method}. 

The novel contributions of our method are in the way that we form the product-convolution approximation ($\Aop \rightarrow \AopPc$). This is the part of the method where we use the four properties of $\Aop$ described above. Fast conversion of $\AopPc$ to H-matrix format ($\AopPc \rightarrow \AmatPc$) is possible because integral kernel entries for $\AopPc$ are easily accessible (unlike integral kernel entries for $\Aop$). Once in H-matrix format, one may use H-matrix methods to perform further useful linear algebra operations involving $\AmatPc$, including matrix-matrix addition, matrix-matrix multiplication, matrix-vector products, matrix factorization, and matrix inversion. These H-matrix methods are fast and scale well to large problems.

\subsection{Motivation: inverse problems with highly informative data}
\label{sec:PDE_hessian_motivation}

Our motivation for this work is approximation of Hessians in distributed parameter inverse problems governed by partial differential equations (PDEs). That is, inverse problems in which one seeks to reconstruct an unknown parameter field, $m$, from noisy observations, 
\begin{equation*}
y=f(m,u(m)) + \text{noise},
\end{equation*}
which depend on  a state variable $u$. In turn, $u$ depends on $m$ implicitly through the solution of a PDE, which we write generically as follows:
\begin{equation}
\label{eq:state_pde}
0=g(m,u).
\end{equation}
Here $u(m)$ denotes the solution of the PDE \eqref{eq:state_pde} as a function of $m$. In the deterministic approach to inverse problems, one typically finds $m$ as the solution to the minimization problem 
\begin{equation}
\label{eq:minimization_problem}
\min_m \quad \frac{1}{2}\|y - f(m,u(m))\|_W^2 + R(m),
\end{equation}
where $\|\cdot\|_W$ is weighted norm which depends on the noise covariance, and $R(m)$ is a regularization term. The objective function in optimization problem \eqref{eq:minimization_problem} is defined indirectly via the implicit function theorem. As a result, the Hessian of the objective function, $\mathcal{H}$, is only accessible matrix-free. In particular, applying $\mathcal{H}$ to a vector requires solving two PDEs \cite{HESSIANACTION}. More accessible approximations of $\mathcal{H}$ are highly desirable, because they allow for fast solution of \eqref{eq:minimization_problem} via Newton-type methods. More accessible Hessian approximations are also central to many methods for uncertainty quantification in Bayesian statistical approaches to the inverse problem, because $\mathcal{H}^{-1}$ locally approximates the Bayesian posterior covariance for $m$. 
%We only have access to $\mathcal{H}$ via its action on an arbitrary vectors $v$, i.e., evaluation of the map $v \mapsto \mathcal{H} v$. Evaluating $v \mapsto \mathcal{H} v$ is an expensive process that involves solving two auxiliary linear PDEs.

The most popular existing Hessian approximation methods are based on forming a low rank approximation of the data misfit term in the Hessian, or the data misfit term preconditioned by the prior term \cite{CCGOPAPERS}. Conventionally, either the Lanczos method or the randomized singular value decomposition \cite{HMTRANDOM} are used to perform the low rank approximation using only matrix-vector products. These methods suffer from a ``data predicament''---if the data are highly informative about the unknown parameter, then the numerical rank of the data misfit term in the Hessian is large, so a large number of matrix-vector products are required to form the aforementioned low rank approximation. The ideal scenario from a scientific perspective (highly informative data) is therefore the worst case scenario from a computational perspective (large computational cost) \cite{MYDISSERTATION}. Although the method we present in this paper is not applicable to all Hessians, it is applicable to several Hessians of practical interest. For these Hessians, our method offers a \emph{data-scalable} alternative to conventional low-rank Hessian approximation methods, because our method can form high-rank approximations of an operator using a small number of matrix-vector products.

\subsection{Product-convolution approximation}
\label{sec:prodconv_intro}

If $\Aop$ were perfectly translation invariant (i.e., if equality held in \eqref{eq:local_translation_invariance} for all $x$, $y$, $h$) then it is straightforward to show that $\Aop$ would be the convolution operator $u \mapsto \left(\varphi_x \ast u\right)^*$, where the convolution kernel is given by
\begin{equation}
\label{eq:convolution_kernel}
	\varphi_x(y) := \phi_x(y+x).
\end{equation}
The convolution kernel $\varphi_x$ is the result of translating the impulse response $\phi_x$ to re-center it at zero instead of $x$. Locally translation invariant operators act like convolution operators locally, but the convolution kernel, $\varphi_x$, varies as $x$ changes. We therefore approximate $\Aop$ by a spatially varying weighted sum of convolution operators, $\AopPc \approx \Aop$, of the form
\begin{equation}
\label{eq:product_convolution}
	\left(\AopPc~u\right)(v) := \left( v,~ \sum_{i=1}^r \varphi_i \ast \left(w_i \cdot u\right) \right)_{L^2(\Omega)},
\end{equation}
where $f \cdot g$ denotes pointwise multiplication, $f \ast g$ denotes convolution, and $\left(f, g\right)_{L^2(\Omega)}$ denotes the $L^2$ inner product on $\Omega$, for functions $f,g \in L^2(\Omega)$. Approximations of the form \eqref{eq:product_convolution} are known as \emph{product-convolution} approximations, because the action of each term in the sum consists of a pointwise product, followed by a convolution. To avoid nested subscripts, we write 
\begin{equation*}
\varphi_i := \varphi_{x_i} \quad \text{and} \quad \phi_i := \phi_{x_i},
\end{equation*}
with $\varphi_{x_i}$ defined in \eqref{eq:convolution_kernel} and $\phi_{x_i}$ defined in \eqref{eq:impulse_response_defn}, to denote local convolution kernels and impulse responses corresponding to a collection of points $\{x_i\}_{i=1}^r \subset \Omega$ scattered throughout the domain. The functions $w_i$ are spatially varying weighting functions that are used to interpolate the convolution kernels $\varphi_i$. Our operator approximation method is defined by how we compute the convolution kernels, $\varphi_i$, how we choose the points, $x_i$, and what weighting functions, $w_i$, that we use. In Section \ref{sec:overview_intro} we summarize the answers to these questions; we will provide detailed answers to these questions in Section \ref{sec:method}.

The more locally translation invariant an operator is (i.e., the smaller the discrepancy between the left hand side and right hand side in \eqref{eq:local_translation_invariance}), the smaller the number of terms that are required in \eqref{eq:product_convolution} to achieve an accurate product-convolution approximation. 

\subsection{Overview of the method}
\label{sec:overview_intro}

We compute one ``batch'' of $\varphi_i$'s by applying $\Aop$ to a sum of point sources (Dirac comb) associated with a collection of points $x_i$ scattered throughout $\Omega$. The batch of points $x_i$ are chosen so that the support of $\phi_i$ and the support of $\phi_j$ do not overlap (or do not overlap much) if $i \neq j$. Because these supports do not overlap, we can post-process the response of $\Aop$ to the Dirac comb to recover the functions $\phi_i$, and therefore the functions $\varphi_i$, associated with all points $x_i$ in the batch---with one application of $\Aop$, we recover many $\varphi_i$. This is illustrated in Figure FIG. The process is repeated to get more batches of $\varphi_i$'s, until a desired number of batches is reached.

%First, we compute
%\begin{equation}
%\label{eq:eta_intro}
%\eta := \left\langle \mathcal{A}, \xi \right\rangle^*
%\end{equation}
%where 
%\begin{equation*}
%	\xi := \sum_{i=1}^l \delta_{x_i}
%\end{equation*}
%is the Dirac comb associated with point sources centered at a collection of scattered points $x_i$. The batch of points $x_i$ are chosen so that the support of $\phi_i$ and the support of $\phi_j$ do not overlap (or do not overlap much) if $i \neq j$. By linearity, and the expression for $\phi_x$ in \eqref{eq:impulse_response_delta_action}, we have
%\begin{equation*}
%\eta = \sum_{i=1}^l \phi_i.
%\end{equation*}
%We may therefore post-process $\eta$ 
%
%to recover $\phi_i$ for all of the points $x_i$ in the batch. Since the functions $\varphi_i$ are translated versions of the functions $\phi_i$, we also recover $\varphi_i$ for all points $x_i$ in the batch. This is illustrated in Figure FIG. The process is repeated to get more batches of $\varphi_i$'s, until a desired number of batches is reached. 

In order to choose the points $x_i$, we need to estimate the support of $\phi_i$ \emph{before} we compute it. The supports of the functions $\phi_x$ are estimated a-priori, for all $x \in \Omega$ simultaneously, via a procedure that involves applying $\Aop^T$ to a small number of specially chosen functions (three functions if $d=1$, six if $d=2$, ten if $d=3$), then post processing the results (Section \ref{eq:mean_and_covariance_estimation}). This procedure is the only part of the paper that requires non-negativity of the integral kernel. 

We choose the weighting functions, $w_i$, to be the solutions to the following optimization problems:
\begin{equation}
\label{eq:wi_optimization_problem_intro}
\begin{aligned}
\min_{w_i \in H^2_N(\Omega)} &\quad \frac{1}{2}\norm{\Delta w_i}_{L^2(\Omega)}^2 \\
\text{such that} &\quad w_i(x_j) = \delta_{ij}, \quad j=1,\dots,q.
\end{aligned}
\end{equation}
Here $\Delta$ is the Laplacian on $\Omega$, $H^2_N(\Omega)$ is the space of all functions in the Sobolev space $H^2(\Omega)$ with Neumann zero boundary conditions, and the right hand side of the constraint, $\delta_{ij}$, is the Kronecker delta, which takes value one if $i=j$ and zero if $i \neq j$. Since the Laplacian measures local deviation from flatness \cite{JACOBSEN}, the function $w_i$ is the flattest function, in a least-squares sense, that takes the value one at the sample point $x_i$, and the value zero at the other sample points $x_j$, $i \neq j$. Computing the weighting functions $w_i$ requires solving two Poisson PDEs per weighting function, and these PDEs are solved cheaply using multigrid. We call this interpolation scheme \emph{Poisson interpolation}. The advantage of Poisson interpolation over other interpolation methods, most notably radial basis function interpolation \cite{ESCANDE}, is that Poisson interpolation incorporates information about the domain geometry when constructing the weighting functions. This is illustrated in Figure FIG. 

Once the $\varphi_i$ and $w_i$ are computed, we convert $\AopPc$ to hierarchical matrix (H-matrix) format. Let $\Phi_\text{pc}$ denote the integral kernel associated with $\AopPc$. While $\Phi(y,x)$ is not easily computable, $\Phi_\text{pc}(y,x)$ is given by the formula
\begin{equation}
\label{eq:kernel_entries}
	\Phi_\text{pc}(y,x) = \sum_{i=1}^r \varphi_i(y-x) w_i(x).
\end{equation}
This follows from~\eqref{eq:product_convolution} and the fact that the convolution operator $u \mapsto \varphi_i \ast u$ has $(y,x)$ kernel entry given by $\varphi(y-x)$.
Formula~\eqref{eq:kernel_entries} allows us to construct a H-matrix representation of $\AopPc$ using the conventional adaptive cross H-matrix construction method, in which one forms low-rank approximations of blocks of the matrix by sampling rows and columns of those blocks \cite{HACA}. Once in H-matrix format, fast H-matrix arithmetic is used to invert $\AopPc$, or perform other useful matrix operations.

Often we are interested in approximating operators $\Aop$ that are symmetric positive semi-definite. Unfortunately, $\AopPc$ is, in general, non-symmetric and indefinite due to errors in the product convolution approximation. In this case, we modify the H-matrix representation of $\AopPc$ to make it positive definite, by symmetrizing it, then applying a specially chosen rational matrix function to the symmetrized H-matrix.



\subsection{Existing work}

In general, product-convolution approximations take the form shown in \eqref{eq:product_convolution}, but $\varphi_i$ and $w_i$ may be arbitrary functions. Product-convolution approximations have been used in a wide variety of fields going back several decades \cite{PRODCONVLIST}.
For background on product-convolution approximations, we recommend reading the following papers: \cite{PRODCONVGOOD}. 

We build upon the class of product convolution approximations in which the functions, $\varphi_i$, are impulse responses of $\Aop$ to point sources at a collection of points $x_i$ \cite{PRODCONVLIST}. A popular choice for methods in this class is to choose the points $x_i$ to be nodes in a regular grid, and interpolate the functions $\varphi_i$ with piecewise linear interpolation \cite{NAGY}, or splines \cite{PRODCONVSPLINES}. With a regular grid, a large number of points $x_i$ is typically required to achieve an accurate product-convolution approximation.
%This is computationally prohibitive in our applications, because computing the functions $\varphi_i$ requires one matrix vector product with $\Aop$ for each point $x_i$, and these matrix vector products are computationally costly. 
In our previous work, we reduced the computational cost by starting with a coarse grid of points $x_i$, then adaptively refining the grid in the regions where the error in the approximation is large \cite{PRODCONVMYPAPER}. But even with adaptive refinement, many matrix vector products with $\Aop$ may be required. In this paper, rather than using adaptive refinement, we instead reduce the computational cost by picking points $x_i$ such that many $\varphi_i$ are computed with each matrix vector product. 

%We do this by taking advantage of the fact that the functions $\varphi_i$ are locally supported; existing methods typically do not require this local support property.

Our method of estimating the support of the functions $\varphi_i$ was inspired by resolution analysis in seismic imaging \cite{RESOLUTION}. In resolution analysis, $\Aop$ is the Hessian for a seismic inverse problem, and the width of $\varphi_p$ is used to estimate of the minimum length scale on which features of the parameter can be inferred near the point $p$. If the width of $\varphi_p$ is ten meters, then features of the parameter near $p$ can be accurately inferred from data if those features are larger than roughly ten meters in size. In \cite{RESOLUTION}, the width of $\varphi_p$ is estimated to be the local autocorrelation length of the function $\Aop^T \zeta$ near $p$, where $\zeta$ is a random noise funtion. In this paper, rather than probing $\Aop^T$ with random noise functions, we use specific constant, linear, and quadratic functions. Our method estimates the support of $\varphi_p$ more accurately and reliably than resolution analysis, but our method requires that $\Aop$ has a positive integral kernel, while the resolution analysis method does not have this requirement.

Matrix-vector products with product-convolution approximations can be performed using the fast Fourier transform if the problem is discretized on a regular grid \cite{PRODCONVFFT}. The product-convolution approximation can then replace the original operator when using Krylov methods to solve linear systems with the original operator as the coefficient operator \cite{PRODCONVKRYLOV}. However, for complex geometries, problems are rarely discretized using regular grids, so we cannot easily use the fast Fourier transform. Furthermore, in big-data inverse problems the required number of Krylov iterations is large \cite{MYDISSERTATION,OTHERS}. Thus, after constructing the product-convolution approximation, it is desirable to convert the product-convolution approximation to other matrix formats that are amenable to fast linear algebra operations. Wavelet compression methods have been used for this purpose \cite{PRODCONVWAVELETS}. Here we follow our previous work \cite{PRODCONVMYPAPER}, in which we convert the product-convolution approximation to H-matrix format. 

Conventional H-matrix construction methods require access to matrix entries of the matrix being approximated, and therefore cannot be used to efficiently form H-matrix approximations of operators that are only available through matrix-vector products. There are matrix-free methods for H-matrix approximation \cite{LEXINGPEELINGPROCESS}, and these methods have been used to form H-matrix representations of Hessians in PDE constrained inverse problems \cite{ILONAHMATRIX,NOEMIHMATRIX}. While these matrix-free H-matrix construction methods are asymptotically scalable in theory, the required number of matrix vector products can be large in practice. 


Product-convolution type approximations of Hessians have been used in a seismic inverse problem in \cite{GEORGSEISMICPRODCONV}, and in an advection-diffusion inverse problem in \cite{PRODCONVMYPAPER}. In \cite{DEMANETSEISMIC}, matrix probing \cite{MATRIXPROBING} is used to approximate the Hessian in a seismic inverse problem as the sum of simple pseudodifferential operators. Although it is not explicitly mentioned, the approximation in \cite{DEMANETSEISMIC} could be interpreted as a convolution-product interpolation, which is like a product-convolution approximation, except the order of pointwise multiplications and convolutions is reversed in \eqref{eq:product_convolution}.


\section{Constructing the product-convolution H-matrix approximation}
\label{sec:method}
In this section we describe how we choose the sample points, $x_i$ (Section \ref{sec:sample_point_selection}), how we form the convolution kernels, $\varphi_i$ (Section \ref{sec:get_impulse_response}), and how we form the weighting functions, $w_i$ (Section \ref{sec:weighting_functions}), for the product-convolution approximation, $\AopPc \approx \Aop$, given in \eqref{eq:product_convolution}. We also describe how we convert $\AopPc$ to H-matrix format (Section \ref{sec:H_matrix_conversion}), and (optionally) how we modify the resulting H-matrix, $\AmatPc$, to force it to be symmetric positive semi-definite (Section \ref{sec:make_spd}). The complete algorithm for constructing $\AmatPc$ is shown in Algorithm \ref{alg:construct_Atilde}. 
%The method for evaluating kernel entries of $\Phi_\text{pc}$ is shown in Section \ref{sec:eval_matrix_entries}.


%\begin{itemize}
%\item We extract $\varphi_{x_i}$ for many points $x_i$ by applying $\mathcal{A}$ to Dirac combs associated with ``batches'' of points (Section \ref{sec:get_impulse_response}). To reduce memory usage, all kernels $\varphi_i$ associated with a given batch, $b$, are stored in a single function, $\eta^b$, rather than as distinct functions.
%\item We use a greedy algorithm to choose as many points as possible per batch, while ensuring that the points in each batch are not too close to each other, and are not too close to $\partial \Omega$ (Section \ref{sec:greedy_point_selection}). 
%\item We ensure that the points $x_i$ are well separated from each other by forming a-priori estimates of the supports of the functions $\psi_x$ (Section \ref{eq:mean_and_covariance_estimation}). 
%\item For the weighting functions, we use the smoothest functions (in a least-squares sense) that interpolate the points $x_i$; constructing the weighting functions requires solving two Poisson PDEs per point $x_i$ (Section \ref{sec:weighting_functions}).
%\end{itemize}
%The complete algorithm for constructing $\AopPc$ is shown in Algorithm \ref{alg:construct_Atilde}. The method for evaluating kernel entries of $\Phi_\text{pc}$ is shown in Section \ref{sec:eval_matrix_entries}. The method for converting $\AopPc$ to H-matrix format is shown in Section \ref{sec:H_matrix_conversion}. The method for modifying the resulting H-matrix to make it symmetric positive semi-definite is shown in Section \ref{sec:make_spd}.

\begin{algorithm2e}
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	{	
		\Input{Linear operator $\Aop$, parameter $\text{max\_batches}$}
		\Output{H-matrix $\AmatPc$ (optionally, $\AmatPcSymPlus$)}
		
		Compute $\alpha$, $\mu$, and $\Sigma$ using Algorithm \ref{alg:varhpi_mean_cov}.
		
		\For{$b=1,2,\dots,\text{max\_batches}$}{
			Choose a batch of sample points, $S_b$, using Algorithm \ref{alg:point_choice}
			
			Compute $\eta^b$ by applying $\Aop$ to the dirac comb for $S_b$ (Section \ref{sec:get_impulse_response})
			
		}

		Form convolution kernels, $\varphi_{x_i}$, on regular grids (Section \ref{sec:convolution_kernel_regular_grid})
	
		Modify convolution kernels to address boundary artifacts (Section \ref{sec:boundary_extension})
	
		Compute all weighting functions $w_i$ using Algorithm \ref{alg:weighting_functions_incremental}
%		In lines 2 and 3 of Algorithm \ref{alg:weighting_functions_incremental}, only solve for $\psi_i$ and $\theta_i$ associated with new points $x_i$ in $S_b$. The functions $\psi_i$ and $\theta_i$ associated with points in previous batches have already been computed in previous iterations of this for loop.

		Form H-matrix $\AmatPc$ (Section \ref{sec:H_matrix_conversion})
		
		(optional) Modify $\AmatPc$ to make $\AmatPcSymPlus$ (Section \ref{sec:make_spd})
		
	}
	\caption{Construct product-convolution H-matrix}
	\label{alg:construct_Atilde}
\end{algorithm2e}


\subsection{Sample point selection}
\label{sec:sample_point_selection}

We choose sample points, $x_i$, in batches. A greedy algorithm is used to choose as many points as possible per batch, while ensuring that the points in each batch are not too close to each other. We ensure that the points $x_i$ are well separated from each other by forming a-priori estimates of the supports of the functions $\phi_x$ (Section \ref{eq:mean_and_covariance_estimation}). The support of $\phi_x$ is estimated to be contained within an ellipsoid, so the process of choosing batches of points is an ellipsoid packing problem (Section \ref{sec:greedy_point_selection}). 
%In the next section (Section \ref{sec:get_impulse_response}), we will extract $\varphi_{x_i}$ for all the points $x_i$ in a given batch by applying $\Aop$ to the Dirac comb associated with that batch.

\subsubsection{Estimating impulse response supports}
\label{eq:mean_and_covariance_estimation}

Because of the non-negative kernel property, for each $x \in \Omega$, the impulse response $\phi_x$ is a non-negative function $y \mapsto \phi_x(y)$. Hence, $\phi_x$ is a scaled probability distribution. Let $\alpha$ denote the scaling factor, let $\widehat{\phi}_x := \phi_x / \alpha(x)$ denote the normalized version of $\phi_x$, and let $\mu(x)$ and $\Sigma(x)$ denote the mean and covariance of $\widehat{\phi}_x$, respectively. We make the approximation that the support of $\phi_x$ is contained within the ellipsoid
\begin{equation}
\label{eq:support_ellipsoid}
E_x := \{x' \in \Omega: (x' - \mu(x))^T \Sigma(x)^{-1} (x' - \mu(x)) \le \tau^2\},
\end{equation}
where $\tau$ is a fixed constant. 
%The ellipsoid $E_{x}$ is the set of points within $\tau$ standard deviations of the mean of the Gaussian approximation of the normalized version of $\langle \mathcal{A}, \delta_{x}\rangle^{*}$ which has the same mean and covariance. 
The ellipsoid $E_x$ is the set of points within $\tau$ standard deviations from the mean of the Gaussian distribution with mean $\mu(x)$ and covariance $\Sigma(x)$, i.e., the Gaussian distribution which has the same mean and covariance as $\widehat{\phi}_x$ (see Figure FIG). The quantity $\tau$ is a parameter that must be chosen by the user of the algorithm. The larger $\tau$ is, the larger the ellipsoid $E_x$ is, and the more conservative the estimate is for the support of $\phi_x$. However, the cost of our algorithm will depend on how many non-overlapping ellipsoids $E_x$ we can ``pack'' in the domain $\Omega$ (more ellipsoids $\implies$ lower cost), and choosing a larger value of $\tau$ means that fewer ellipsoids will fit in $\Omega$. We find that $\tau=3$ yields a reasonable balance between these competing interests, and use $\tau=3$ in our numerical results. The fraction of the ``mass'' of $\phi_x$ residing outside of $E_x$ is less than $1/\tau^2$ by Chebyshev's inequality, though this bound is conservative and typically far less mass resides in this region. 
% a good balance between these competing goals. 
%The bigger $\tau$ is, the smaller the overlap between impulse responses within a batch will be, while the smaller $\tau$ is, the more impulse responses we can fit within each batch.

The ellipsoid $E_x$ is not immediately available. To use $E_x$, we must first compute the scalar $\alpha(x)$, the length-$d$ vector $\mu(x)$, and the $d \times d$ matrix $\Sigma(x)$ (recall $d \in \{1,2,3\}$ is the spatial dimension of $\Omega$). The direct approach to compute $\alpha(x)$, $\mu(x)$, and $\Sigma(x)$ is to apply $\Aop$ to a point source centered at $x$ to get $\phi_x$, as per \eqref{eq:impulse_response_delta_action}. Then one can post-process $\phi_x$ to determine $\alpha(x)$, $\mu(x)$, and $\Sigma(x)$. But this direct approach is computationally infeasible because and it requires one operator action of $\Aop$ per point $x$ that we wish to evaluate $E_x$ at, and we will need to evaluate $E_x$ at a large number of points. Fortunately, it is possible to compute $\alpha(x)$, $\mu(x)$, and $\Sigma(x)$, \emph{for all points $x$ simultaneously}, by applying $\Aop^T$ to one constant function, $d$ linear functions, and $d(d+1)/2$ quadratic functions. We present this efficient method of computing $\alpha$, $\mu$, and $\Sigma$ in Theorem \ref{thm:vol_mean_cov} and summarize the method in Algorithm \ref{alg:varhpi_mean_cov}.

%In Algorithm \ref{alg:varhpi_mean_cov} we show how to compute  Definitions required for Algorithm \ref{alg:varhpi_mean_cov} are given in Definition \ref{defn:alpha_rho_mu_sigma}, Definition \ref{defn:C_L_Q}, and Definition \ref{defn:pointwise}. The theoretical basis for Algorithm \ref{alg:varhpi_mean_cov} is shown in Theorem \ref{thm:vol_mean_cov}.

%We compute the scaling factor, denoted $\alpha(x)$, for all $x$ simultaneously by applying $\mathcal{A}^T$ to a constant function. We compute the spatially varying mean, $\mu(x)$, and covariance, $\Sigma(x)$ of the normalized version of $\phi_x$, for all $x$ simultaneously, by applying $\mathcal{A}^T$ to a small number of linear and quadratic functions. Definitions of $\alpha$, $\mu$, and $\Sigma$ are given in Definition \ref{defn:alpha_rho_mu_sigma}. The procedure for computing these quantities is shown in Algorithm \ref{alg:varhpi_mean_cov}, and the theoretical basis for this procedure is shown in Theorem \ref{thm:vol_mean_cov}. 


\begin{theorem}[Impulse response moments]
	\label{thm:vol_mean_cov}
	Let $\alpha:\Omega \rightarrow \mathbb{R}$, $\mu:\Omega \rightarrow \mathbb{R}^d$, and $\Sigma:\Omega \rightarrow \mathbb{R}^{d \times d}$, be the following functions:
	\begin{equation*}
	\alpha(x) := \int_{\Omega} \phi_x(y) dy, \qquad
	\mu(x) := \mathbb{E}(\widehat{\phi}_x), \qquad
	\Sigma(x) := \Var(\widehat{\phi}_x),
	\end{equation*}
	where $\mathbb{E}(\widehat{\phi}_x)$ denotes the mean of $\widehat{\phi}_x$, and $\Var(\widehat{\phi}_x)$ denotes the variance of $\widehat{\phi}_x$.	Let  $x^i$ denote the $i^\text{th}$ component of $x$, i.e., $x = \left(x^1, x^2, \dots, x^d\right)$. Let $C$, $\{L^i\}_{i=1}^d$, and ${\{Q^{ij}\}_{i=1}^d}_{j=1}^d$ be the following constant, linear, and quadratic functions:
	\begin{equation*}
	C(x) := 1, \qquad
	L^i(x) := x^i, \qquad
	Q^{ij}(x) = x^i x^j.
	\end{equation*}
	Let $f/g$ to denote the pointwise division of functions, $\left(f/g\right)(x) = f(x)/g(x)$, and $f\cdot g$ to denote the pointwise multiplication of functions, $(f\cdot g)(x) = f(x)g(x)$. The following results hold:
	\begin{subequations}
		\label{eq:vol_mean_var}
	\begin{align}
	\alpha =& \left(\Aop^T C\right)^* \\
	\mu^i =& \left(\Aop^T L^i\right)^* / \alpha \\
	\Sigma^{ij} =& \left(\Aop^T Q^{ij}\right)^* / \alpha - \mu^i\cdot \mu^j
	\end{align}
	\end{subequations}
	 for $i=1,\dots, d$, $j=1,\dots,d$.
\end{theorem}

The proof of Theorem \ref{thm:vol_mean_cov} is shown in Appendix \ref{app:proofs}. The proof is straightforward, and follows directly from the definitions.

%
%\begin{defn}[Constant, linear, and quadratic functions]	
%	\label{defn:C_L_Q}
%	Let  $x^i$ denote the $i^\text{th}$ component of $x$, i.e., $x = \left(x^1, x^2, \dots, x^d\right)$. We define $C$, $\{L^i\}_{i=1}^d$, and ${\{Q^{ij}\}_{i=1}^d}_{j=1}^d$ be the following constant, linear, and quadratic functions, respectively:
%	\begin{equation*}
%	C(x) := 1, \qquad
%	L^i(x) := x^i, \qquad
%	Q^{ij}(x) = x^i x^j.
%	\end{equation*}
%\end{defn}
%
%\begin{defn}[Pointwise operations]
%	\label{defn:pointwise}
%	We write $f/g$ to denote the pointwise division of functions, $\left(f/g\right)(x) = f(x)/g(x)$, and $f\cdot g$ to denote the pointwise multiplication of functions, $(f\cdot g)(x) = f(x)g(x)$.
%\end{defn}

%\begin{theorem}
%	\label{thm:vol_mean_cov}
%	Let  $x^i$ denote the $i^\text{th}$ component of $x$, i.e., $x = \left(x^1, x^2, \dots, x^d\right)$, and let $C$, $\{L^i\}_{i=1}^d$, and ${\{Q^{ij}\}_{i=1}^d}_{j=1}^d$ be the following constant, linear, and quadratic functions, respectively:
%	\begin{equation*}
%		C(x) := 1, \qquad
%		L^i(x) := x^i, \qquad
%		Q^{ij}(x) = x^i x^j.
%	\end{equation*}
%	We have
%	\begin{align}
%		\alpha =& \left(\mathcal{A}^TC\right)^*, \label{eq:vol_mean_var_thm1}\\
%		\mu^i =& \left(\mathcal{A}^T L^i\right)^* / \alpha, \label{eq:vol_mean_var_thm2}\\
%		\Sigma^{ij} =& \left(\mathcal{A}^T Q^{ij}\right)^* / \alpha - \mu^i\cdot\mu^j, \label{eq:vol_mean_var_thm3}
%	\end{align}
%	We write $f/g$ to denote the pointwise division of functions, $\left(f/g\right)(x) = f(x)/g(x)$, and $f\cdot g$ to denote the pointwise multiplication of functions, $(f\cdot g)(x) = f(x)g(x)$.
%\end{theorem}
%
%The proof of Theorem \ref{thm:vol_mean_cov} is shown in Appendix \ref{app:proofs}.


\begin{algorithm2e}
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	{	
		\Input{Operator $\mathcal{A}$}
		\Output{$\alpha$, $\mu$, and $\Sigma$}
		
		\tcp{Compute scaling factor $\alpha$}
		
		Form constant function $C(x)=1$

		Compute $\alpha = \left(\mathcal{A}^T C\right)^*$
		
		\tcp{Compute mean $\mu$}
		\For{$i=1,2,\dots,d$}{
			Form linear function $L^i(x) = x^i$
			
			Compute $\mu^i = \left(\mathcal{A}^T L^i\right)^* / \alpha$
		}
		\tcp{Compute covariance $\Sigma$}
		\For{$i=1,2,\dots,d$}{
			\For{$j=1,\dots,i$}{
				Form quadratic function $Q^{ij}(x) = x^i x^j$
				
				Compute $\Sigma^{ij} = \left(\mathcal{A}^T Q^{ij}\right)^* / \alpha - \mu^i\cdot \mu^j$
				
				Set $\Sigma^{ji} = \Sigma^{ij}$
			
			}
		}
		
	}
	\caption{Compute scaling factor $\alpha$, mean $\mu$, and covariance $\Sigma$}
	\label{alg:varhpi_mean_cov}
\end{algorithm2e}






\subsubsection{Greedy ellipsoid packing}
\label{sec:greedy_point_selection}

We use a greedy ellipsoid packing algorithm to choose sets of sample points, $S_b$, such that there is no overlap between the ellipsoids, $E_{x_i}$, associated with the sample points, $x_i$, within a batch. These sample point batches will be used in Section \ref{sec:get_impulse_response} to compute many impulse responses of $\Aop$ using only a small number of operator actions of $\Aop$. The support of $\phi_x$ is (approximately) contained in the ellipsoid $E_x$, so by applying $\mathcal{A}$ to the Dirac comb associated with all sample points in a batch, we will recover the impulse responses for all sample points in that batch.

We start with a finite set of candidate points $P$. To build a batch of sample points, $S_b$, first we initialize $S_b$ as an empty set. Then we select the point $p \in P$ that is the farthest away from all points in previous sample point batches. That is, $p$ is a maximizer of the following optimization problem:
\begin{equation*}
\max_{p \in P} \min_{x \in S_1 \cup \dots \cup S_{b-1}} \|p - x\|.
\end{equation*}
Candidate points for the first batch are chosen arbitrarily from $P$.
Once $p$ is selected, we remove $p$ from $P$. If $p$ is sufficiently far from all of the previously chosen points in the current batch, in the sense that $E_p \cap E_q = \{\}$ for all $q \in S_b$,
%and if $p$ is sufficiently far from the boundary, in the sense that $E_p \cap \partial \Omega = \{\}$, 
then we add $p$ to $S_b$. Otherwise we discard $p$. 
%Recall that $E_p$ and $E_q$ are ellipsoids associated with points $p$ and $q$, respectively, as defined in \eqref{eq:support_ellipsoid}. 
This process repeats until there are no more points in $P$.  This is detailed in Algorithm \ref{alg:point_choice}. We determine whether $E_p \cap E_q = \{\}$ using the fast ellipsoid intersection test described in Appendix \ref{sec:fast_ellipsoid_intersection_test}.

We repeat the process to construct several batches of points $S_1, S_2, \dots$, until the number of batches exceeds a desired threshold. In our implementation, for each batch the set of candidate points $P$ is initialized as the set of all Lagrange nodes for the finite element basis functions used to discretize the problem, minus all points in previously chosen batches.


\begin{algorithm2e}
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwProg{Fn}{Function}{}{}
	
	\Input{Finite set of candidate points $P\subset \Omega$, \\mean $\mu(x)$ and covariance $\Sigma(x)$, \\domain boundary information $\partial \Omega$, \\previous sample point batches $S_1, \dots, S_{b-1}$}
	\Output{Batch of new sample points $S_b$}
	
	Initialize empty new batch of sample points, $S_b = \{\}$
	
	\While{$P$ is not empty}{
		Determine the point $p \in P$ that is farthest from all points in the previous sample point batches $S_1,\dots,S_{b-1}$
		
		Remove $p$ from $P$

	
		\If{$E_p \cap E_q \neq \{\}$ for all $q \in S_b$ and $E_p \cap \partial \Omega = \{\}$}{
			\tcp{$E_p$ and $E_q$ are the ellipsoids defined in \eqref{eq:support_ellipsoid}}
			
			Add $p$ to $S_b$
			
			Remove all points $p'$ satisfying $\mu(p') \in E_p$ from $P$
		}
	}

    \SetKwFunction{FMain}{point\_is\_acceptable}
	\caption{Choosing one batch of sample points, $S_b$}
	\label{alg:point_choice}
\end{algorithm2e}


\subsection{Convolution kernels}
\label{sec:get_impulse_response}

We compute convolution kernels $\varphi_{x_i}$ in batches by applying $\Aop$ to a small number Dirac combs. Each application of $\Aop$ to a Dirac comb yields one batch of impulse responses, $\phi_{x_i}$, which yields one batch of convolution kernels, $\varphi_{x_i}$.
%The batches of impulse responses, $\eta^b$, will ultimately be interpolated with weighting functions described in Section \ref{sec:weighting_functions} to form the desired approximation $\widetilde{\mathcal{A}}\approx \mathcal{A}$.  
The Dirac comb, $\xi_b$, associated with a batch of sample points, $S_b$, is the sum of Dirac distributions (point sources) centered at the points $x_i \in S_b$. That is,
\begin{equation*}
	\xi^b := \sum_{x_i \in S_b} \delta_{x_i}.
\end{equation*}
For each batch $S_b$, we compute the action of $\mathcal{A}$ on the associated Dirac comb:
\begin{equation}
\label{eq:dirac_comb_H_action}
	\eta^b := \langle\mathcal{A}, \xi^b\rangle^*.
\end{equation}
By linearity, the resulting function $\eta^b$ may be written as
\begin{equation}
\label{eq:phi_b}
	\eta^b = \left\langle\mathcal{A},\sum_{x_i \in S_b} \delta_{x_i}\right\rangle^* = \sum_{x_i \in S_b} \langle\mathcal{A}, \delta_{x_i}\rangle^* = \sum_{x_i \in S_b} \phi_{x_i}.
\end{equation}
Since the points $x_i$ are chosen so that the ellipsoid $E_{x_i}$ that (approximately) supports $\phi_i$, and the ellipsoid $E_{x_j}$ that (approximately) supports $\phi_j$ do not overlap when $i \neq j$, from \eqref{eq:convolution_kernel} and \eqref{eq:phi_b} we have (approximately)
\begin{equation}
\label{eq:varphi_eval}
	\varphi_{x_i}(z) = \phi_i(z+x_i) = \begin{cases}
		\eta^b(z+x_i), & z+x_i \in E_{x_i} \\
		0, & \text{otherwise}
		\end{cases}
\end{equation}
for all $x_i \in S_b$. By performing one matrix-vector product, \eqref{eq:dirac_comb_H_action}, we recover the convolution kernels $\varphi_{x_i}$ associated with every point $x_i \in S_b$. 

\subsubsection{Interpolation onto local regular grids}
\label{sec:convolution_kernel_regular_grid}

To convert $\AopPc$ to H-matrix format (Section \ref{sec:H_matrix_conversion}), we will need to evaluate $\varphi_{x_i}(z)$ for a large number of points $z$. Per \eqref{eq:varphi_eval}, evaluating $\varphi_{x_i}(z)$ requires evaluating $\eta_b(z+x_i)$. In computations, $\eta^b$ will be a finite element function formed on an irregular mesh, and $z+x_i$ will typically not be a gridpoint of the mesh. Evaluating $\eta^b(z+x_i)$ therefore requires finding which mesh cell that $z+x_i$ is in, then evaluating several polynomial functions to evaluate the finite element function on that cell. Although this procedure is asymptotically scalable, it is slow in practice \cite{FEMEVALSLOW}. 

To speed up evaluation of $\varphi_{x_i}$, we interpolate $\varphi_{x_i}$ onto a regular rectilinear grid on a coordinate axis aligned box that just barely contains the ellipsoid $E_{x_i}$. We transfer the function $\varphi_{x_i}$ from the irregular mesh to the regular grid by evaluating $\varphi_{x_i}$ at all gridpoints in the regular grid, using the slow process described in the previous paragraph. This is an upfront cost which is done ``offline.'' After $\varphi_{x_i}$ has been transferred to the regular grid, we use fast regular grid interpolation to perform all subsequent ``online'' evaluations of $\varphi_{x_i}$. This yields a substantial overall speedup because the number of finite element function evaluations required to transfer the functions $\varphi_{x_i}$ to the regular grid is $O(N)$, while the number of kernel evaluations required to convert $\AopPc$ to H-matrix format is $O(k N \log N)$. Here $N$ is the number of finite element degrees of freedom, and $k$ is the H-matrix rank. Furthermore, transferring $\varphi_{x_i}$ to a regular grid requires evaluating $\varphi_{x_i}$ at a predetermined and highly structured collection of points. This structure allows for faster determination of which mesh cells that points are in, and allows one to evaluate the finite element function at all regular gridpoints in parallel. In contrast, constructing the H-matrix requires evaluating $\varphi_{x_i}$ at an unpredictable sequence of scattered of points, which is an expensive task for finite element function evaluation, but a cheap task for regular grid interpolation. 
Having $\varphi_{x_i}$ stored on a regular grid also simplifies the implementation of the method that we use to deal with boundary artifacts, which will be described in Section \ref{sec:boundary_extension}.

We incur interpolation error when transferring $\varphi_{x_i}$ from the irregular mesh to the regular grid. To limit this interpolation error, the density of the regular grid should be as dense or denser than the local density of the finite element mesh. In our numerical results, the distance between neighboring points in the regular grid is chosen to be half the minimum distance between any pair of finite element Lagrange nodes that are contained in $E_{x_i}$. The grid transfer procedure is illustrated in Figure FIG. 

When $x_i$ is near a boundary, it may occur that some gridpoints for the regular grid are in locations where $\varphi_{x_i}$ is undefined. In this case, we store a placeholder value for $\varphi_{x_i}$ at those gridpoints. We use the IEEE standard value of ``NaN'' (not a number) for this placeholder. In the next section (Section \ref{sec:boundary_extension}) we replace these NaN values with more appropriate values.

%To minimize memory usage, the functions $\varphi_i$ are not stored individually. Instead, we store the functions $\eta^b$, and use formula \eqref{eq:varphi_eval} to perform function evaluations $z \mapsto \varphi_i(z)$ as needed.

\subsubsection{Addressing boundary artifacts}
\label{sec:boundary_extension}

Evaluating the kernel for our product-convolution approximation, $\AkerPc(y,x)$, requires evaluating $\varphi_{x_i}(y-x)$. But $\varphi_{x_i}(y-x)$ is undefined  when $y - x + x_i$ is outside $\Omega$, which may occur even if $x \in \Omega$ and $y \in \Omega$. The default approach to deal with this problem is to extend $\varphi_{x_i}$ by zero wherever it is undefined. But this leads to substantial errors, known as boundary artifacts, which are especially significant when the sample point $x_i$ is near the boundary of the domain. In this case, the impulse response is artificially chopped off by the domain boundary. See Figure FIG for an illustration. 

While $\varphi_{x_i}$ may be undefined at a point $z$, often there is a sample point $x_j$ that is near $x_i$, but further away from the boundary than $x_i$, such that $\varphi_{x_j}$ is well defined at $z$. To address boundary artifacts, we fill in missing/undefined portions of convolution kernels using information from other convolution kernels associated with the $k$ nearest sample points (we use $k=8$ in our numerical results). Kernels associated with nearer sample points are given precedence over kernels associated with farther away sample points. 
%After the convolution kernel has been extended using information from neighboring convolution kernels, it is then further extended by zero. 

In detail, let $p_1, p_2, \dots, p_k$ be the $k$ nearest sample points to $x_i$, ordered by nearness, including $x_i$ itself. That is, $p_1=x_i$, $p_2$ is the nearest neighbor to $x_i$, $p_3$ is the second nearest neighbor to $x_i$, and so on. Also, recall from the previous section (Section \ref{sec:convolution_kernel_regular_grid}) that the functions $\varphi_{x_i}$ are stored on regular rectilinear grids. To extend $\varphi_{x_i}$, we form an expanded regular rectilinear grid that has the same grid spacing as the grid used for $\varphi_{x_i}$, and contains that grid as a subgrid, but is large enough to contain the boxes associated with all of the neighbor kernels, $\varphi_{p_1}, \varphi_{p_2}, \dots, \varphi_{p_k}$ (recall that the functions $\varphi_{x_i}$ are all centered at zero). We initialize an extended convolution kernel, $\varphi_{x_i}^E$, to zero on this expanded grid. Then we interpolate the functions $\varphi_{p_j}$ onto this expanded grid in reverse order. First we interpolate $\varphi_{p_k}$ onto the grid, then $\varphi_{p_{k-1}}$, and so on until the last step in which we interpolate the original kernel $\varphi_{p_1} = \varphi_{x_i}$. At each step, we overwrite existing values of $\varphi_{x_i}^E$ with new values from the kernel currently being interpolated, except when the new value of $\varphi_{p_j}$ is undefined (``NaN''). At gridpoints where the kernel being interpolated is undefined, we leave $\varphi_{x_i}^E$ unchanged. In this way, we fill in undefined values of $\varphi_{x_i}$ with values from successively further and further away kernels. This process is illustrated in Figure FIG. To avoid notational clutter, and since we will henceforth only work with the extended kernel, $\varphi_{x_i}^E$, in the remainder of this paper the symbol $\varphi_{x_i}$ refers to $\varphi_{x_i}^E$.


\subsection{Poisson weighting functions}
\label{sec:weighting_functions}

%For the weighting functions, we use the smoothest functions (in a least-squares sense) that interpolate the points $x_i$. Constructing the weighting functions requires solving two Poisson PDEs per sample point $x_i$.

We choose the $i^\text{th}$ weighting function, $w_i(x)$, to be the solution to the following optimization problem:
\begin{equation}
\label{eq:wi_optimization_problem}
\begin{aligned}
\min_{w_i \in H^2_N(\Omega)} &\quad \frac{1}{2}\norm{\Delta w_i}_{L^2(\Omega)}^2 \\
\text{such that} &\quad w_i(x_j) = \delta_{ij}, \quad j=1,\dots,r.
\end{aligned}
\end{equation}
Here $\Delta$ is the Laplacian on $\Omega$, $\delta_{ij}$, is the Kronecker delta, which takes value one if $i=j$ and zero if $i \neq j$, and $H^2_N(\Omega)$ is the space of all functions in $H^2(\Omega)$ with Neumann zero boundary conditions. Using these weighting functions yields the ``flattest possible'' interpolation of the convolution kernels $\varphi_i$, in a least-squares sense (Theorem \ref{thm:smoothest_interpolant}). Also, these weighting functions sum to one pointwise (Proposition \ref{thm:wi_sum_to_one}).

For each $i=1,\dots,r$, optimization problem \eqref{eq:wi_optimization_problem} is an example of the following generic omtimization problem:
\begin{equation}
\label{eq:wi_optimization_problem_generic}
\begin{aligned}
\min_{v \in H^2_N(\Omega)} &\quad \frac{1}{2}\norm{\Delta v}_{L^2(\Omega)}^2 \\
\text{such that} &\quad v(x_j) = f_j, \quad j=1,\dots,r,
\end{aligned}
\end{equation}
where $f_j \in \mathbb{R}$, $j=1,\dots,r$ are now arbitrary function values that $v$ must take at the points $x_j$. We may efficiently solve optimization problem \eqref{eq:wi_optimization_problem_generic} via the following procedure:
\begin{enumerate}
\item For each sample point $x_i$, we compute a function $\psi_i$ by solving the following Neumann Poisson problem:
\begin{equation}
\label{eq:psi_eq}
\begin{cases}
\Delta \psi_i = \delta_{x_i} - 1/|\Omega|, & \text{in }\Omega, \\
\nu \cdot \nabla \psi_i = 0 & \text{on } \partial \Omega,
\end{cases}
\end{equation}
with condition $\int_\Omega \psi_i(x) dx = 0$. Here $\delta_{x_i}$ is the point source centered at $x_i$, $|\Omega|$ is the measure of the domain $\Omega$, and $\nu$ is the normal vector to $\partial \Omega$.
\item For each function $\psi_i$, we compute a function $\theta_i$ by solving another Neumann Poisson problem:
\begin{equation}
\label{eq:theta_eq}
\begin{cases}
\Delta \theta_i = \psi_i, & \text{in }\Omega, \\
\nu \cdot \nabla \theta_i = 0 & \text{on } \partial \Omega,
\end{cases}
\end{equation}
with condition $\int_\Omega \theta_i(x) dx = 0$.
%\item For each sample point $x_i$, we solve the Poisson equation with pure Neumann boundary conditions, using $\delta_{x_i}$ (a point source centered at $x_i$) as the right hand side source. We call the solution to this first Poisson problem $\psi_i$.
%\item For each function $\psi_i$, we solve the Poisson equation with pure Neumann boundary conditions, using $\psi_i$ as the right hand side source. We call the solution to this second Poisson problem $\theta_i$.
\item The solution to optimization problem \eqref{eq:wi_optimization_problem_generic}, which we denote by $v_*$, is formed as follows:
\begin{equation}
\label{eq:optimal_v}
v_*(x) := \beta + \sum_{i=1}^r c_i \theta_i(x),
\end{equation}
where the constants $c_i \in \mathbb{R}$, $i=1,\dots,r$, and $\beta \in \mathbb{R}$ are the solution to the following linear system:
\begin{equation}
	\label{eq:interpolation_linear_system}
	\begin{bmatrix}
	S & \mathbf{1} \\ \mathbf{1}^T & 0
	\end{bmatrix}
	\begin{bmatrix}
	\mathbf{c} \\ \beta
	\end{bmatrix}
	=
	\begin{bmatrix}
	\mathbf{f} \\ 0
	\end{bmatrix}.
\end{equation}
Here $\mathbf{c} \in \mathbb{R}^r$ is the vector with $i$th component $c_i$, $\mathbf{1} \in \mathbf{R}^r$ is the column vector of all ones, $\mathbf{f} \in \mathbb{R}^r$ is the vector with $i$th component $f_i$, and $S \in \mathbb{R}^{r \times r}$ is the matrix with entries given by
\begin{equation}
\label{eq:S_matrix}
S_{ij} := \int_\Omega \psi_i(x) \psi_j(x) dx.
\end{equation}
\end{enumerate}
In Theorem \ref{thm:wi_alg_is_correct}, we prove that the function $v_*$ produced by this process indeed solves optimization problem \eqref{eq:wi_optimization_problem_generic}.

Constructing all $r$ of the functions $w_i$ thus requires solving the Poisson equation $2r$ times with different right hand side sources to form the functions $\psi_i$ and $\theta_i$, then post-processing these functions. This is shown in Algorithm \ref{alg:weighting_functions_incremental}. The Poisson PDE solves, which are the most expensive part of this procedure, can be performed cheaply with multigrid. 

%All of the weighting functions, $\{w_i\}_{i=1}^r$, will change if a new point, $x_{r+1}$, is added. However, the dominant computational cost in constructing the weighting functions is the solution of the Poisson PDEs, and these Poisson PDE solves can be performed incrementally---we only need to compute functions $\psi_i$ and $\theta_i$ associated with the new points. 
%, rather than $2(r+1)$ new PDE solves.

It is important to note that functions in $H^2_N(\Omega)$ only have well-defined pointwise values if the spatial dimension of the domain is $d=1$, $2$, or $3$. Hence optimization problem \eqref{eq:wi_optimization_problem} is only well-posed in three spatial dimensions or fewer. In four or higher spatial dimensions, one could construct appropriate weighting functions using radial basis functions, though we do not discuss this here.

\begin{thm}[Solution of optimization problem \ref{eq:wi_optimization_problem_generic}]
	\label{thm:wi_alg_is_correct}
	The function $v_*$ given in \eqref{eq:optimal_v} is the unique solution to optimization problem \ref{eq:wi_optimization_problem_generic}.
\end{thm}

\begin{proof}
	Let
	\begin{equation*}
		X := \{v \in H^2_N(\Omega) : v(x_i) = v_i \text{ for }i=1,\dots,r\}
	\end{equation*}
	denote the feasible set for optimization problem \ref{eq:wi_optimization_problem_generic}, and let 
	\begin{equation*}
	X_0 := \{v \in H^2_N(\Omega) : v(x_i) = 0 \text{ for }i=1,\dots,r\}
	\end{equation*}	
	denote the tangent space to $X$. Note that pointwise values of functions in $H_N^2(\Omega)$ are well-defined since the Sobolev imbedding theorem (see, e.g., Theorem 7.19 in \cite{Arbogast}) implies that $H^2(\Omega)$ is continuously imbedded in the space of continuous functions $C^0(\Omega)$ when $d < 4$, and we are considering $d \in \{1,2,3\}$. The feasible set $X$ is therefore a closed affine subspace of $H^2_N(\Omega)$, the tangent space $X_0$ is a closed subspace of $H^2_N(\Omega)$, and optimization problem \ref{eq:wi_optimization_problem_generic} is well-defined.
	
	The proof now proceeds in three steps. First, we show that $v_*$ is feasible ($v_* \in X$). Second, we show that $v_*$ is the unique minimizer of \eqref{eq:wi_optimization_problem_generic} if it satisfies a certain variational equation. Third, we verify that $v_*$ satisfies this variational equation.
	
	\paragraph{Feasibility} To show that $v_*$ is feasible (i.e., $v_* \in X$), we must show that $v_*(x_i) = f_i$ for $i=1,\dots,r$. We have
	\begin{align*}
	v_*(x_i) &= \beta + \sum_{j=1}^r c_j \theta_j(x_i) \\
	&= \beta + \sum_{j=1}^r c_j \left(\psi_i, \Delta \theta_j\right)_{L^2(\Omega)} + \avg\left(\theta_j\right) \\
	&= \beta + \sum_{j=1}^r c_j \left(\psi_i, \psi_j\right)_{L^2(\Omega)} \\
	&= \beta + \sum_{j=1}^r c_j S_{ij}
	\end{align*}
	In the first line we used the definition of $v_*$ in \eqref{eq:optimal_v}. In the second line we used Lemma LEM. In the third line we used the facts that, by construction, $\Delta \theta_j = \psi_j$ and $\int_\Omega \theta_j(x) dx = 0$. In the fourth line we used the definition of $S$ in \eqref{eq:S_matrix}. Hence the condition $v_*(x_i) = f_i$ for $i=1,\dots,r$ may be written in matrix form as
	\begin{equation*}
	S \mathbf{c} + \beta \mathbf{1} = \mathbf{f},
	\end{equation*}
	and this must hold because it is the first block row of system \eqref{eq:interpolation_linear_system} which is solved to compute $\mathbf{c}$ and $\beta$. So $v_* \in X$.
	
	\paragraph{Optimality} Since $X$ is an affine subspace, and $X_0$ is the tangent space to $X$, any $v \in X$ may be written in the form $v = v_* + p$ for some $p \in X_0$. We may therefore reframe optimization problem \ref{eq:wi_optimization_problem_generic} as 
	\begin{equation}
	\label{eq:p_optimization}
		\min_{p \in X_0} J(p),
	\end{equation}
	where
	\begin{equation*}
		J(p) := \frac{1}{2}||\Delta (v_* + p)||^2.
	\end{equation*}
	Showing that $v_*$ is the unique minimizer of \eqref{eq:wi_optimization_problem_generic} is equivalent to showing that $0$ is the unique minimizer of \eqref{eq:p_optimization}. 
	
	By direct expansion, we have
	\begin{equation}
		\label{eq:Ju_expanded}
		J(p) = J(0) + \left(\Delta u, \Delta p\right)_{L^2(\Omega)} + \frac{1}{2}||\Delta p||^2.
	\end{equation}
	Since $p$ has Neumann zero boundary conditions, if $\Delta p = 0$, then $p$ is constant on each connected component of $\Omega$. Furthermore, since $p(x_i)$ is zero for at least one point $x_i$ per connected component of $\Omega$, if $\Delta p = 0$ then $p=0$. Hence, by contrapositive we have
	\begin{align*}
	\label{eq:contrapositive_laplacian_zero}
		p \neq 0 \quad \implies \quad& 0 < ||\Delta p||_{L^2(\Omega)}\\
		\implies \quad& J(0) < J(p) + \left(\Delta v_*, \Delta p\right)_{L^2(\Omega)}.
	\end{align*}
	If $v_*$ satisfies the following variational equation,
	\begin{equation}
	\label{eq:variational_up}
		(\Delta v_*, \Delta p)_{L^2(\Omega)} = 0 \quad \text{for all~} p \in X_0,
	\end{equation} 
	then
	\begin{equation*}
		p \neq 0 \quad \implies \quad J(0) < J(p).
	\end{equation*}
	Showing that $v_*$ is the unique minimizer of \eqref{eq:wi_optimization_problem_generic} therefore reduces to showing that $v_*$ satisfies variational equation \eqref{eq:variational_up}. 
	
	\paragraph{Variational equation} To show that $v_*$ satiafies variational equation \eqref{eq:variational_up}, notice that
	\begin{align*}
	(\Delta v_*, \Delta p)_{L^2(\Omega)} =& \sum_{i=1}^r c_i(\Delta \theta_i, \Delta p)_{L^2(\Omega)} \\
	=& \sum_{i=1}^r c_i \left(p(x_i) + \avg\left(p\right)\right) \\
	=& \avg\left(p\right) \sum_{i=1}^r c_i. 
	\end{align*}
	In the first line we used the definition of $\theta_i$ and the fact that the Laplacian of a constant is zero. In the second line we used Lemma LEM. In the third line we used the fact that $p(x_i)=0$ because $p \in X_0$. Hence variational equation \eqref{eq:variational_up} holds if $\sum_{i=1}^r c_i = 0$, which may be written in matrix form as $\mathbf{1}^T \mathbf{c} = 0$. This must hold because it is the second block row in system \eqref{eq:interpolation_linear_system}.
\end{proof}


\begin{thm}[Poisson weighting functions are optimal]
	\label{thm:smoothest_interpolant}
Let $y \in \mathbb{R}^d$, $d \in \{1,2,3\}$, and let $\gamma_y$ be the function
\begin{equation}
\label{eq:u_defn_in_thm}
\gamma_y(x) := \sum_{i=1}^r \varphi_i(y)w_i(x) = \AkerPc\left(y+x,x\right),
\end{equation}
where the weighting functions $w_i$ are the solutions to optimization problem \eqref{eq:wi_optimization_problem}. Then $\gamma_y$ solves the optimization problem
\begin{equation}
\label{eq:uy_optimization_problem}
\begin{aligned}
\min_{\gamma_y \in H^2_N(\Omega)} &\quad \frac{1}{2}\norm{\Delta \gamma_y}_{L^2(\Omega)}^2 \\
\text{such that} &\quad \gamma_y(x_i) = \varphi_i(y), \quad i=1,\dots,q.
\end{aligned}
\end{equation}
\end{thm}

\begin{prop}[Poisson weighting functions sum to one]
	\label{thm:wi_sum_to_one}
	We have
	\begin{equation*}
		\sum_{i=1}^r w_i(x) = 1
	\end{equation*}
	for all $x \in \Omega$.
\end{prop}



\begin{algorithm2e}
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	{	
		\Input{Sample points $\{x_i\}_{i=1}^r$, domain geometry $\Omega$}
		\Output{Weighting functions $\{w_i\}_{i=1}^r$}
		
%		Initialize constant function $C(x) = 1$
		
%		Compute Lebesgue measure of the domain, $|\Omega|$
		
%		\tcp{Compute Poisson impulse responses $\{\psi_i\}_{i=1}^r$ and Poisson squared impulse responses $\{\theta_i\}_{i=1}^r$}
		
		\For{$k=1,2, \dots, r$}{
%			Initialize $\delta_{x_i}$ as the point source (delta distribution) centered at $x_i$.
			
			Solve the Neumann Poisson problem \eqref{eq:psi_eq} for $\psi_k$.
			
%			Compute $\psi_i$ by solving the Neumann Poisson problem
%			\begin{equation}
%			\label{eq:psi_eq}
%			\begin{cases}
%			\Delta \psi_i = \delta_{x_i} - 1/|\Omega|, & \text{in }\Omega, \\
%			\nu \cdot \nabla \psi_i = 0 & \text{on } \partial \Omega,
%			\end{cases}
%			\end{equation}
%			with condition $\int_\Omega \psi_i(x) dx = 0$, where $\nu$ is the normal vector to $\partial \Omega$.

			Solve the Neumann Poisson problem \eqref{eq:theta_eq} for $\theta_k$.
			
%			Compute $\theta_i$ by solving the Neumann Poisson problem
%			\begin{equation}
%			\label{eq:theta_eq}
%						\begin{cases}
%			\Delta \theta_i = \psi_i, & \text{in }\Omega, \\
%			\nu \cdot \nabla \theta_i = 0 & \text{on } \partial \Omega,
%			\end{cases}
%			\end{equation}
%			with condition $\int_\Omega \theta_i(x) dx = 0$, where $\nu$ is the normal vector to $\partial \Omega$.
			
		}
		
		Form the $r \times r$ matrix $S$ given in \eqref{eq:S_matrix}.
		
%		\tcp{Compute weighting functions $\{w_i\}_{i=1}^r$}
		
		\For{$i=1,2,\dots,r$}{
			Solve the linear system
			\begin{equation*}
			\begin{bmatrix}
			S & \mathbf{1} \\ \mathbf{1}^T & 0
			\end{bmatrix}
			\begin{bmatrix}
			\mathbf{c}^{(i)} \\ \beta^{(i)}
			\end{bmatrix}
			=
			\begin{bmatrix}
			\mathbf{e}^{(i)} \\ 0
			\end{bmatrix}.
			\end{equation*}
			for $\mathbf{c}^{(i)}=(c_1^{(i)}, c_2^{(i)}, \dots, c_r^{(i)})$ and $\beta^{(i)}$. Here $\mathbf{1} = (1,1,\dots,1) \in \mathbb{R}^r$, and $\mathbf{e}^{(i)} = (0,\dots,0,1,0,\dots,0)\in \mathbb{R}^r$ is the unit vector with $i^\text{th}$ component equal to one and all other components equal to zero.
			
			Form the weighting function $w_i$ given by
			\begin{equation*}
			w_i(x) = \beta^{(i)} + \sum_{k=1}^r c_k^{(i)} \theta_k(x).
			\end{equation*}
			
		}
	}
	\caption{Compute weighting functions $w_i$, $i=1,\dots,r$.}
	\label{alg:weighting_functions_incremental}
\end{algorithm2e}



%\subsection{Evaluating kernel entries of the approximation}
%\label{sec:eval_matrix_entries}
%
%We now describe how to efficiently evaluate $\widetilde{\Phi}(y,x)$ at an arbitrary point $(x,y) \in \Omega \times \Omega$. From \eqref{eq:kernel_entries}, we have
%\begin{equation}
%\label{eq:eval_y_x_000}
%\AkerPc(y,x) = \sum_{i=1}^r w_i(x) \varphi_{x_i}(y-x),
%\end{equation}
%and from \eqref{eq:varphi_eval}, we have
%\begin{equation}
%\label{eq:kernel_entry_formua}
%\varphi_{x_i}(y-x) = \begin{cases}
%\eta^{b(i)}(y - x + x_i), & y - x + x_i \in E_{x_i} \\
%0, & \text{otherwise},
%\end{cases}
%\end{equation}
%where $b(i)$ denotes the batch index such that $x_i \in S^{b(i)}$. To compute $z=\AkerPc(y,x)$, we start with $z=0$, then loop through all batches $b$, and all sample points in each batch. For each sample point, we add one term in the sum in \eqref{eq:eval_y_x} to $z$, using \eqref{eq:kernel_entry_formua} as needed to evaluate $\varphi_{x_i}$. This is shown in Algorithm \ref{alg:eval_y_x}.
%
%
%\begin{algorithm2e}
%	\SetAlgoNoLine
%	\SetKwInOut{Input}{Input}
%	\SetKwInOut{Output}{Output}
%	
%	\Input{points $x \in \Omega$, $y \in \Omega$, \\sample point batches $S_1, \dots, S_\text{max\_batches}$, \\Dirac comb responses $\{\eta^b\}_{b=1}^{\text{max\_batches}}$, \\weighting functions $\{w_i\}_{i=1}^r$}
%	\Output{Kernel entry $z = \widetilde{A}(y,x)$}
%
%	$z \gets 0$
%	
%	\For{$b=1,2,\dots,\text{}max\_batches$}{
%		\For{$x_i \in S_b$}{
%			\If{$y - x + x_i \in E_{x_i}$}{
%				$z \gets z + w_i(x)\eta^b(y-x+x_i)$
%			}
%		}
%	}	
%	\caption{Compute approximate kernel entry $z = \widetilde{A}(y,x)$}
%	\label{alg:eval_y_x}
%\end{algorithm2e}


\subsection{Discretization and hierarchical matrix construction}
\label{sec:H_matrix_conversion}

Here we discuss how we construct an H-matrix representation of our product-convolution approximation, after discretizing the problem with finite elements. 

\subsubsection{Finite element discretization of the integral kernel}

Let $f_1, f_2, \dots, f_N$ be a set of finite element basis functions used to discretize the problem on a mesh with mesh size parameter $h$, and let $V_h := \Span\left(f_1, f_2, \dots, f_N\right)$ be the corresponding finite element space. Further, let $p_i \in \mathbb{R}^d$, $i=1,\dots, N$ be the Lagrange nodes associated with the functions $f_i$. That is, the points such that $f_i(p_j)$ equals one if $i=j$, and zero otherwise.

In Section \ref{sec:intro} we noted that the original integral kernel, $\Aker$, is inaccessible, while in Equation \eqref{eq:kernel_entries}  we saw that the integral kernel for our product-convolution approximation, $\AkerPc \approx \Aker$, is accessible, and may be cheaply evaluated at arbitrary pairs of points $(y,x) \in \Omega \times \Omega$ via the following formula:
\begin{equation}
\label{eq:eval_y_x}
\AkerPc(y,x) = \sum_{i=1}^r w_i(x) \varphi_{x_i}(y-x).
\end{equation}
We now define a further approximation, $\AkerPcMesh \approx \AkerPc$, as follows:
\begin{equation}
\label{eq:defn_of_Akerpcmesh}
\AkerPcMesh(y,x) := \sum_{i=1}^N \sum_{j=1}^N \AkerPc(p_i,p_j) f_i(y) f_j(x).
\end{equation}
The kernel $\AkerPcMesh$ is the interpolation of $\AkerPc$ onto $V_h \otimes V_h$. Error in the overall kernel approximation, $\AkerPcMesh \approx \Aker$, arises both because of error in the product-convolution approximation, and because of finite element discretization error incurred from interpolation onto $V_h \otimes V_h$. 

Replacing $\Aker$ with $\AkerPcMesh$ within the definition of $\Aop$ in \eqref{eq:kernel_representation} yields the following operator approximation, $\AopPcMesh$:
\begin{equation}
\label{eq:kernel_representation_pcmesh}
(\AopPcMesh u)(v) := \int_\Omega \int_\Omega v(y) \AkerPcMesh(y,x) u(x) dx dy.
\end{equation}
While $\AopPcMesh$ is well-defined as a mapping $L^2(\Omega) \rightarrow L^2(\Omega)'$, we are interested in the restriction of $\AopPcMesh$ to functions in the finite element space. That is, $\AopPcMesh : V_h \rightarrow V_h'$.
%Error in the approximation $\Aop \approx \AopPcMesh$ is a combination of error in the product-convolution approximation, and error due to finite element discretization.
Let $u_h \in V_h$ and $v_h \in V_h$, and let $\mathbf{u}$ and $\mathbf{v}$ be the coefficient vectors for $u_h$ and $v_h$, respectively, with respect to the finite element basis $f_1, \dots, f_N$. I.e., 
\begin{equation}
\label{eq:fem_coeff_basis}
u_h(x) = \sum_{i=1}^N \mathbf{u}_i f_i(x),
\end{equation}
and similar for $v_h$. Further, let $\mathbf{M} \in \mathbb{R}^{N \times N}$ be the finite element mass matrix,
% which has entries
%\begin{equation}
%\label{eq:mass_matrix_defn}
%\mathbf{M}_{ij} := \int_\Omega f_i(x) f_j(x) dx,
%\end{equation}
and let $\AkerPcMat \in \mathbb{R}^{N \times N}$ be following matrix of approximate kernel entries:
\begin{equation}
\label{eq:Akerpcmat_entries}
\left(\AkerPcMat\right)_{ij} := \AkerPcMesh(p_i, p_j).
\end{equation}
Using \eqref{eq:defn_of_Akerpcmesh}, \eqref{eq:kernel_representation_pcmesh}, \eqref{eq:fem_coeff_basis}, and \eqref{eq:Akerpcmat_entries}, the definition of the mass matrix, and performing algebraic manipulations, it is straightforward but tedious to show that
\begin{equation*}
\left(\AopPcMesh u_h \right)(v_h) = \mathbf{v}^T \mathbf{M} \AkerPcMat \mathbf{M} \mathbf{u}.
\end{equation*}
% PUT INTO APPENDIX!!!!
%\begin{align*}
%\left(\AopPcMesh u_h \right)(v_h) =& \int_\Omega \int_\Omega v_h(y) \AkerPcMesh(y,x) u_h(x) dx dy \\
%=& \int_\Omega \int_\Omega \left(\sum_{i=1}^N \mathbf{v}_i f_i(y)\right) \left(\sum_{j=1}^N \sum_{k=1}^N \AkerPc(p_j,p_k) f_j(y) f_k(x)\right) \left(\sum_{l=1}^N \mathbf{u}_l f_l(x)\right) dx dy \\
%=& \sum_{i,j,k,l=1}^N \mathbf{v}_i \left(\int_\Omega f_i(y) f_j(y) dy\right) \AkerPcMesh(p_j, p_k) \left(\int_\Omega f_k(x) f_l(x) dx\right) \mathbf{u}_l \\
%=& \sum_{i,j,k,l=1}^N \mathbf{v}_i \mathbf{M}_{ij} \left(\AkerPcMat\right)_{jk} \mathbf{M}_{kl} \mathbf{u}_l \\
%=& \mathbf{v}^T \mathbf{M} \AkerPcMat \mathbf{M} \mathbf{u},
%\end{align*}
The finite element matrix representation of $\AopPcMesh$, which we denote by $\AmatPc$, is therefore given by
\begin{equation}
\label{eq:Amatpc_defn}
\AmatPc := \mathbf{M} \AkerPcMat \mathbf{M}.
\end{equation}

\subsubsection{Hierarchical matrix construction}

We form an H-matrix representation of $\AmatPc$ by converting $\AkerPcMat$ and $\mathbf{M}$ to H-matrix format, then using fast H-matrix methods to multiply these matrices according to \eqref{eq:Amatpc_defn}. Once $\AmatPc$ is in H-matrix format, useful matrix operations such as matrix-vector products, matrix-matrix addition, matrix-matrix multiplication, and matrix inversion may be performed using fast scalable H-matrix methods \cite{BORMHACKBUSCHBOOK}. We use H1 matrices in our numerical results, but any of the other H-matrix formats (such as H2, HODLR, HSS, HBS, and others \cite{HMATRIX}) could be used instead. For more details on H-matrices, we recommend \cite{HMATRIXGOOD}. 
%We discuss H-matrices in greater detail in Appendix \ref{app:h_matrix}.

We convert $\AkerPcMat$ into H1 matrix format using the standard geometrical clustering/adaptive cross method implemented within the HLIBPro software package \cite{HLIBPRO}. Although $\AkerPcMat$ is a dense $N \times N$ matrix, constructing the H-matrix approximation of $\AkerPcMat$ only requires evaluation of $O(k^2 N \log N)$ entries of $\AkerPc$, and these entries are computed via the formula in Equation \eqref{eq:eval_y_x}. Here $k$ is the rank of the highest-rank block in the H-matrix. A dense representation of $\AkerPcMat$ is never formed. We describe this H-matrix construction process in detail in Appendix \ref{app:h_matrix}. We convert $\mathbf{M}$ to H-matrix format using standard H-matrix methods for sparse matrices implemented within HLIBPro, using the same recursive block partitioning structure as was used for $\AkerPcMat$. 


\subsection{Rational positive semi-definite modification}
\label{sec:make_spd}

In many problems of practical interest (e.g., Hessian approximation) $\Amat$ is symmetric positive semi-definite. However, $\AmatPc$ is generally non-symmetric and indefinite because of errors in the product-convolution approximation. 
This is undesirable. Symmetry and positive semi-definiteness are important properties which should be preserved if possible. Also, lacking these properties may prevent one from using highly effective algorithms, such as the conjugate gradient method, to perform further useful operations involving $\AmatPc$.
In this section we describe a method for modifying $\AmatPc$ to make it symmetric positive semi-definite.

First, we use fast H-matrix addition to symmetrize $\AmatPc$:
\begin{equation*}
\AmatPcSym := \frac{1}{2}\left(\AmatPc + \AmatPc^T\right).
\end{equation*}
Then we modify $\AmatPcSym$ to make it positive semi-definite by forming a rational matrix function of the following form: 
\begin{equation}
\label{eq:rational_matrix_function}
\AmatPcSymPlus := c_0 \mathbf{I} + c_1 \AmatPcSym + c_2 \left(\AmatPcSym + \mu \mathbf{I}\right)^{-1},
\end{equation}
where $\mathbf{I}$ is the identity matrix which has the same shape as $\AmatPcSym$. Approximation \eqref{eq:rational_matrix_function} may be written as $\AmatPcSymPlus = \ratfct(\AmatPcSym)$, where $\ratfct$ is the following rational function:
\begin{equation}
\label{eq:rational_function_scalar}
\ratfct(\lambda) := c_0 + c_1 \lambda + \frac{c_2}{\lambda+\mu}.
\end{equation}
The scalars $c_0$, $c_1$, $c_2$, and $\mu$ are chosen so that the moderate and large positive eigenvalues of $\AmatPcSymPlus$ approximately equal the corresponding moderate and large positive eigenvalues of $\AmatPcSym$, while the negative and small positive eigenvalues of $\AmatPcSym$ are modified so that $\AmatPcSymPlus$ is positive semi-definite. Later in this section we will explain how $c_0$, $c_1$, $c_2$, and $\mu$ are chosen and how our choice of these scalars affects the spectral properties of $\AmatPcSymPlus$. In Figure FIG we illustrate $\ratfct(\lambda)$. Forming $\AmatPcSymPlus$ requires computing an H-matrix inverse, $\left(\AmatPcSym + \mu \mathbf{I}\right)^{-1}$. While explicit computation of matrix inverses should usually be avoided, here it is computationally acceptable because $\AmatPcSym + \mu \mathbf{I}$ is an H-matrix. Inversion of H-matrices is a routine procedure which only requires $O(N \log(N)^2)$ work \cite{Hmatrixinverse}.

We choose $c_0, c_1, c_2$, and $\mu$, to be the solution to the following system of equations:
\begin{subequations}
	\label{eq:four_eq_system}
\begin{align}
\ratfct(0) &= 0 \label{eq:desired_property_1} \\
\ratfct'(0) &= 0 \label{eq:desired_property_2} \\
\ratfct(\lambda_\text{min}) &= |\lambda_\text{min}| \label{eq:desired_property_3} \\
\ratfct(\lambda_\text{max}) &= \lambda_\text{max}, \label{eq:desired_property_4}
\end{align}
\end{subequations}
where $\lambda_\text{min}$ and $\lambda_\text{max}$ are the smallest and largest eigenvalues of $\AmatPcSym$, respectively. We also require $\mu > |\lambda_\text{min}|$, so that the pole of $\ratfct$ is not in $[\lambda_\text{min},\lambda_\text{max}]$. Note that $\lambda_\text{min} < 0$ since $\AmatPcSym$ is indefinite. 
We estimate $\lambda_\text{min}$ and $\lambda_\text{max}$ using the implicitly restarted Lanczos method \cite{lanczos,scipy}. We describe how we solve system \eqref{eq:four_eq_system} in Appendix \ref{app:solve_rational_system}. Solving \eqref{eq:four_eq_system} reduces to solving a smooth one-dimensional optimization problem, which is computationally trivial. 

Together, equations \eqref{eq:desired_property_1}, \eqref{eq:desired_property_2}, \eqref{eq:desired_property_3},  and \eqref{eq:desired_property_4}, and the condition $\mu > |\lambda_\text{min}|$, imply that $\ratfct(0)=0$ is the unique minimum of $\ratfct$ on $[\lambda_\text{min},\lambda_\text{max}]$, which implies that $\ratfct$ is non-negative on $[\lambda_\text{min},\lambda_\text{max}]$, which implies that  $\AmatPcSymPlus$ is positive semi-definite. Furthermore, \eqref{eq:desired_property_1} implies that the null space of $\AmatPcSymPlus$ contains the null-space of $\AmatPcSym$. This is important when the operator $\mathcal{A}$ has a large (or infinite) dimensional null space, or a spectrum that clusters at zero. Hessians in ill-posed distributed parameter inverse problems typically have both of these properties. Equation \eqref{eq:desired_property_4} forces the moderate and large positive eigenvalues of $\AmatPcSymPlus$ to be close to the corresponding moderate and large positive eigenvalues of $\AmatPcSym$. I.e., $\ratfct$ modifies the ``important'' part of the spectrum of $\AmatPcSym$ as little as possible. Equation \eqref{eq:desired_property_3} ensures that the negative eigenvalues of $\AmatPcSym$, which result from error in the product-convolution approximation, do not get amplified in magnitude by the function $\ratfct$. There is a tradeoff: if we choose a larger value for $s(\lambda_\text{min})$, then the positive eigenvalues of $\AmatPcSymPlus$ will be closer to the corresponding positive eigenvalues of $\AmatPcSym$ (good), but the erroneous negative eigenvalues of $\AmatPcSym$ will be amplified in magnitude (bad). This is shown in Figure FIG. We find that $\ratfct(\lambda_\text{min})=|\lambda_\text{min}|$ is an appropriate balance between these competing interests.




\subsection{Computational cost}

c\begin{itemize}
	\item computational complexity in terms of PDE solves:
	\item cost of global low rank approximation (data scalability)
	\item cost to build approximation
	\item cost without compression to do a solve
	\item cost with compression to do a solve
	\item computational complexity for H-matrix alone vs. prod-conv + h-matrix $O(C k^2 \log N)$ PDE solves, C is big, k is big. Here $10$
	\item HODLR: C is smaller, but k is bigger, vs. H1 peeling process
	\item H-matrix operations cost for our method solve
	\item irregular grid
\end{itemize}

\section{Rational positive definite modification}
WRITE THIS



\section{Numerical results}
\label{sec:numerical_results}

\textbf{Heat equation}
\begin{itemize}
	\item Hessian per-column error plots for 1, 5, and 15 batches
	\item $H-P$ error vs number of batches for interior and whole domain
	\item $P^{-1}H-I$: error vs rank for $P=R$ and $P=P_\text{pch}$ diffusion time, for 1, 5, and 15 batches
	\item Krylov iterations to tols 1e-1 and 1e-6 vs diffusion parameter
	\item GLR vs PCH vs diffusion time
	\item Krylov method convergence plot: $R$ vs $P_\text{pch}$ vs no preconditioner
	\item Krylov iterations to tols 1e-6 vs mesh size $h$, PCH vs R vs no preconditioner (hold)
	\item true parameter (H)
	\item deterministic reconstruction (H)
	\item noisy measurements (1\%, 5\%, 10\% noise). $u|_\text{top}$ (H)
	\item recovered state at top (H)
	\item mesh scalability of PCH (CC) (H)
	\item put plot data into data directory
	\item save function .pvd files in paraview  directory
\end{itemize}

\begin{itemize}
	\item GLR vs PCH number of obs (Computational cost in PDE solves) (S)
	\item GLR vs PCH aspect ratio (CC) (S)
	\item turn on preconditioner after number of krylov iterations exceeds 15 in an iteration. Rebuild "as needed". (S)
\end{itemize}



Some numerical results here.

\section{Conclusions}
\label{sec:conclusions}

Some conclusions here. 

\appendix

\section{Hierarchical matrix details}
\label{app:h_matrix}

Often, large dense matrices of practical interest may be permuted, then partitioned into blocks recursively, in such a way that many off-diagonal blocks of the matrix are numerically low rank, even if the matrix is high rank. Such matrices are known as hierarchical matrices (H-matrices). Many classes of H-matrices exist (H1, H2, HSS, HBS, and more), and all of these types of H-matrices could be used in conjunction with our product-convolution approximation. Here we use classical H1 matrices. For this section, when we say H-matrix, we are referring to H1 matrices. 

While a dense $N \times N$ matrix traditionally requires $O(N^2)$ memory to store, H-matrices may be stored using $O(N \log N)$ memory, by storing only the low rank factors for the low rank blocks. Recursive algorithms can take advantage of the H-matrix low rank block structure to perform matrix arithmetic fast. Conventional dense matrix algorithms for matrix inversion, matrix factorization, matrix-vector products, matrix-matrix products, and matrix-matrix addition require either $O(N^2)$ or $O(N^3)$ time and memory, while the aforementioned recursive algorithms can perform these matrix operations in $O(N \log(N)^a)$ time and memory for H-matrices. Here $a=0, 1, 2$, or $3$ depending on the operation and type of H-matrix. For more details on H-matrices, we recommend \cite{HMATRIXGOOD}. 

\subsection{H-matrix construction}

%The H-matrix construction method uses geometric information about the spatial locations of the degrees of freedom associated with the rows and columns of the matrix (here, these locations are the finite element Lagrange nodes $p_i$, $i=1,\dots,N$) to construct a recursive block partitioning of $\AkerPcMat$, such that all blocks at the finest level of the partitioning are either expected to be low rank, or are small. Then, the method forms low rank approximations of the blocks that are expected to be low rank by sampling specially chosen rows and columns of those blocks. The small high rank blocks are formed directly by evaluating their entries. 

In detail, the process of constructing an H-matrix representation of $\AkerPcMat$ proceeds as follows. First, we construct hierarchical partitionings of the degrees of freedom for the columns and rows of the matrix (cluster trees, Section \ref{sec:cluster_trees}). Second, we construct a hierarchical partitioning of the blocks of the matrix, in such a way that many of the blocks in the partitioning are expected to be low rank, and the remaining high rank blocks are small (block cluster tree, Section \ref{sec:block_cluster_tree}). Finally, we form low rank approximations of the blocks of the matrix that are expected to be low rank (adaptive cross approximation, Section \ref{sec:adaptive_cross}), and fill in the remaining high rank  blocks with their numerical values. The first two steps require geometric information about the spatial locations of the degrees of freedom associated with the rows and columns of the matrix, but these steps do not depend on the particular values of matrix entries. The third step requires us to evaluate $O(N \log N)$ specially-chosen entries of $\AkerPcMat$, and we evaluate these entries using \eqref{eq:Akerpcmat_entries}. 

\subsubsection{Cluster trees}
\label{sec:cluster_trees}

We use recursive hyperplane splitting to hierarchically cluster the degrees of freedom associated with the columns and rows of the matrix into a \emph{column cluster tree} and a \emph{row cluster tree}, respectively. Here we describe construction of the column cluster tree; the row cluster tree is constructed similarly. 

Since we use finite elements to discretize the problem, the $i^\text{th}$ column of $\AkerPcMat$ corresponds to the Lagrange node in $\mathbb{R}^d$ associated with the $i^\text{th}$ finite element basis function. The columns of the matrix thus correspond to a point cloud in $\mathbb{R}^d$. We split this point cloud into two equally sized \emph{child} point clouds, using a hyperplane which is perpendicular to the coordinate axis direction in which the point cloud is widest (e.g., either the $x$, $y$, or $z$ axis in $3$D). The two child point clouds are split in the same way. This splitting process repeats until the point clouds have less than a preset number of points (we use $32$ points). This hierarchical partitioning of the point cloud into smaller and smaller point clouds corresponds to a hierarchical partitioning of the columns of the matrix into smaller and smaller \emph{clusters} of columns. This hierarchical partitioning of the columns forms a binary tree, which is called the column cluster tree. The root of the tree is the set of all columns, and the leaves of the tree are clusters of columns that are not subdivided any further. 

A depth-first search ordering of the column cluster tree leaves is then generated. When the columns of the matrix are permuted into this depth-first ordering, the columns associated with each cluster in the cluster tree are contiguous.

In the same way, the degrees of freedom associated with the rows of the matrix are hierarchically clustered into another cluster tree, and a depth-first search ordering for the rows is generated. In our examples, the degrees of freedom for the columns coincide with the degrees of freedom for the rows, so the cluster trees for the rows and columns are the same, but this is not required in general.

\subsubsection{Block cluster tree} 
\label{sec:block_cluster_tree}

We partition the matrix into a recursive hierarchy of mostly low rank blocks called the \emph{block cluster tree}. The idea is that a block of the matrix is likely to be low rank if the point cloud associated with the rows of the block is far away from the point cloud associated with the columns of the block. This is reasonable to expect here because of the locality property of $\mathcal{A}$. Indeed, locality implies that many blocks of the matrix corresponding to far away point cloud clusters will be rank zero.

After reordering the rows and columns of $\AkerPcMat$ via the depth-first ordering described above, we partition the reordered version of $\AkerPcMat$ into a tree of $2 \times 2$ block matrices recursively. We use a geometric admissibility condition (discussed below) to decide which blocks to subdivide, and use the cluster trees for the rows and columns to determine how to subdivide those blocks. For the first stage of subdivision, let $r_1$ and $r_2$ be the children row clusters for the root of the row cluster tree, let $c_1$ and $c_2$ be the children column clusters for the root of the column cluster tree. The matrix $\AkerPcMat$ is partitioned into blocks as follows:
\begin{equation*}
\begin{bmatrix}
\left(\AkerPcMat\right)_{11} & \left(\AkerPcMat\right)_{12} \\
\left(\AkerPcMat\right)_{21} & \left(\AkerPcMat\right)_{22},
\end{bmatrix}
\end{equation*}
where $\left(\AkerPcMat\right)_{11}$ denotes the block of $\AkerPcMat$ with rows $r_1$ and columns $c_1$, $\left(\AkerPcMat\right)_{12}$ denotes the block of $\AkerPcMat$ with rows $r_1$ and columns $c_2$, and so on for $\left(\AkerPcMat\right)_{21}$ and $\left(\AkerPcMat\right)_{22}$.

We now loop through the four blocks, $\left(\AkerPcMat\right)_{11}$, $\left(\AkerPcMat\right)_{12}$, $\left(\AkerPcMat\right)_{21}$, and $\left(\AkerPcMat\right)_{22}$, and decide which blocks should be subdivided further. For the purpose of explanation, consider $\left(\AkerPcMat\right)_{12}$. If
\begin{equation}
\label{eq:weak_admissibility_cond}
\dist\left(r_1, c_2\right) \ge \eta \min\left(\diam\left(r_1\right), \diam\left(c_2\right)\right),
\end{equation}
then we mark $\left(\AkerPcMat\right)_{12}$ as \emph{admissible} (expected to be low rank) and leave it alone. Here $\dist\left(r_1, c_2\right)$ is the Euclidean distance between the axis-aligned bounding box for the point cloud associated with the row cluster $r_1$, and the axis aligned bounding box for the point cloud associated with the column cluster $c_2$. The quantity $\diam\left(r_1\right)$ is the diameter of the axis aligned bounding box for the point cloud associated with the row cluster $r_1$, and $\diam\left(c_2\right)$ is the analogous diameter associated with the column cluster $c_2$. The quantity $\eta$ is a scalar constant; we use $\eta=2.0$. Basically, if the point clouds associated with $r_1$ and $c_2$ are far away from each other relative to their diameters, then we expect that the corresponding block of the matrix will be low rank. This process is repeated for the other blocks to determine which blocks are admissible and which are not. For us, the diagonal blocks $\left(\AkerPcMat\right)_{11}$ and $\left(\AkerPcMat\right)_{22}$ are not admissible because the distance between a point cloud and itself is zero. 

Next, we subdivide all blocks that are not admissible and are larger than a predetermined size (we use size $32 \times 32$), using the same process that was used to subdivide $\AkerPcMat$. But now we subdivide a block based on the two child row clusters and two child column clusters for the rows and columns of that block. This subdivision process continues recursively until all blocks are either admissible, or smaller than the predetermined size mentioned above. The resulting hierarchical partitioning of matrix blocks forms a tree, which is called the \emph{block cluster tree}. The root of the tree is the whole matrix, internal nodes in the tree are blocks that are subdivided, and the leaves of the tree are blocks that are either expected to be low rank, or are small.


\subsubsection{Adaptive cross low rank approximation of blocks}
\label{sec:adaptive_cross}

Once the block cluster tree has been constructed, low rank approximations of the admissible (low rank) blocks are formed using the adaptive cross method \cite{ACA}. Let $X \in \mathbb{R}^{m \times m}$ be an admissible block of $\AkerPcMat$. The idea of the adaptive cross method is to form a low rank approximation of $X$ by sampling a small number of rows and columns of $X$. 
\begin{itemize}
	\item Let $C \in \mathbb{R}^{m \times r}$ be a matrix consisting of a subset of $r$ columns of $X$, such that the span of the columns in $C$ approximates the column space of $X$. 
	\item Let $R\in\mathbb{R}^{r \times m}$ be a subset of the rows of $X$, such that the span of the rows in $R$ approximates the row space of $X$.
	\item Let $U \in \mathbb{R}^{r \times r}$ be the block of $X$ consisting of the intersection of the rows from $R$ with the columns from $C$.
\end{itemize}
Then it is well-established that
\begin{equation}
\label{eq:CUR}
X \approx C U^+ R,
\end{equation}
where $U^+$ is the pseudoinverse of $U$ \cite{CUR}. The quality of approximation \eqref{eq:CUR} depends on how well the columns of $C$ approximate the column space of $X$, and how well the rows of $R$ approximate the row space of $X$. 

In the adaptive cross method, ``good'' columns, $C$, and rows, $R$, are chosen via an alternating iterative process. An initial set of columns $C$ is chosen. Keeping $C$ fixed, a set of rows $R$ is chosen so that the so that determinant of the submatrix $U$ within $C$ is as large as possible. This may be done via either the maxvol procedure \cite{GOODSUBMATRIX}, or via QR factorization. Now, keeping $R$ fixed, a set of new columns $C$ is chosen so that the submatrix $U$ within $R$ is as large as possible. This process repeats a small number of times. This results in matrices $R$ and $C$ such that the error in the approximation, \eqref{eq:CUR}, is small. The dominant cost of this procedure is the cost of computing $kr$ columns of $X$ and $kr$ rows of $X$, where $k$ is the number of alternating iterations. There is also a small linear algebra overhead cost for the determinant maximization process that is performed at each step. For more details on adaptive cross low rank approximation, see \cite{ACA}. The key point is that the adaptive cross method allows us to form a rank-$r$ approximation of an $m \times m$ block of the matrix via a process that only requires accessing $O(mr)$ entries of that block.

We use the adaptive cross method to form low rank approximations for each admissible block of $\AkerPcMat$. We directly compute all entries of the small dense blocks of $\AkerPcMat$ that are not admissible. This process requires us to compute $O(r^2 N \log N)$ entries of $\AkerPcMat$, which is relatively cheap compared to the operator actions of $\Aop$ that are required to form the product-convolution approximation.



\section{Fast ellipsoid intersection test}
\label{sec:fast_ellipsoid_intersection_test}
The procedure for choosing sample points relies on quickly determining whether two ellipsoids intersect. Let $E_p$ and $E_q$ be the ellipsoids defined as
\begin{align*}
	E_p :=& \{x : (x - \mu_p)^T \Sigma_p^{-1} (x - \mu_p) \le \tau^2\} \\
	E_q :=& \{x : (x - \mu_q)^T \Sigma_q^{-1} (x - \mu_q) \le \tau^2\}, \\
\end{align*}
where $\mu_p, \mu_q \in \mathbb{R}^d$, and $\Sigma_p, \Sigma_q \in \mathbb{R}^{d \times d}$ are positive definite. Let $K$ be the following one dimensional convex function:
\begin{equation*}
	K(s) := 1 - \frac{1}{\tau^2} (\mu_p - \mu_q)^T \left(\frac{1}{1-s}\Sigma_p + \frac{1}{s}\Sigma_q\right)^{-1}(\mu_p - \mu_q)	
\end{equation*}
In \cite{ELLIPSOIDINTERSECT} it is shown that $E_p \cap E_q = \{\}$ if and only if $K(s) < 0$ for some $s\in (0,1)$. We check whether $E_p$ and $E_q$ intersect by minimizing $K(s)$ on $(0,1)$. If $K(s^*) <0$ at the minimizer $s^*$, then $E_p \cap E_q = \{\}$. Otherwise $E_p \cap E_q \neq \{\}$.

The function $K(s)$ may be evaluated quickly for many $s$ by pre-computing the solution to the generalized eigenvalue problem
\begin{equation*}
	\Sigma_p P = \Sigma_q P \Lambda,
\end{equation*}
where $P \in \mathbb{R}^{d \times d}$ is the matrix of generalized eigenvectors (which may be non-orthogonal), and $\Lambda = \diag(\lambda_1,\lambda_2,\dots,\lambda_d)$ is the diagonal matrix of generalized eigenvalues $\lambda_i$. The matrix $P$ simultaneously diagonalizes $\Sigma_p$ and $\Sigma_q$, in the sense that $P^T\Sigma_p P = \Lambda$, and $P^T\Sigma_q P = I$, where $I$ is the $d \times d$ identity matrix. Using this diagonalization, and some algebraic manipulations, we may write $K(s)$ as
\begin{equation}
\label{eq:Ks_generalized}
K(s) = 1 - \frac{1}{\tau^2} \sum_{i=1}^d \frac{s(1-s)}{1 + s (\lambda_i - 1)}v_i^2,
\end{equation}
where $v := \Phi^T\left(\mu_p - \mu_q\right)$. We compute the generalized eigenvalue decomposition of $\Sigma_p$ and $\Sigma_q$, then minimize $K(s)$ in the form \eqref{eq:Ks_generalized} on the interval $(0,1)$ using Brent's algorithm \cite{BRENT} (any fast 1 dimensional convex optimization routine may be used). The resulting algorithm for checking whether $E_p$ and $E_q$ intersect is summarized in Algorithm \ref{alg:ellipsoid_intersection_test}.

\begin{algorithm2e}
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwProg{Fn}{Function}{}{}
	\Input{Ellipsoid $E_p$ with mean $\mu_p$ and covariance $\Sigma_p/\tau^2$\\
		Ellipsoid $E_q$ with mean $\mu_q$ and covariance $\Sigma_q/\tau^2$ 
	}
	
	\Output{Boolean $\text{ellipsoids\_intersect}$ which is true if $E_p \cap E_q \neq \{\}$ and false otherwise}
	
	
	Solve generalized eigenvalue problem $\Sigma_p \Phi = \Sigma_q \Phi \Lambda$
		
	$v \gets \Phi^T\left(\mu_p - \mu_q\right)$
		
	$\displaystyle K^* \gets \min_{s \in (0,1)}~1 - \frac{1}{\tau^2} \sum_{i=1}^d \frac{s(1-s)}{1 + s (\lambda_i - 1)}v_i^2$
		
	\If{$K^* < 0$}{
			
		$\text{ellipsoids\_intersect} \gets \text{False}$
			
	}
	\Else{
			
		$\text{ellipsoids\_intersect} \gets \text{True}$
			
	}
	\caption{Determining whether two ellipsoids intersect}
	\label{alg:ellipsoid_intersection_test}
\end{algorithm2e}


\section{Solving for rational function scalars}
\label{app:solve_rational_system}

Using the definition of $\ratfct$ in \eqref{eq:rational_function_scalar}, and basic calculus and linear algebra, we may write Equations \eqref{eq:desired_property_1}, \eqref{eq:desired_property_2}, and \eqref{eq:desired_property_4} in the following matrix form:
\begin{equation}
\label{eq:rational_3_linear_system}
\begin{bmatrix} 1 & 0 & 1/\mu \\ 0 & 1 & -1/\mu^2 \\ 1 & \lambda_\text{max} & 1/(\lambda_\text{max}+\mu) \end{bmatrix} \begin{bmatrix} c_0 \\ c_1 \\ c_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ \lambda_\text{max} \end{bmatrix},
\end{equation}
and we may write Equation \eqref{eq:desired_property_3} as follows: 
\begin{equation}
\label{eq:rational_extra_eq}
c_0 + c_1 \lambda_\text{min} + \frac{c_2}{\lambda_\text{min}+\mu} - |\lambda_\text{min}| = 0.
\end{equation}
Let 
\begin{equation*}
g(\mu) := c_0(\mu) + c_1(\mu) \lambda_\text{min} + \frac{c_2(\mu)}{\lambda_\text{min}+\mu} - |\lambda_\text{min}|
\end{equation*}
denote the left hand side of \eqref{eq:rational_extra_eq}, but with $c_0(\mu)$, $c_1(\mu)$, and $c_2(\mu)$ being the solution of \eqref{eq:rational_3_linear_system} for the given value of $\mu$. I.e., within $g(\mu)$ the quantities $c_0$, $c_1$, and $c_2$ are no longer free variables; they now depend on $\mu$ implicitly through the solution of a linear system. We solve the combined system, \eqref{eq:rational_3_linear_system} and $\eqref{eq:rational_extra_eq}$, by solving the one-dimensional nonlinear root finding problem
\begin{equation}
\label{eq:g_mu_zero}
g(\mu)=0
\end{equation}
with bound $\mu > |\lambda_\text{min}|$ and initial guess $\mu_0 = 2 |\lambda_\text{min}|$. Each evaluation of $g$ requires solving the $3 \times 3$ linear system \eqref{eq:rational_3_linear_system}, and once the solution $\mu$ is found, we solve \eqref{eq:rational_3_linear_system} one more time to get the final values of $c_0$, $c_1$, and $c_2$.



\section{Proofs}
\label{app:proofs}

%These results rely on straightforward prerequisites that we present in Lemma \ref{lem:T_is_linear}. We prove correctness of Algorithm \ref{alg:weighting_functions_incremental} in Theorem \ref{thm:wi_alg_is_correct}. Theorem \ref{thm:wi_alg_is_correct} relies on a prerequisite result which we present in Lemma \ref{lem:psi_delta_eval}.

\begin{defn}[Weighting function optimization problem]
	Let $\Omega \subset \mathbb{R}^d$, $d\in\{1,2,3\}$, be an open bounded domain with, let $x_i \in \Omega$ for $i=1,\dots,r$, and let $f \in \mathbb{R}^r$. We consider the optimization problem
	\begin{equation}
	\label{eq:generic_optimization_initial}
	\begin{aligned}
	\min_{\gamma \in H^2_N(\Omega)} &\quad \frac{1}{2}\norm{\Delta \gamma}_{L^2(\Omega)}^2 \\
	\text{such that} &\quad \gamma(x_i) = f_i, \quad i=1,\dots,r.
	\end{aligned}
	\end{equation}
	A minimizer, $\gamma^*$, of \eqref{eq:generic_optimization_initial} is a function defined on $\Omega$ that interpolates the data pairs $(x_i,f_i)$, and is as flat as possible in a least squares sense. 
	We denote the objective function for optimization problem \eqref{eq:generic_optimization_initial} by 
	\begin{equation}
	\label{eq:wi_objective}
	J(\gamma) := \frac{1}{2}||\Delta \gamma||^2_{L^2(\Omega)}.
	\end{equation}
	We denote the feasible set for optimization problem \eqref{eq:generic_optimization_initial} by 
	\begin{equation}
	\label{eq:wi_feasible_set}
		X := \{u \in H^2_N : u(x_i)=f_i~\text{for}~ i=1,\dots,r\}.
	\end{equation}
	We denote the tangent space to the feasible set by
	\begin{equation}
	\label{eq:wi_tangent_space}
	X_0 := \{u \in H^2_N : u(x_i)=0~\text{for}~ i=1,\dots,r\}.
	\end{equation}
\end{defn}

\begin{lem}[Elliptic regularity on $X_0$]
	\label{lem:elliptic_regularity_x0}
	If $\partial \Omega$ is $C^{1,1}$, then there exists a constant $C > 0$ such that
	\begin{equation*}
		||u||_{H^2(\Omega)} \le C ||\Delta u||_{L^2(\Omega)}
	\end{equation*}
	for all $u \in X_0$.
\end{lem}


\begin{proof}
	Let $\widetilde{u}$ be version of $u$ that has been shifted up or down by a constant such that $\int_\Omega \overline{u} = 0$. We have
	\begin{align*}
		||D^1 u||_{\left(L^2(\Omega)\right)^d}^2 + ||D^2 u||_{\left(L^2(\Omega)\right)^{d \times d}}^2 &= ||D^1 \widetilde{u}||_{\left(L^2(\Omega)\right)^d}^2 + ||D^2 \widetilde{u}||_{\left(L^2(\Omega)\right)^{d \times d}}^2\\
		&\le ||\widetilde{u}||_{H^2(\Omega)}^2 \\
		&\le C_1 ||\Delta \widetilde{u}||_{L^2(\Omega)}^2 \\
		&= C_1 ||\Delta u||_{L^2(\Omega)}^2
	\end{align*}
	for some constant $C_1$. In the first and fourth lines we used the fact that the derivative of a constant is zero, in the second line we used the definition of the $H^2$ norm, and in the third line we used the standard elliptic regularity result for pure Neumann problems (see, e.g., Theorem 8.13 (b) in \cite{Arbogast}).
	
	To complete the proof, we will now show that there exists a constant $C_2$ such that $||u||_{L^2(\Omega)}^2 \le C_2 ||\Delta u||_{L^2(\Omega)}^2$ for all $u \in X_0$. For this, we use a standard compactness argument, which closely follows the proof of Theorem 8.12 in \cite{ARBOGAST}. Suppose for the sake of contradiction that this result does not hold. Then there exists a sequence $\{u_k\}_{k=1}^\infty \subset X_0$ such that
	$||u_k||_{L^2(\Omega)}=1$ and $||\Delta u_k||_{L^2(\Omega)} \le 1/k$.
	By the result already proven, the sequence $u_k$ is bounded in $H^2(\Omega)$ as follows:
	\begin{align*}
	||u_k||_{H^2(\Omega)}^2 &= ||u_k||_{L^2(\Omega)}^2 + ||D^1 u||_{\left(L^2(\Omega)\right)^d}^2 + ||D^2 u||_{\left(L^2(\Omega)\right)^{d \times d}}^2 \\
	&\le ||u_k||_{L^2(\Omega)}^2 + C_1 ||\Delta u||_{L^2(\Omega)}^2 \le 1 + C_1.
	\end{align*}
	Therefore, by the Banach-Alaoglu theorem, there exists a subsequence $u_{k_j}$ and function $u \in X_0$ such that
	\begin{equation*}
		u_{k_j} \overset{w}{\rightharpoonup} u \quad \text{weakly in }H^2(\Omega).
	\end{equation*}
	Let $f \in L^2(\Omega)$. By the Cauchy-Schwarz inequality, the linear functional $v \mapsto \int_\Omega f(x) \Delta v(x) dx$ is bounded in $H^2(\Omega)$, and is therefore an element of $\left(H^2(\Omega)\right)^*$. Weak convergence therefore implies
	\begin{equation*}
		\int_\Omega f(x) \Delta u_k(x) dx \rightarrow \int_\Omega f(x) \Delta u(x) dx.
	\end{equation*}
	By the assumption $||\Delta u_k||_{L^2(\Omega)} \le 1/k$ and Cauchy-Schwarz, we also have 
	\begin{equation*}
	\int_\Omega f(x) \Delta u_k(x) dx \rightarrow 0,
	\end{equation*}
	so we must have
	\begin{equation*}
		\int_\Omega f(x) \Delta u(x) dx = 0.
	\end{equation*}
	Since this holds for all $f \in L^2(\Omega)$, by the fundamental lemma of the calculus of variations, $\Delta u(x) = 0$ almost everywhere in $\Omega$. This, combined with the fact that $u$ has Neumann zero boundary conditions, implies that $u$ is a constant function. Since $u$ has value zero at least one point in each connected component of $\Omega$, we have $u=0$. Note that pointwise values of $u$ are well-defined since the Sobolev embedding theorem imples that $H^2(\Omega)$ is continuously imbedded in the space of continuous functions $C^0(\Omega)$ when $d < 4$, and we are considering $d \in \{1,2,3\}$ (see, e.g., Theorem 7.19 in \cite{Arbogast}). 

	On the other hand, by the Rellich-Kondrachov theorem, $H^2(\Omega)$ is compactly imbedded in $L^2(\Omega)$, so there must exist a sub-sub-sequence, $u_{k_{j_i}}$ such that 
	\begin{equation*}
		u_{k_{j_i}} \rightarrow u
	\end{equation*}
	strongly in $L^2(\Omega)$. By assumption $||u_{k_{j_i}}||_{L^2(\Omega)}=1$, so $||u||_{L^2(\Omega)}=1$. This contradicts the result $u=0$ that we previously derived, so we must conclude that there exists a constant $C_2 > 0$ such that
	\begin{equation*}
		||u||_{L^2(\Omega)} \le C_2 ||\Delta u||_{L^2(\Omega)}.
	\end{equation*}
	for all $u \in X_0$. Combining this with out previous result, we have
	\begin{equation*}
	||u||^2_{H^2(\Omega)} \le (C_1 + C_2)||\Delta u||^2
	\end{equation*}
	for all $u \in X_0$, which completes the proof.
\end{proof}

\begin{lem}[Well-posedness of weighting function optimization problem]
	\label{lem:wi_existence_and_uniqueness}
	There exists a unique solution to optimization problem \eqref{eq:generic_optimization_initial}.
\end{lem}

\begin{proof}
	By the Sobolev Imbedding Theorem (see, e.g., Theorem 7.20 in \cite{ARBOGAST}), $H^2_N(\Omega)$ is continuously imbedded in $C^0(\Omega)$, so pointwise evaluation $u \mapsto u(x_i)$ of functions at a point $x_i \in \Omega$ is a continuous linear functional on $H^2_N$. The set 
	\begin{equation}
	\label{eq:inverse_image_of_point}
	\{u \in H^2_N : u(x_i)=f_i\}
	\end{equation}
	is therefore a closed affine set in $H^2_N$, because it is the inverse image of a point under a continuous linear functional. Hence the feasible set $X$, defined in \eqref{eq:wi_feasible_set}, is a closed affine set in $H^2_N$, because it is the intersection of the sets in \eqref{eq:inverse_image_of_point} for $i=1,\dots,r$. 
	
	The feasible set $X$ is convex because it is affine, and it is non-empty because interpolating the data pairs $\{(x_i, f_i)\}_{i=1}^r$ using any smooth interpolation method yields an element of $X$. Furthermore, the objective function, 
	\begin{equation}
	J(\gamma) := \frac{1}{2}||\Delta \gamma||^2_{L^2(\Omega)},
	\end{equation}
	is convex because it is the composition of a convex function with a linear map. Furthermore, the elliptic regularity result in Lemma \ref{lem:elliptic_regularity_x0} implies
	\begin{equation*}
		J''(\gamma)v^2 = ||\Delta v||^2_{L^2(\Omega)} \ge C ||v||_{H^2(\Omega)}^2
	\end{equation*}
	for all $v \in X_0$, which implies that $J$ is strongly convex on $X$.
\end{proof}

\begin{lem}[Linearity of weighting function solution operator]
	\label{lem:T_is_linear}
	Let $\mathcal{T}:\mathbb{R}^r \rightarrow H^2(\Omega)$ denote the solution operator for optimization problem \eqref{eq:generic_optimization_initial}	as a function of $f$. That is, 
		\begin{equation}
		\label{eq:opt_soln_operator}
		\mathcal{T}(f) := \gamma^*,
		\end{equation}
		where $\gamma^*$ is the minimizer of \eqref{eq:generic_optimization_initial}. The operator $\mathcal{T}$ is a linear operator.
\end{lem}

\begin{proof}
	asdf
\end{proof}


\begin{proof}[Proof of Theorem \ref{thm:smoothest_interpolant}]
	Let $f = (\varphi_1(y), \varphi_2(y), \dots, \varphi_r(y))$ and let $e_i = (0,\dots,0,1,0,\dots,0)$ be the length $r$ vector with $i^\text{th}$ component equal to one and all other components equal to zero. The solution to optimization problem \eqref{eq:uy_optimization_problem} is given by
	\begin{align*}
	\mathcal{T}(f) &= \varphi_1(y) \mathcal{T}(e_1) + \varphi_2(y) \mathcal{T}(e_2) + \dots +\varphi_r(y) \mathcal{T}(e_r) \\
	&= \varphi_1(y) w_1 + \varphi_2(y) w_2 + \dots +\varphi_r(y) w_r \\
	&= \gamma_y.
	\end{align*}
	In the first line we used linearity of the solution map, $\mathcal{T}$ (Lemma \ref{lem:T_is_linear}), in the second line we used the fact that $w_i = \mathcal{T}(e_i)$, and in the third line we used the definition of $\gamma_y$ in \eqref{eq:u_defn_in_thm}.
\end{proof}

\begin{proof}[Proof of Proposition \ref{thm:wi_sum_to_one}]
	By linearity of $\mathcal{T}$ (Lemma \ref{lem:T_is_linear}), we have
	\begin{equation*}
	\sum_{i=1}^r w_i(x) = \sum_{i=1}^r \mathcal{T}(e_i)(x) = \mathcal{T}\left(\sum_{i=1}^r e_i\right)(x) = \mathcal{T}(\mathbf{1})(x) = 1,
	\end{equation*}
	where $\mathbf{1} = (1,1,\dots,1)$ is the length $r$ vector which has all components equal to one. The last equality ($\mathcal{T}(\mathbf{1})(x) = 1$) holds because the constant function $C \in H^2(\Omega)$, $C(x)=1$, satisfies the constraint of optimization problem \eqref{eq:generic_optimization_initial} with $f = \mathbf{1}$, and yields the minimum possible value of zero for the objective function.
\end{proof}

\begin{lem}[Preparation for Theorem \ref{thm:wi_alg_is_correct}]
	\label{lem:psi_delta_eval}
	We have
	\begin{equation}
	\label{eq:inline_wi_lemma}
	h(x_j) = \left(\psi_j, \Delta h\right)_{L^2(\Omega)} + \avg(h)
	\end{equation}
	for all $h\in H^2_N(\Omega)$, where
	\begin{equation*}
	\avg(h) :=	 \frac{1}{|\Omega|} \int_\Omega h ~dx,
	\end{equation*}
	and $|\Omega| = \int_\Omega 1 ~dx$ is the Lebesgue measure of $\Omega$.
\end{lem}

\begin{proof}
	The result follows from multiplying \eqref{eq:psi_eq} by $h$, integrating both sides of the equation over $\Omega$, performing integration by parts to move derivatives from $\psi_j$ onto $h$, and using the Neumann zero boundary conditions for $\psi_j$ and $h$ to eliminate boundary integral terms.
\end{proof}

\begin{proof}[Proof of Theorem \ref{alg:weighting_functions_incremental}]
	For notational convenience, in this proof we write $w$, $\lambda$, and $e$ instead of $w_i$, $\lambda_i$, and $e_i$, respectively. Here subscripts for $\lambda$, and $e$ instead denote components of those vectors. For example, here $\lambda_j$ denotes the $j^\text{th}$ component of the vector $\lambda$, which corresponds to the $j^\text{th}$ component of the vector $\lambda_i$ in Algorithm \ref{alg:weighting_functions_incremental}.
	
	From standard optimization theory, the solution to optimization problem \ref{eq:wi_optimization_problem} is the stationary point of the Lagrangian,
	\begin{equation*}
	\mathcal{L}(w,\lambda) = \frac{1}{2}\|\Delta w\|^2 + \sum_{j=1}^r \lambda_j\left(w(x_j) - e_j\right).
	\end{equation*}
	The plan of this proof is to show that $\nabla \mathcal{L}(w,\lambda)=0$, which will verify that $w$ solves optimization problem \ref{eq:wi_optimization_problem}.
	
	Using the definition of $w_i$ in \eqref{eq:definition_of_wi_alg}, we may write the gradient of $\mathcal{L}$ with respect to $\lambda_j$ as
	\begin{equation*}
	\nabla_{\lambda_j} \mathcal{L}(w, \lambda) = w(x_j) - e_j
	= \beta - \sum_{k=1}^r \lambda_k \theta_k(x_j) - e_j.
	\end{equation*}
	Using Lemma \ref{lem:psi_delta_eval} with $h=\theta_k$, the equation for $\theta_k$ in \eqref{eq:theta_eq}, and the definition of $S$ in \eqref{eq:defn_of_S_alg}, we have
	\begin{align*}
	\sum_{k=1}^r \lambda_k \theta_k(x_j) &= \sum_{k=1}^r \lambda_k \left(\psi_j, \Delta \theta_k\right)_{L^2(\Omega)}\\
	&= \sum_{k=1}^r \lambda_k \left(\psi_j, \psi_k\right)_{L^2(\Omega)} = \left(S \lambda\right)_j.
	\end{align*}
	Combining all of these components of $\nabla_\lambda \mathcal{L}$ into a single vector and using the definition of $\lambda$ in \eqref{eq:definition_of_mu_alg} yields
	\begin{align*}
	\nabla_\lambda \mathcal{L}(w, \lambda) &= \beta \mathbf{1} - e - S \lambda \\
	&= \beta \mathbf{1} - e - S \left(-S^{-1}\left(e - \beta \mathbf{1}\right)\right) = 0.
	\end{align*}
	
	Taking the directional derivative of $\mathcal{L}$ with respect to $w$ in direction $h$ and using Lemma \ref{lem:psi_delta_eval} yields
	\begin{align}
	\nabla_w \mathcal{L}(w, \lambda)h &= \left(\Delta w, \Delta h\right)_{L^2(\Omega)} + \sum_{j=1}^r \lambda_j h(x_j) \nonumber \\
	&= \left(\Delta w, \Delta h\right)_{L^2(\Omega)} + \sum_{j=1}^r \lambda_j \left(\psi_j, \Delta h\right)_{L^2(\Omega)} + \lambda_j \avg(h) \nonumber \\
	&= \left(\Delta w + \sum_{j=1}^r \lambda_j \psi_j,~ \Delta h\right)_{L^2(\Omega)} + \avg(h) \mathbf{1}^T \lambda. \label{eq:L_w_38}
	\end{align}
	Using the definition of $w$ in Algorithm \ref{alg:weighting_functions_incremental}, and the fact that the Laplacian of a constant function is zero, we have
	\begin{equation*}
	\Delta w = -\sum_{j=1}^r \lambda_j \Delta \theta_j 
	= -\sum_{j=1}^r \lambda_j \psi_j,
	\end{equation*}
	which implies that the first term in \eqref{eq:L_w_38} is zero. By the definition of $\lambda$ in \eqref{eq:definition_of_mu_alg} and $\beta$ in \eqref{eq:definition_of_alpha_alg}, we have
	\begin{align*}
	\mathbf{1}^T \lambda &= -\mathbf{1}^T S^{-1} \left(e - \beta \mathbf{1}\right) \\
	&= -\mathbf{1}^T S^{-1} e + \beta \mathbf{1}^T S^{-1} \mathbf{1} \\
	&= -\mathbf{1}^T S^{-1} e + \frac{\mathbf{1}^T S^{-1} e}{\mathbf{1}^T S^{-1} \mathbf{1}} \mathbf{1}^T S^{-1} \mathbf{1} = 0,
	\end{align*}
	which implies that the second term in \eqref{eq:L_w_38} is zero. Thus $\nabla_w \mathcal{L}(w, \lambda) = 0$, so Algorithm \ref{alg:weighting_functions_incremental} produces the solution to optimization problem \ref{eq:wi_optimization_problem}.
\end{proof}


\begin{defn}[Moments of $\phi_x$]
	\label{defn:alpha_rho_mu_sigma}
	We define the spatially varying scaling factor, $\alpha$, the scaled impulse response, $\widehat{\phi}_x$, the spatially varying mean, $\mu$, and the spatially varying covariance, $\Sigma$, as follows:
	\begin{align*}
	\alpha:\Omega \rightarrow \mathbb{R}, &\qquad \alpha(x) := \int_{\Omega} \phi_x(z) dz, \\
	\widehat{\phi}_x:\Omega \rightarrow \mathbb{R}, &\qquad \widehat{\phi}_x := \phi_x \big/ \alpha(x), \\
	\mu:\Omega \rightarrow \mathbb{R}^d, &\qquad \mu(x) := \int_\Omega z \widehat{\phi}_x(z) dz, \\
	\Sigma:\Omega \rightarrow \mathbb{R}^{d \times d}, &\qquad\Sigma(x) := \int_\Omega (z - \mu(x))(z - \mu(x))^T \widehat{\phi}_x(z) dz.
	\end{align*}
\end{defn}


\begin{proof}[Proof of Theorem \ref{thm:vol_mean_cov}]
	Let $v \in L^2$ be arbitrary. We have
	\begin{align*}
		\left(v, \alpha\right)_{L^2(\Omega)} =& \int_\Omega v(x) \int_{\Omega} \langle\mathcal{A},\delta_x\rangle^*(z) dz dx\\
		=& \int_\Omega v(x) \int_{\Omega} A(z,x) dz dx \\
		=& \int_\Omega \int_\Omega v(x) A(z,x) C(z) dz dx \\
		=& \left(\mathcal{A}^T C\right)(v),
	\end{align*}
	which implies $\alpha = \left(\mathcal{A}^TC\right)^*$.
	
	Using similar techniques, we have
	\begin{align*}
		\left(v, \alpha \cdot \mu^i\right)_{L^2(\Omega)} =& \int_\Omega v(x) \alpha(x) \int_{\Omega} z^i \langle\mathcal{A},\delta_x\rangle^*(z)/\alpha(x) dz dx\\
		=& \int_\Omega \int_\Omega v(x) L^i(z) A(z,x) dz dx \\
		=& \left(\mathcal{A}^T L^i\right)(v),
	\end{align*}
	which implies $\mu^i = \left(\mathcal{A}^T L^i\right)^* / \alpha$. 
	
	Using the identity
	\begin{equation*}
		\Sigma(x) = \int_\Omega zz^T \widehat{\phi}_x(z) dz - \mu(x) \mu(x)^T,
	\end{equation*}
	we have
	\begin{align*}
		\left(v, \alpha \cdot \left(\Sigma^{ij}+\mu^i \cdot \mu^j\right)\right)_{L^2(\Omega)} =& \int_\Omega v(x) \alpha(x) \int_\Omega z^i z^j \langle\mathcal{A},\delta_x\rangle^*(z) / \alpha(x) dz dx \\
		=& \int_\Omega \int_\Omega v(x) Q^{ij}(z) A(z,x) dz dx \\
		=& \left(\mathcal{A}Q^{ij}\right)(v),
	\end{align*}
	which implies $\Sigma^{ij} = \left(\mathcal{A}^T Q^{ij}\right)^* / \alpha - \mu^i\cdot\mu^j$.
	
\end{proof}

\section{Background: distributions}
\label{app:distributions}

Let $\overline{\Omega}$ be the closure of $\Omega$, and let $C(\overline{\Omega})$ be the space of continuous functions mapping $\overline{\Omega}\rightarrow \mathbb{R}$. The action of $\mathcal{A}$ is extended to distributions $\mu:C\left(\overline{\Omega}\right) \rightarrow \mathbb{R}$ via the formula
\begin{equation*}
\langle\mathcal{A},\mu\rangle(v) := \int_\Omega v(y) \mu\left(A(y, \cdot)\right) dy, 
\end{equation*}
where $A(y,\cdot)$ is the function $x \mapsto A(y,x)$. This is derived formally as follows:
\begin{align*}
\langle\mathcal{A},\mu\rangle(v) &= \int_\Omega \int_\Omega v(y) A(y,x) \textrm{``} \mu(x) \textrm{''} dx dy \\
&= \int_\Omega v(y) \int_\Omega A(y,x) \textrm{``} \mu(x) \textrm{''} dx \\
&= \int_\Omega v(y) \mu\left(A(y,\cdot)\right) dx.
\end{align*}
For example, the delta distribution $\delta_x$ is defined by $\delta_x(v) = v(x)$, and the action of $\mathcal{A}$ on $\delta_x$ is given by
\begin{equation}
\label{eq:appendix_impulse_response}
\langle\mathcal{A},\delta_x\rangle(v)	= \int_\Omega v(y) A(y,x) dy.
\end{equation}
Recall that the impulse response of $\mathcal{A}$ to $\delta_x$, denoted $\phi_x$, is the Riesz representation of $\langle\mathcal{A},\delta_x\rangle$. From \eqref{eq:appendix_impulse_response} and the definition of the Riesz representation, we have $\phi_x = \langle\mathcal{A},\delta_x\rangle^* = A(~\cdot~,x)$. That is, $\phi_x$ is the function $y \mapsto A(y,x)$.


\section{Discretization}
\label{sec:discretization}

Let $f_i$, $i=1,\dots,N$ be a set of finite element basis functions, let $h$ denote the mesh size parameter for the finite element mesh, and let
\begin{equation*}
V_h = \Span\left(f_1, f_2, \dots, f_N\right)
\end{equation*}
be the corresponding  finite element space with the $L^2$ inner product. Functions $u\in L^2(\Omega)$ are approximated by functions $u_h \in V_h$. In turn, functions $u_h\in V_h$ are represented in computations by length $N$ arrays $\mathbf{u}$, such that the array entries of $\mathbf{u}$ are the coordinates of $u_h$ in the finite element basis. That is,
\begin{equation}
\label{eq:finite_bijection}
u_h(x) = \sum_{i=1}^N \mathbf{u}_i f_i(x).
\end{equation}

We write $\mathbf{M} \in \mathbb{R}^{N \times N}$ to denote the finite element \emph{mass matrix}, which has entries
\begin{equation*}
\mathbf{M}_{ij} = \left(f_i, f_j\right)_{L^2(\Omega)} = \int_{\Omega} f_i(x) f_j(x) dx,
\end{equation*}
and we write $\mathbb{R}^N_\mathbf{M}$ to denote the space $\mathbb{R}^N$ with the matrix-weighted inner product
\begin{equation*}
\left(\mathbf{u},\mathbf{v}\right)_\mathbf{M} := \mathbf{v}^T\mathbf{M} \mathbf{u}.
\end{equation*}
Direct calculation shows that $V_h$ is isometrically isomorphic $\mathbb{R}^N_\mathbf{M}$. That is, equation \eqref{eq:finite_bijection} establishes a bijection between functions $u_h \in V_h$ and coefficient vectors $\mathbf{u} \in \mathbb{R}^N_\mathbf{M}$, and this bijection preserves the inner product in the sense that $\left(u_h, v_h\right)_{L^2(\Omega)} = \left(\mathbf{u},\mathbf{v}\right)_\mathbf{M}$. Likewise, the dual space, $V_h'$, is isometrically isomorphic to $\mathbb{R}^N_{\mathbf{M}^{-1}}$. 

Here vectors in $\mathbf{u} \in \mathbb{R}^N_{\mathbf{M}}$ and $\boldsymbol{\sigma} \in \mathbb{R}^N_{\mathbf{M}^{-1}}$ are viewed as column vectors, and $\mathbf{u}^T$ and $\boldsymbol{\sigma}^T$ are the row vectors that result from taking the standard matrix transposes of $\mathbf{u}$ and $\boldsymbol{\sigma}$, respectively. The discrete dual of a vector $\mathbf{v}$ with respect to the $\mathbf{M}$ inner product is written as $\mathbf{v}^*$, and $\mathbf{v}^*$ is viewed as a column vector.
%These isomorphisms allow us to identify functions $u_h \in V_h$ with their corresponding coefficient arrays $\mathbf{u} \in \mathbb{R}^N_\mathbf{M}$, and linear functionals $\sigma_h \in V_h'$ with their corresponding coefficient arrays $\boldsymbol{\sigma} \in \mathbb{R}^N_{\mathbf{M}^{-1}}$.


\subsection{Discretized operations}
\label{app:discretized_operations}

Here we list the function space operations on $V_h$ and $V_h'$ that are required for this paper, and show how to perform the corresponding matrix and vector operations on $\mathbb{R}^N_{\mathbf{M}}$ and $\mathbb{R}^N_{\mathbf{M}^{-1}}$.

%We summarize coefficient vector versions of the function space operation that are used in this paper in Appendix \ref{app:discretized_operations}.
%
%As described in Section \ref{sec:discretization}, a function $u_h$ in a finite element space $V_h$ may be written in terms of its vector of coefficients, $\mathbf{u} \in \mathbb{R}^N_\mathbf{M}$, with respect the finite element basis $\{\phi_i\}_{i=1}^N$. Recall that $\mathbf{M}$ is the mass matrix. We have
%\begin{equation*}
%	u(x) \approx u_h(x) = \sum_{i=1}^N \mathbf{u}_i \phi_i(x).
%\end{equation*}
%The finite element function space $V_h$ is isometrically isomorphic to the coefficient space $\mathbb{R}^N_\mathbf{M}$, and the dual space $V_h'$ is isometrically isomorphic to $\mathbb{R}^N_{\mathbf{M}^{-1}}$. In computations, we perform operations with coefficient vectors in $\mathbb{R}^N_\mathbf{M}$ and coefficient dual vectors in $\mathbb{R}^N_{\mathbf{M}^{-1}}$, rather than functions in $V_h$ and functionals in $V_h'$. Here we describe how to perform common function space operations in terms of coefficient vectors and coefficient dual vectors.

\begin{description}
	\item[Coefficients of a functional:] The coefficient dual vector, $\boldsymbol{\sigma} \in \mathbb{R}^N_{\mathbf{M}^{-1}}$, corresponding to a linear functional $\sigma_h \in V_h'$ has components given by
	\begin{equation*}
		\boldsymbol{\sigma}_i = \sigma_h(f_i), \quad i=1,\dots,N.
	\end{equation*}
	\item[Action of a functional:] The action of a linear functional $\sigma_h \in V_h'$ on a function $u_h \in V_h$ may be computed as follows:
	\begin{equation*}
		\sigma_h(v_h) = \boldsymbol{\sigma}^T \mathbf{u}.
	\end{equation*}
	\item[Riesz representation:] Let $\sigma_h \in V_h'$. The Riesz representation of $\sigma_h$ is the unique  function $\sigma_h^* \in V_h$ satisfying 
	\begin{equation*} 
	\left(\sigma_h^*, v_h\right)_{L^2(\Omega)} = \sigma_h(v_h) \quad\forall v_h \in V_h.
	\end{equation*}
	The coefficient dual vector $\boldsymbol{\sigma}^*$ corresponding to $\sigma_h$ is given by
	\begin{equation*} 
		\boldsymbol{\sigma}^* = \mathbf{M}^{-1} \boldsymbol{\sigma}.
	\end{equation*}
	\item[Finite element $L^2$ projection:] Let $g \in L^2(\Omega)$. The orthogonal projection of $g$ onto $V_h$ with respect to the $L^2$ inner product is the unique function $g^\pi_h \in V_h$ satisfying
	\begin{equation*}
		\left(g^\pi_h, v_h\right)_{L^2(\Omega)} = \left(g, v_h\right)_{L^2(\Omega)} \quad\forall v_h \in V_h.
	\end{equation*}
	 Let $\left(\mathbf{g}^\pi\right)^* \in \mathbb{R}^N_{\mathbf{M}^{-1}}$ be the vector with components
	\begin{equation*}
		\left(\mathbf{g}^\pi\right)^*_i = \int_\Omega g(x) f_i(x) dx, \quad i=1,\dots,N.
	\end{equation*}
	The coefficient vector $\mathbf{g}^\pi \in \mathbb{R}^N_{\mathbf{M}}$ corresponding to $g^\pi_h$ is given by
	\begin{equation*}
		\mathbf{g}^\pi = \mathbf{M}^{-1}\left(\mathbf{g}^\pi\right)^*.
	\end{equation*}
	\item[Finite element interpolation:] Let $p_i \in \mathbb{R}^d$, $i=1,\dots,N$, be the Lagrange nodes associated with the finite element basis functions $f_i$. The finite element interpolant of a function $g \in C\left(\overline{\Omega}\right)$ is the function $g^I \in V_h$ given by
	\begin{equation*}
		g_h^I(x) = \sum_{i=1}^N g(p_i) f_i(x).
	\end{equation*}
	The coefficient vector, $\mathbf{g}$, for $g_h^I$ has components 
	\begin{equation*}
	\mathbf{g}_i^I = g(p_i), \quad i=1.,\dots,N.
	\end{equation*}
	\item[Projection vs. interpolation:]
	Typically, $g^\pi$ is a more accurate approximation of $g$ than $g^I$, but $g^I$ is cheaper to compute than $g^\pi$ because computing $g^\pi$ requires solving a linear system, while computing $g^I$ does not. If the finite element basis satisfies the following Kronecker property,
	\begin{equation*}
		f_i(p_j) = \begin{cases}1 & i=j \\
		0 & \text{otherwise},\end{cases}
	\end{equation*}
	then $g^I(p_i) = g(p_i)$ for $i=1,\dots,N$ (i.e., the interpolant equals the original function at the Lagrange nodes). Interpolation is typically sufficiently accurate for practical purposes if the Kronecker property is satisfied. Projection should be used if the Kronecker property is not satisfied.
	\item[Matrix representation of an operator:] Let $\mathcal{B}_h : V_h \rightarrow V_h'$ be a linear operator, and let $\mathbf{B} \in \mathbf{R}^{N \times N}$ be the matrix with entries
	\begin{equation*}
		\mathbf{B}_{ij} = \left(\mathcal{B}_h f_j\right)(f_i).
	\end{equation*}
	The matrix $\mathbf{B}$ maps $\mathbb{R}^N_\mathbf{M} \rightarrow \mathbb{R}^N_{\mathbf{M}^{-1}}$ by matrix multiplication, and
	\begin{equation*}
		\sigma_h = \mathcal{B}_h u_h \quad \Leftrightarrow \quad \boldsymbol{\sigma} = \mathbf{B} \mathbf{u}.
	\end{equation*}
	\item[Transpose:] The matrix $\mathbf{B}^T$ maps $\mathbb{R}^N_\mathbf{M} \rightarrow \mathbb{R}^N_{\mathbf{M}^{-1}}$ via matrix multiplication, and
	\begin{equation*}
		\sigma_h = \mathcal{B}_h^T u_h \quad \Leftrightarrow \quad \boldsymbol{\sigma} = \mathbf{B}^T \mathbf{u}.
	\end{equation*}

	\item[Distributions:] Let $\mu:C\left(\overline{\Omega}\right)\rightarrow \mathbb{R}$ be a distribution. If $\mu(f_i)$ is well-defined for $i=1,\dots,N$, then the restriction of $\mu$ to domain $V_h$ is a linear functional $\mu_h\in V_h'$. The functional $\mu_h$ has a coefficient dual vector $\boldsymbol{\mu} \in \mathbb{R}^N_{\mathbf{M}^{-1}}$ with entries 
	\begin{equation*}
	\boldsymbol{\mu}_i=\mu(f_i), \quad i=1,\dots,N.
	\end{equation*} 
	For example, let $\delta_p$ be the Dirac distribution centered at a point $p\in\Omega$. If the finite element basis functions, $f_i$, are continuous at $p$, then restricting the domain of $\delta_p$ to the space $V_h$ yields a linear functional in $V_h'$. The corresponding coefficient dual vector, $\boldsymbol{\delta}_p$, has entries 
	\begin{equation*}
	\left(\boldsymbol{\delta}_p\right)_i = f_i(p), \quad i=1,\dots,N.
	\end{equation*}
	
	\item[Action of an operator on a distribution:] Since $V_h$ is finite-dimensional, the restriction, $\mu_h \in V_h$, of a distribution, $\mu$, to the space $V_h$ has a a Riesz representation $\mu_h^* \in V_h$. Let $B_h$ be the integral kernel associated with an operator $\mathcal{B}_h:V_h \rightarrow V_h'$. For $v_h \in V_h$, we have
	\begin{align*}
		\langle \mathcal{B}_h, \mu_h \rangle(v) &= \int_\Omega v_h(y) \mu_h\left(B_h(y, \cdot)\right) dy \\
		&= \int_\Omega v_h(y) \left(\int_\Omega B_h(y,x) \mu_h^*(x) dx\right) dy \\
		&= \int_\Omega \int_\Omega v_h(y) B_h(y,x) \mu_h^*(x) dx dy = \left(\mathcal{B}_h \mu_h^*\right)(v).
	\end{align*}
	Going from the first to the second line we used the definition of the Riesz representation. The action of $\mathcal{B}_h$ on $\mu_h$ in the sense of distributions is therefore equal to the action of $\mathcal{B}_h$ on $\mu^*_h$ in the conventional sense. We have
	\begin{equation*}
		\sigma_h = \langle \mathcal{B}_h, \mu_h \rangle \quad \Leftrightarrow \quad \sigma_h = \mathcal{B}_h \mu_h^* \quad \Leftrightarrow \quad \boldsymbol{\sigma} = \mathbf{B} \mathbf{M}^{-1} \boldsymbol{\mu}.
	\end{equation*}
\end{description}

\subsection{Discretized mean and covariance estimation} 

\begin{algorithm2e}
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	{	
		\Input{Operator $\mathbf{A}$}
		\Output{$\boldsymbol{\alpha}\in \mathbb{R}^N$, $\boldsymbol{\mu}\in \mathbb{R}^{d \times N}$, and $\boldsymbol{\Sigma} \in \mathbb{R}^{d \times d \times N}$}
		
		Form vector $\mathbf{C} \in \mathbb{R}^N_{\mathbf{M}}$ by either projecting or interpolating constant function $C(x)=1$ onto $V_h$
		
		$\boldsymbol{\alpha} = \left(\mathbf{M}^{-1}\mathbf{A}^T \mathbf{C}\right)^*$
		
		\For{$i=1,2,\dots,d$}{
			Form vector $\mathbf{L}^i \in \mathbb{R}^N_{\mathbf{M}}$ by either projecting or interpolating linear function $L^i(x) = x^i$ onto $V_h$
			
			$\mathbf{\mu}^i = \left(\mathbf{M}^{-1}\mathbf{A}^T \mathbf{L}^i\right)^* / \boldsymbol{\alpha}$
		}
		\For{$i=1,2,\dots,d$}{
			\For{$j=1,\dots,i$}{
				Form quadratic function $\mathbf{Q}^{ij} = x^i x^j$
				
				$\boldsymbol{\Sigma}^{ij} = \left(\mathbf{M}^{-1}\mathbf{A}^T \mathbf{Q}^{ij}\right)^* / \boldsymbol{\alpha} - \boldsymbol{\mu}^i\cdot \boldsymbol{\mu}^j$
				
				$\boldsymbol{\Sigma}^{ij} = \boldsymbol{\Sigma}^{ji}$
				
			}
		}
		
	}
	\caption{Discretized version of Algorithm \ref{alg:varhpi_mean_cov}}
	\label{alg:varhpi_mean_cov_discrete}
\end{algorithm2e}


In computations, $\alpha$, $\mu^{i}$, and $\Sigma^{ij}$ are replaced with finite element approximations $\alpha_h, \mu_h^i, \Sigma_h^{ij}\in V_h$, respectively, which have coefficient vectors $\boldsymbol{\alpha}, \boldsymbol{\mu}^{ij}, \boldsymbol{\Sigma}^{ij}\in\mathbb{R}^N_\mathbf{M}$, respectively. The functions $C$, $L^i$, and $Q^{ij}$ are replaced with either finite element projections onto $V_h$ (if the finite element basis does not satisfy the Kronecker property), or finite element interpolations onto $V_h$ (if the finite element basis satisfies the Kronecker property). In our numerical examples we use finite element basis functions that satisfy the Kronecker property, and we use finite element interpolations. The coefficient vectors corresponding to either the interpolations or projections are denoted $\mathbf{C}$, $\mathbf{L}^i$, and $\mathbf{Q}^{ij}$, respectively. The discretized versions of \eqref{eq:vol_mean_var_thm1}, \eqref{eq:vol_mean_var_thm2}, and \eqref{eq:vol_mean_var_thm3} are
\begin{align*}
\boldsymbol{\alpha} &= \mathbf{M}^{-1}\mathbf{A}^T\mathbf{C} \\
\boldsymbol{\mu}^i &= \left(\mathbf{M}^{-1}\mathbf{A}^T \mathbf{L}^i\right) / \boldsymbol{\alpha}\\
\boldsymbol{\Sigma}^{ij} &= \left(\mathbf{M}^{-1}\mathbf{A}^T \mathbf{Q}^{ij}\right) / \boldsymbol{\alpha} - \boldsymbol{\mu}^i\cdot \boldsymbol{\mu}^j,
\end{align*}
respectively. Here $\mathbf{f} \cdot \mathbf{g}$ and $\mathbf{f} / \mathbf{g}$ denote the component-wise multiplication and division of vectors, respectively.


\subsection{Discretized sampling of impulse responses}
In computations, the Dirac comb coefficient dual vector for $\xi_h^b$ is the vector $\boldsymbol{\xi}^b \in \mathbb{R}^N_{\mathbf{M}^{-1}}$ which has entries
\begin{equation*}
\boldsymbol{\xi}^b_i = \sum_{x_j \in S_b} f_i(x_j), \quad i=1,\dots,N.
\end{equation*}
The Dirac comb impulse response coefficient vector for $\eta_h^b$ is the vector $\boldsymbol{\eta}^b \in \mathbb{R}^N_\mathbf{M}$ given by
\begin{equation*}
\boldsymbol{\eta}^b = \mathbf{M}^{-1} \mathbf{A}^T \mathbf{M}^{-1}\boldsymbol{\xi}^b.
\end{equation*}


\section{DISORGANIZED OLD STUFF}




\begin{defn}
	\label{def:stiffness_mass}
	We define $\mathcal{K}:H^2(\Omega)\rightarrow L^2(\Omega)'$ and $\mathcal{M}:L^2(\Omega) \rightarrow L^2(\Omega)'$ to be the following linear operators:
	\begin{align*}
	\left(\mathcal{K}u\right)(v) :=& \int_\Omega v \Delta u ~dx \\
	\left(\mathcal{M}u\right)(v) :=& \int_\Omega u v ~dx,
	\end{align*}
	and $\mathcal{P}:H^2(\Omega)\rightarrow \mathbb{R}^r$ to be the following pointwise observation operator:
	\begin{equation*}
	(\mathcal{P}u)_i = u(x_i)
	\end{equation*}
	for $i=1,\dots,r$.
\end{defn}

\begin{prop}
	\label{prop:solution_operator_is_linear}
	Let $f \in \mathbb{R}^r$ be arbitrary. The optimization problem
	\begin{equation}
	\label{eq:generic_optimization_initial_old}
	\begin{aligned}
	\min_{u \in H^2(\Omega)} &\quad \frac{1}{2}\norm{\Delta u}_{L^2(\Omega)}^2 \\
	\text{such that} &\quad u(x_i) = f_i, \quad i=1,\dots,r
	\end{aligned}
	\end{equation}
	may be rewritten as follows:
	\begin{equation}
	\label{eq:generic_optimization}
	\begin{aligned}
	\min_{u \in H^2(\Omega)} &\quad \frac{1}{2} \left(\mathcal{K}^T\mathcal{M}^{-1}\mathcal{K}u\right)(u) \\
	\text{such that} &\quad 
	\mathcal{P} u = f.
	\end{aligned}
	\end{equation}
\end{prop}

\begin{prop}
	There exists a unique solution to optimization problem \eqref{eq:generic_optimization}. The solution to \eqref{eq:generic_optimization}, $u^*$, and the adjoint variable for enforcing the constraint, $\lambda^*$, solve the following Karun-Kush-Tucker (KKT) system:
	\begin{equation}
	\label{eq:weighting_kkt}
	\begin{bmatrix}
	\mathcal{K}^T\mathcal{M}^{-1}\mathcal{K} & \mathcal{P}^T \\
	\mathcal{P} & 0
	\end{bmatrix}
	\begin{bmatrix}
	u^* \\ \lambda^*
	\end{bmatrix}
	=
	\begin{bmatrix}
	0 \\ f
	\end{bmatrix},
	\end{equation}
	where we use block-matrix notation to denote block-operators.
\end{prop}

\begin{cor}
	\label{cor:linearity_of_solution_operator}
	Let $\mathcal{T}:\mathbb{R}^r \rightarrow H^2(\Omega)$ denote the solution operator for optimization problem \eqref{eq:generic_optimization}	as a function of $f$. That is, 
	\begin{equation*}
	\mathcal{T}(f) := u^*,
	\end{equation*}
	where $u^*$ is the minimizer of \eqref{eq:generic_optimization}. The operator $\mathcal{T}$ is a linear operator.
\end{cor}


\footnote{The reason for this dependence on spatial dimension is related to the fact that the fundamental solution of the biharmonic equation is continuous in $3$ or less dimensions, but singular in $4$ or more dimensions. In the unusual scenario where one is inverting for a parameter field in $4$ or more spatial dimensions, one could use a modified Poisson interpolation where $\Delta$ is replaced with $\Delta^\alpha$ in the objective function of \eqref{eq:wi_optimization_problem}, where $\alpha>1$ is chosen so that the fundamental solution of $\Delta^{2\alpha}$ is continuous.}


The \emph{point spread function} (PSF) of $\mathcal{A}$ at $x\in \Omega$ is
\begin{equation*}
\varphi_x(y) := \langle \mathcal{A},\delta_x\rangle^*(y+x) = A(y+x,x).
\end{equation*}
Here $\langle \mathcal{A}, \delta_x \rangle \in L^2(\Omega)'$ is the result of applying $\mathcal{A}$ to a point source $\delta_x$ (delta distribution) centered at $x$, and $\langle\mathcal{A},\delta_x\rangle^* \in L^2(\Omega)$ is the Riesz representation of $\langle\mathcal{A},\delta_x\rangle$. The PSF, $\varphi_x$, is the result of translating $\langle\mathcal{A},\delta_x\rangle^*$ to re-center it at zero instead of $x$. Recall the Riesz representation of a linear functional $\psi \in L^2(\Omega)'$ is the unique vector $\psi^* \in L^2(\Omega)$ satisfying $\left(\psi^*, v\right)_{L^2(\Omega)} = \psi(v)$ for all $v \in \L^2(\Omega)$.


The approximation $\widetilde{A}(y,x)$ may not be accurate if both $x$ and $y$ are near the boundary. Other approximation methods, if available, should be considered for the block of the operator associated with boundary-boundary interactions. Nevertheless, in our numerical results we see that approximation works well for the the inverse problem considered, without any modifications to the boundary-boundary block.

The solution to this system of equations is found by the following process:
\begin{enumerate}
	\item Compute 
	\begin{equation*}
	\Psi := K^+ E^T
	\end{equation*}
	by solving Neumann poisson problems for point source right hand sides, with point sources located at the points $x_i$, $i=1,\dots,q$.
	\item Compute 
	\begin{equation*}
	\Theta := K^+ M \Psi
	\end{equation*}
	by solving Neumann Poisson problems with the columns of $\Psi$ as distributed sources.
	\item Form 
	\begin{equation*}
	S = \Psi^T M \Psi
	\end{equation*}.
	\item Compute \begin{equation*}
	\alpha = \left(\mathbf{1}^T S^{-1} \mathbf{1}\right)^{-1}\mathbf{1}^T S^{-1} \mathbf{y}.
	\end{equation*}
	\item Compute 
	\begin{equation*}
	\boldsymbol{\lambda} = -S^{-1} \left(\mathbf{y} - \alpha \mathbf{1} \right).
	\end{equation*}
	\item Compute
	\begin{equation*}
	\mathbf{u} = -\Theta \boldsymbol{\lambda} + \alpha \mathbf{c}
	\end{equation*}
\end{enumerate}


Consider the Pine island glacier geometry shown in Figure REF. Regions of the glacier are separated by thin inlets of ocean. With radial basis function interpolation, points on one side of the inlet inappropriately influence the interpolation on the other side of the inlet. With Poisson interpolation, the PSF samples only influence the interpolation on their own side of the inlet.


\subsection{Computation of Poisson weighting functions}
\label{sec:computation_of_weighting_functions}

After discretization, optimization problem \eqref{eq:wi_optimization_problem} becomes
\begin{equation}
\label{eq:discrete_poisson_optimization_1}
\begin{aligned}
\min_{\mathbf{w}} &\quad \frac{1}{2}\mathbf{w}^T K^T M^{-1} K \mathbf{w} \\
\text{such that} &\quad E \mathbf{w} = \mathbf{d}.
\end{aligned}
\end{equation}
Here $\mathbf{w}$ is the discretization of $w_i$, $K$ is the stiffness matrix for the discretized Poisson problem with pure Neumann boundary conditions, $M$ is the mass matrix, and $\mathbf{d}=(0,\dots,0,1,0,\dots,0)$ is the length-$q$ vector with $i^\text{th}$ entry one and all other entries zero. Furthermore, $E$ is the pointwise observation matrix defined by $\left(E\mathbf{u}\right)_j = u(x_j)$, where $u$ denotes the function represented by the vector $\mathbf{u}$. Here we dropped the subscripts $i$ from variables for notational convenience. One must solve $q$ optimization problems of the form \eqref{eq:discrete_poisson_optimization_1}; one for each $i=1,\dots,q$. 

In the remainder of this section we will derive an efficient algorithm for solving \eqref{eq:discrete_poisson_optimization_1}. The algorithm (Algorithm REF) is based on explicit formula for the solution of \eqref{eq:discrete_poisson_optimization_1} which we present in Proposition \ref{prop:poisson_interpolation_formula}. The formulas in Proposition \ref{prop:poisson_interpolation_formula} rely on certain matrices that we define in Definition \ref{defn:poisson_interpolation_matrices}. The primary computational cost of the algorithm is the solution of Poisson problems with pure Neumann boundary conditions.

\begin{defn}
	\label{defn:poisson_interpolation_matrices}
	Let 
	\begin{align*}
	\Psi :=& K^+ E^T, \\
	\Theta :=& K^+ M \Psi \\
	S :=& \Psi^T M \Psi,
	\end{align*}
	where $K^+$ is the Moore-Penrose pseudoinverse of $K$. Further, let $\mathbf{c}$ dente the discretization of the constant function $c(x)=1$ on $\Omega$, and let $\mathbf{1}:=(1,\dots,1)$ denote the vector of length $q$ with all entries equal to one.
\end{defn}

\begin{remark}
	Computing $\mathbf{g} = K^+\mathbf{f}$ is equivalent to solving the discretized Poisson problem with $f$ as the right hand side source, with pure Neumann boundary conditions, and with the constraint that the solution has average value equal to zero. That is,
	\begin{equation}
	\label{eq:pure_neumann_poisson}
	\mathbf{g} = K^+ \mathbf{f} \quad \Leftrightarrow \quad 
	\begin{cases}
	\Delta g = f, & \text{in }\Omega,\\
	\nu \cdot \nabla g = 0, & \text{on }\partial \Omega,\\
	\int_\Omega g(x) dx = 0,
	\end{cases}
	\end{equation}
	where $\nu$ denotes the normal to the boundary.
	
	Hence, the $i^\text{th}$ column of $\Psi$ is the result of solving the discretized pure Neumann Poisson problem, with a point source at location $x_i$. The $i^\text{th}$ column of $\Theta$ is the result of solving the discretized pure Neumann Poisson problem with distributed source given by the $i^\text{th}$ column of $\Psi$.
\end{remark}

\begin{prop}
	\label{prop:poisson_interpolation_formula}
	The solution to optimization problem \eqref{eq:discrete_poisson_optimization_1} is given by
	\begin{equation*}
	\mathbf{w} = -\Theta \boldsymbol{\lambda} + \alpha \mathbf{c},
	\end{equation*}
	where
	\begin{align*}
	\alpha =& \left(\mathbf{1}^T S^{-1} \mathbf{1}\right)^{-1}\mathbf{1}^T S^{-1} \mathbf{d} \\
	\boldsymbol{\lambda} =& -S^{-1} \left(\mathbf{d} - \alpha \mathbf{1} \right). \\
	\end{align*}
\end{prop}

\begin{proof}
	Let
	\begin{equation*}
	v := \mathcal{K} w_i,
	\end{equation*}
	and for the remainder of this proof let $e := e_i$.
	Since the null-space of the Laplacian with pure Neumann boundary conditions is the set of all constant functions, we have
	\begin{equation}
	\label{eq:definition_of_w}
	w_i = \mathcal{K}^+ v + \alpha C
	\end{equation}
	for some scalar $\alpha$.
	
	Writing optimization problem \eqref{eq:wi_optimization_problem} in terms of $v$ and $\alpha$ instead of $w_i$ yields the following equivalent optimization problem:
	\begin{equation}
	\label{eq:discrete_poisson_optimization_2}
	\begin{aligned}
	\min_{v \in L^2(\Omega)', \alpha \in \mathbb{R}} &\quad \frac{1}{2} v\left(\mathcal{M}^{-1} v\right) \\
	\text{such that} &\quad \mathcal{P}\mathcal{K}^+ v + \alpha \mathbf{1} = e.
	\end{aligned}
	\end{equation}
	The Lagrangian for optimization problem \eqref{eq:discrete_poisson_optimization_2} is
	\begin{equation*}
	\mathcal{L} = \frac{1}{2}v\left( \mathcal{M}^{-1} v\right) + \lambda^T\left(\mathcal{P}\mathcal{K}^+ v + \alpha \mathbf{1} - e\right).
	\end{equation*}
	The solution to this optimization problem is the stationary point of the Lagrangian, i.e., the point at which the gradient of the Lagrangian is zero:
	\begin{equation*}
	0 = \nabla \mathcal{L} = 
	\begin{bmatrix}
	\nabla_{v} \mathcal{L} \\
	\nabla_{\lambda} \mathcal{L} \\
	\nabla_\alpha \mathcal{L}
	\end{bmatrix} 
	= \begin{bmatrix}
	\mathcal{M}^{-1} v + \left(\mathcal{K}^+\right)^T \mathcal{P}^T \lambda \\
	\mathcal{P}\mathcal{K}^+v + \alpha \mathbf{1} - e \\
	\mathbf{1}^T \lambda
	\end{bmatrix}.
	\end{equation*}
	This equation may be rewritten as the following block $3 \times 3$ linear system:
	\begin{equation*}
	\begin{bmatrix}
	\mathcal{M}^{-1} & \left(\mathcal{K}^+\right)^T \mathcal{P}^T & 0 \\
	\mathcal{P}\mathcal{K}^+ & 0 & \mathbf{1} \\
	0 & \mathbf{1}^T & 0
	\end{bmatrix}
	\begin{bmatrix}
	v \\ \lambda \\ \alpha
	\end{bmatrix}
	=
	\begin{bmatrix}
	0 \\ e \\ 0
	\end{bmatrix}.
	\end{equation*}
	Performing block Gaussian elimination on this system allows us to reduce the system to the following block triangular form
	\begin{equation*}
	\begin{bmatrix}
	\mathcal{M}^{-1} & \left(\mathcal{K}^+\right)^T \mathcal{P}^T & 0 \\
	& -S & \mathbf{1} \\
	0 & 0 & \mathbf{1}^T S^{-1} \mathbf{1}
	\end{bmatrix}
	\begin{bmatrix}
	v \\ \lambda \\ \alpha
	\end{bmatrix}
	=
	\begin{bmatrix}
	0 \\ e \\ \mathbf{1}^T S^{-1} e
	\end{bmatrix}
	\end{equation*}
	where we recall that $S := \Psi^T \mathcal{M} \Psi = \mathcal{P}\mathcal{K}^+ \mathcal{M} \left(\mathcal{K}^+\right)^T \mathcal{P}^T$. From this block triangular system, we read off the solution as:
	\begin{align*}
	\alpha =& \left(\mathbf{1}^T S^{-1} \mathbf{1}\right)^{-1}\mathbf{1}^T S^{-1} \mathbf{d} \\
	\boldsymbol{\lambda} =& -S^{-1} \left(\mathbf{d} - \alpha \mathbf{1} \right) \\
	\mathbf{v} =& -M K^+ E^T \boldsymbol{\lambda}
	\end{align*}
	Substituting these results into the definition of $\mathbf{w}$ in \eqref{eq:definition_of_w}, we have
	\begin{equation*}
	\mathbf{w} = -K^+ M K^+ E^T \boldsymbol{\lambda} + \alpha \mathbf{c} = -\Theta \boldsymbol{\lambda} + \alpha \mathbf{c},
	\end{equation*}
	as required.
\end{proof}

We now design an incremental algorithm for constructing interpolants $\mathbf{u}$, in which we can add points $x_i$ more efficiently. Let
\begin{align*}
\Psi :=& K^+ E^T \\
\Theta :=& K^+ M \Psi
\end{align*}
so that 
\begin{equation*}
S = \Psi^T M \Psi
\end{equation*}
and
\begin{equation*}
\mathbf{u} = -\Theta \boldsymbol{\lambda} + \alpha \mathbf{c}
\end{equation*}
The columns of $\Psi$ are the responses of $A^+$ to point sources (delta distributions) centered at the sample points $x_i$. The columns of $\Theta$ are the responses of $A^+M^{-1} A$ to these point sources. The matrices $\Psi$ and $\Theta$ may be constructed incrementally as new points are added. 


\section*{Acknowledgments}
We thank J.J. Alger, Longfei Gao, and Rami Nammour for helpful discussions.

\bibliographystyle{siamplain}
\bibliography{references}
\end{document}
