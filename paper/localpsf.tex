% SIAM Article Template
\documentclass[review,onefignum,onetabnum]{siamart190516}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{localpsf_shared}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={Fast matrix-free approximation of smoothly varying blur operators, with application to Hessians in PDE-constrained inverse problems with highly informative data},
  pdfauthor={N. Alger, N. Petra, and O. Ghattas}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

\externaldocument{localpsf_supplement}

% FundRef data to be entered by SIAM
%<funding-group specific-use="FundRef">
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
  We present an efficient matrix-free method for approximating locally translation invariant operators that have locally supported non-negative integral kernels. The idea of the method is to compute impulse responses of the operator at a collection of scattered points, then interpolate these computed impulse responses to approximate impulse responses at other arbitrary points. This results in a product-convolution approximation, which we convert to hierarchical matrix format in order to solve linear systems and perform other matrix operations efficiently using fast hierarchical matrix arithmetic. Impulse responses are computed by applying the operator to a small number of Dirac comb ``batches'' of point sources. A key innovation of our method is a matrix-free procedure for choosing as many points as possible per batch, while ensuring that the supports of the impulse responses within each batch do not overlap. We apply the method to approximate Hessians in large-scale PDE-constrained inverse problem with highly informative data. Numerical results demonstrate that our method substantially outperforms existing state-of-the-art Hessian approximation methods which are based on low-rank approximation. Our method is able to form high quality approximations of high rank Hessians using only a small number of Hessian matrix-vector products.
\end{abstract}

% REQUIRED
\begin{keywords}
  example, \LaTeX
\end{keywords}

% REQUIRED
\begin{AMS}
  68Q25, 68R10, 68U05
\end{AMS}

\section{Introduction}

We present a fast matrix free method for approximating locally translation-invariant operators, $\mathcal{A}$, that have locally supported non-negative integral kernels. Such operators arise, for example, in Schur complements methods for solving partial differential equations (PDEs) \cite{TOBYPAPER}, as Poincare-Steklov operators, as covariance operators, as blurring operators in imaging problems, and as Hessians in distributed parameter PDE-constrained inverse problems.

Let $\Omega \subset \mathbb{R}^d$ be a bounded domain in dimension $d=1$, $2$, or $3$, and let $\mathcal{A}:L^2(\Omega)\rightarrow L^2(\Omega)'$ be an integral operator of the form
\begin{equation}
\label{eq:kernel_representation}
(\mathcal{A}u)(v) := \int_\Omega \int_\Omega v(y) A(y,x) u(x) dx dy,
\end{equation}
with integral kernel $A:\Omega \times \Omega \rightarrow \mathbb{R}$. Here $\mathcal{A}u \in L^2(\Omega)'$ is the linear functional that results from applying $\mathcal{A}$ to $u\in L^2(\Omega)$, and $\left(\mathcal{A}u\right)(v)$ is the scalar that results from applying that linear functional to $v \in L^2(\Omega)$. We focus on operators of this form which have the following properties:
\begin{description}
\item[Matrix-free:] One cannot easily access kernel entries $A(y,x)$. Access to $\mathcal{A}$ is available only through application of $\mathcal{A}$ and $\mathcal{A}^T$ to arbitrary functions (i.e., operator actions). That is, evaluation of the maps
\begin{equation*}
	u \mapsto\mathcal{A}u \quad \text{and} \quad v \mapsto\mathcal{A}^Tv.
\end{equation*} 
\item[Non-negative kernel:] For all $(y,x) \in \Omega \times \Omega$, we have
\begin{equation*}
	A(y,x) \ge 0.
\end{equation*}
\item[Local support:] Let $\phi_x$ be the following \emph{impulse response}:
\begin{equation}
\label{eq:impulse_response_defn}
\phi_x(y) := A(y, x).
\end{equation} 
We say that $\mathcal{A}$ has local support if, for all $x\in \Omega$, the support of $\phi_x$ is contained (or approximately contained) in a neighborhood of $x$. The smaller these supports are, the better our algorithm will perform.
\item[Local translation invariance:] If $h$ is not too large, then
\begin{equation}
	\label{eq:local_translation_invariance}
	A(y+h, x+h) \approx A(y,x).
\end{equation}
\end{description}

After discretization (see Appendix \ref{app:discretized_operations}), the operator $\mathcal{A}$ becomes a dense matrix, $\mathbf{A}$, that is typically too large to be built and stored. The first property is called ``matrix-free'' because at the discrete level it means that one can perform matrix-vector products of $\mathbf{A}$ and $\mathbf{A}^T$ with arbitrary vectors, but one cannot easily access matrix entries $\mathbf{A}_{ij}$. In the matrix-free regime, to compute $\mathbf{A}_{ij}$ one could compute $\mathbf{v} = \mathbf{A}\mathbf{e}_j$, then extract $\mathbf{A}_{ij} = \mathbf{v}_i$. However, this process is wasteful because one computes the entire vector $\mathbf{v}$, then discards all but one component. Here $\mathbf{e}_j=(0,\dots,0,1,0,\dots,0)$ is the unit vector which has $j^\text{th}$ component equal to one and all other components equal to zero. In general, $\mathbf{A}$ is matrix-free if $\mathbf{A}$ is defined implicitly via its action on vectors, and the process of applying $\mathbf{A}$ to a vector requires performing a nontrivial computational procedure, such as solving a large linear system, or timestepping. For example, in reduced space approaches to PDE-constrained optimization and inverse problems, Hessians information is available only through the application of the Hessian to vectors, and one such Hessian application requires solving two PDEs \cite{HESSIANADJOINT}. 

Let $\delta_x$ denote the point source (delta distribution) centered at $x$. The function $\phi_x$ is called the impulse response because straightforward analysis shows that it may be expressed as follows:
\begin{equation}
\label{eq:impulse_response_delta_action}
	\phi_x = \left\langle \mathcal{A}, \delta_x \right\rangle^*,
\end{equation}
where $\left\langle \mathcal{A}, \delta_x \right\rangle \in L^2(\Omega)'$ denotes the result of applying the operator $\mathcal{A}$ to the distribution $\delta_x$, and $\left\langle \mathcal{A}, \delta_x \right\rangle^* \in L^2(\Omega)$ denotes the Riesz representation of $\langle\mathcal{A}, \delta_x\rangle$ with respect to the $L^2$ inner product.\footnote{Recall that the Riesz representative of a functional $\psi \in L^2(\Omega)'$ with respect to the $L^2$ inner product is the unique function $\psi^* \in L^2(\Omega)$ such that $\psi(v) = \left(\psi^*,v\right)_{L^2(\Omega)}$ for all $v \in L^2(\Omega)$.} Note that while the domain of $\mathcal{A}$ is defined as $L^2(\Omega)$, which does not contain $\delta_x$, the action of $\mathcal{A}$ may be extended to distributions in a natural way, as described in Appendix \ref{app:distributions}. 
%The action of $\mathcal{A}$ on a distribution $\mu$ is written as $\langle \mathcal{A}, \mu \rangle$. We use this angle bracket notation to distinguish the action of $\mathcal{A}$ on distributions from the action of $\mathcal{A}$ on functions.
%We take the Riesz representative of the linear functional $\left\langle \mathcal{A}, \delta_x \right\rangle \in L^2(\Omega)'$ to convert it into a function $\left\langle \mathcal{A}, \delta_x \right\rangle^* \in L^2(\Omega)$. 
After discretization, the process of applying $\mathcal{A}$ to the distribution $\delta_x$, then forming the Riesz representative of the resulting linear functional, amounts to computing $\boldsymbol{\phi}_x = \mathbf{M}^{-1}\mathbf{A} \mathbf{M}^{-1} \boldsymbol{\delta}_x$, where $\boldsymbol{\phi}_x$, $\mathbf{A}$, and $\boldsymbol{\delta}_x$ are discretized versions of $\phi_x$, $A$ and $\delta_x$, respectively, and $\mathbf{M}$ is a discretization of the $L^2$ inner product bilinear form (e.g., a mass matrix in the finite element context). From \eqref{eq:impulse_response_delta_action}, we see that the local support property means that the response of $\mathcal{A}$ to a point source at $x$ is zero (or small) at points $y$ that are far from $x$.

Local translation invariance is illustrated in Figure FIG. If $\mathcal{A}$ were perfectly translation invariant (i.e., if equality held in \eqref{eq:local_translation_invariance} for all $x$, $y$, $h$) then it is straightforward to show that $\mathcal{A}$ would be a convolution operator, and the convolution kernel, $\varphi_x$, would be given by
\begin{equation}
\label{eq:convolution_kernel}
	\varphi_x(y) := \phi_x(y+x).
\end{equation}
The convolution kernel $\varphi_x$ is the result of translating the impulse response $\phi_x$ to re-center it at zero instead of $x$. Locally translation invariant operators act like convolution operators locally, but the convolution kernel, $\varphi_x$, varies as $x$ changes. We therefore approximate $\mathcal{A}$ by a spatially varying weighted sum of convolution operators, $\widetilde{\mathcal{A}} \approx \mathcal{A}$, of the form
\begin{equation}
\label{eq:product_convolution}
	\left(\widetilde{\mathcal{A}}u\right)(v) := \left( v,~ \sum_{i=1}^r \varphi_i \ast \left(w_i \cdot u\right) \right)_{L^2(\Omega)},
\end{equation}
where $f \cdot g$ denotes pointwise multiplication, $f \ast g$ denotes convolution, and $\left(f, g\right)_{L^2(\Omega)}$ denotes the $L^2$ inner product on $\Omega$, for functions $f,g \in L^2(\Omega)$. To avoid nested subscripts, we write 
\begin{equation*}
\varphi_i := \varphi_{x_i} \quad \text{and} \quad \phi_i := \phi_{x_i},
\end{equation*}
with $\varphi_{x_i}$ defined in \eqref{eq:convolution_kernel} and $\phi_{x_i}$ defined in \eqref{eq:impulse_response_defn}, to denote local convolution kernels and impulse responses corresponding to a collection of points $\{x_i\}_{i=1}^r \subset \Omega$ scattered throughout the domain.  The functions $w_i$ are spatially varying weighting functions that are used to interpolate the convolution kernels $\varphi_i$. Our operator approximation method is defined by how we compute the convolution kernels, $\varphi_i$, how we choose the points, $x_i$, what weighting functions, $w_i$, that we use, and how we use the resulting product-convolution approximation to perform linear algebra operations such as solving linear systems. In Section \ref{sec:overview_intro} we summarize the answers to these questions; we will provide detailed answers to these questions in Section \ref{sec:method}.

Approximations of the form \eqref{eq:product_convolution} are known as \emph{product-convolution} approximations, because the action of each term in the sum consists of a pointwise product, followed by a convolution. The more locally translation invariant an operator is (i.e., the smaller the discrepancy between the left hand side and right hand side in \eqref{eq:local_translation_invariance}), the smaller the number of terms that are required in \eqref{eq:product_convolution} to achieve an accurate product-convolution approximation. 

\subsection{Overview of the method}
\label{sec:overview_intro}

We compute one ``batch'' of $\varphi_i$'s by applying $\mathcal{A}$ to a sum of point sources (Dirac comb) associated with a collection of points $x_i$ scattered throughout $\Omega$. The batch of points $x_i$ are chosen so that the support of $\phi_i$ and the support of $\phi_j$ do not overlap (or do not overlap much) if $i \neq j$. Because these supports do not overlap, we can post-process the response of $\mathcal{A}$ to the Dirac comb to recover the functions $\phi_i$, and therefore the functions $\varphi_i$, associated with all points $x_i$ in the batch---with one application of $\mathcal{A}$, we recover many $\varphi_i$. This is illustrated in Figure FIG. The process is repeated to get more batches of $\varphi_i$'s, until a desired number of batches is reached.

%First, we compute
%\begin{equation}
%\label{eq:eta_intro}
%\eta := \left\langle \mathcal{A}, \xi \right\rangle^*
%\end{equation}
%where 
%\begin{equation*}
%	\xi := \sum_{i=1}^l \delta_{x_i}
%\end{equation*}
%is the Dirac comb associated with point sources centered at a collection of scattered points $x_i$. The batch of points $x_i$ are chosen so that the support of $\phi_i$ and the support of $\phi_j$ do not overlap (or do not overlap much) if $i \neq j$. By linearity, and the expression for $\phi_x$ in \eqref{eq:impulse_response_delta_action}, we have
%\begin{equation*}
%\eta = \sum_{i=1}^l \phi_i.
%\end{equation*}
%We may therefore post-process $\eta$ 
%
%to recover $\phi_i$ for all of the points $x_i$ in the batch. Since the functions $\varphi_i$ are translated versions of the functions $\phi_i$, we also recover $\varphi_i$ for all points $x_i$ in the batch. This is illustrated in Figure FIG. The process is repeated to get more batches of $\varphi_i$'s, until a desired number of batches is reached. 

In order to choose the points $x_i$, we need to estimate the support of $\phi_i$ \emph{before} we compute it. The supports of the functions $\phi_x$ are estimated a-priori, for all $x \in \Omega$ simultaneously, via a procedure that involves applying $\mathcal{A}^T$ to a small number of specially chosen functions (three functions if $d=1$, six if $d=2$, ten if $d=3$), then post processing the results (Section \ref{eq:mean_and_covariance_estimation}). This procedure is the only part of the paper that requires non-negativity of the integral kernel. 

We choose the weighting functions, $w_i$, to be the solutions to the following optimization problems:
\begin{equation}
\label{eq:wi_optimization_problem_intro}
\begin{aligned}
\min_{w_i \in H^2_N(\Omega)} &\quad \frac{1}{2}\norm{\Delta w_i}_{L^2(\Omega)}^2 \\
\text{such that} &\quad w_i(x_j) = \delta_{ij}, \quad j=1,\dots,q.
\end{aligned}
\end{equation}
Here $\Delta$ is the Laplacian on $\Omega$, $H^2_N(\Omega)$ is the space of all functions in the Sobolev space $H^2(\Omega)$ with Neumann zero boundary conditions, and the right hand side of the constraint, $\delta_{ij}$, is the Kronecker delta, which takes value one if $i=j$ and zero if $i \neq j$. Since the Laplacian measures local deviation from flatness \cite{JACOBSEN}, the function $w_i$ is the flattest function, in a least-squares sense, that takes the value one at the sample point $x_i$, and the value zero at the other sample points $x_j$, $i \neq j$. Computing the weighting functions $w_i$ requires solving two Poisson PDEs per weighting function, and these PDEs are solved cheaply using multigrid. We call this interpolation scheme \emph{Poisson interpolation}. The advantage of Poisson interpolation over other interpolation methods, most notably radial basis function interpolation \cite{ESCANDE}, is that Poisson interpolation incorporates information about the domain geometry when constructing the weighting functions. This is illustrated in Figure FIG. 

Once the $\varphi_i$ and $w_i$ are computed, we convert $\widetilde{\mathcal{A}}$ to hierarchical matrix (H-matrix) format. Let $\widetilde{A}$ denote the integral kernel associated with $\widetilde{\mathcal{A}}$. While $A(y,x)$ is not easily computable, $\widetilde{A}(y,x)$ is given by the formula
\begin{equation}
\label{eq:kernel_entries}
	\widetilde{A}(y,x) = \sum_{i=1}^r \varphi_i(y-x) w_i(x).
\end{equation}
This follows from~\eqref{eq:product_convolution} and the fact that the convolution operator $u \mapsto \varphi_i \ast u$ has $(y,x)$ kernel entry given by $\varphi(y-x)$.
Formula~\eqref{eq:kernel_entries} allows us to construct a H-matrix representation of $\widetilde{\mathcal{A}}$ using the conventional adaptive cross H-matrix construction method, in which one forms low-rank approximations of blocks of the matrix by sampling rows and columns of those blocks \cite{HACA}. Once in H-matrix format, fast H-matrix arithmetic is used to invert $\widetilde{\mathcal{A}}$, or perform other useful matrix operations.


\subsection{Motivation: PDE constrained inverse problems with highly informative data}
\label{sec:PDE_hessian_motivation}

Our motivation for this work is approximation of Hessians in distributed parameter inverse problems governed by partial differential equations (PDEs). That is, inverse problems in which one seeks to reconstruct an unknown parameter field, $m$, from noisy observations, 
\begin{equation*}
y=f(m,u(m)) + \text{noise},
\end{equation*}
which depend on  a state variable $u$. In turn, $u$ depends on $m$ implicitly through the solution of a PDE, 
\begin{equation}
\label{eq:state_pde}
0=g(m,u).
\end{equation}
Here $u(m)$ denotes the solution of the PDE \eqref{eq:state_pde} as a function of $m$. In the deterministic approach to inverse problems, one typically finds $m$ as the solution to the minimization problem 
\begin{equation}
\label{eq:minimization_problem}
\min_m \quad \frac{1}{2}\|y - f(m,u(m))\|_W^2 + R(m),
\end{equation}
where $\|\cdot\|_W$ is weighted norm which depends on the noise covariance, and $R(m)$ is a regularization term. Approximation of the Hessian of the objective function, $\mathcal{H}$, allows for fast solution of \eqref{eq:minimization_problem} via Newton-type methods. Hessian approximations are also central to many methods for uncertainty quantification in Bayesian statistical approaches to the inverse problem, because $\mathcal{H}^{-1}$ locally approximates the Bayesian posterior covariance for $m$. 
%We only have access to $\mathcal{H}$ via its action on an arbitrary vectors $v$, i.e., evaluation of the map $v \mapsto \mathcal{H} v$. Evaluating $v \mapsto \mathcal{H} v$ is an expensive process that involves solving two auxiliary linear PDEs.

The most popular existing Hessian approximation methods are based on forming a low rank approximation of the data misfit term in the Hessian, or the data misfit term preconditioned by the prior term \cite{CCGOPAPERS}. Either the Lanczos method or the randomized singular value decomposition \cite{HMTRANDOM} are used to perform the low rank approximation using only matrix-vector products. These methods suffer from a ``data predicament''---if the data are highly informative about the unknown parameter, then the numerical rank of the data misfit term in the Hessian is large, so a large number of matrix-vector products are required to form the aforementioned low rank approximation. The ideal scenario from a scientific perspective (highly informative data) is the worst case scenario from a computational perspective (large computational cost) \cite{MYDISSERTATION}. Although the method we present in this paper is not applicable to all Hessians, it is applicable to several Hessians of practical interest. For these Hessians, our method offers a \emph{data-scalable} alternative to conventional low-rank Hessian approximation methods, because high-rank approximations of an operator can be formed using a small number of matrix-vector products.


\subsection{Existing work}

In general, product-convolution approximations take the form shown in \eqref{eq:product_convolution}, but $\varphi_i$ and $w_i$ may be arbitrary functions. Product-convolution approximations have been used in a wide variety of fields going back several decades \cite{PRODCONVLIST}.
For background on product-convolution approximations, we recommend reading the following papers: \cite{PRODCONVGOOD}. 

We build upon the class of product convolution approximations in which the functions, $\varphi_i$, are impulse responses of $\mathcal{A}$ to point sources at a collection of points $x_i$ \cite{PRODCONVLIST}. A popular choice for methods in this class is to choose the points $x_i$ to be nodes in a regular grid, and interpolate the functions $\varphi_i$ with piecewise linear interpolation \cite{NAGY}, or splines \cite{PRODCONVSPLINES}. With a regular grid, a large number of points $x_i$ is typically required to achieve an accurate product-convolution approximation. This is computationally prohibitive in our applications, because computing the functions $\varphi_i$ requires one matrix vector product with $\mathcal{A}$ for each point $x_i$, and these matrix vector products are computationally costly. In our previous work, we reduced the computational cost by starting with a coarse grid of points $x_i$, then adaptively refining the grid in the regions where the error in the approximation is large \cite{PRODCONVMYPAPER}. But even with adaptive refinement, many matrix vector products with $\mathcal{A}$ may be required. In this paper, rather than using adaptive refinement, we instead reduce the computational cost by picking points $x_i$ such that many $\varphi_i$ are computed with each matrix vector product. 

%We do this by taking advantage of the fact that the functions $\varphi_i$ are locally supported; existing methods typically do not require this local support property.

Our method of estimating the support of the functions $\varphi_i$ was inspired by resolution analysis in seismic imaging \cite{RESOLUTION}. In resolution analysis, $\mathcal{A}$ is the Hessian for a seismic inverse problem, and the width of $\varphi_p$ is used to estimate of the minimum length scale on which features of the parameter can be inferred near the point $p$. If the width of $\varphi_p$ is ten meters, then features of the parameter near $p$ can be accurately inferred from data if those features are larger than roughly ten meters in size. In \cite{RESOLUTION}, the width of $\varphi_p$ is estimated to be the local autocorrelation length of the function $\mathcal{A}^T \zeta$ near $p$, where $\zeta$ is a random noise funtion. In this paper, rather than probing $\mathcal{A}^T$ with random noise functions, we use specific constant, linear, and quadratic functions. We observe that our method estimates the support of $\varphi_p$ more accurately and reliably than resolution analysis, but our method requires that $\mathcal{A}$ has a positive integral kernel, while the resolution analysis method does not have this requirement.

Matrix-vector products with product-convolution approximations can be performed using the fast Fourier transform if the problem is discretized on a regular grid \cite{PRODCONVFFT}. The product-convolution approximation can then replace the original operator when using Krylov methods to solve linear systems with the original operator as the coefficient operator \cite{PRODCONVKRYLOV}. However, for complex geometries, problems are rarely discretized using regular grids, so we cannot easily use the fast Fourier transform. Furthermore, in big-data inverse problems the required number of Krylov iterations is large \cite{MYDISSERTATION,OTHERS}. Thus, after constructing the product-convolution approximation, it is desirable to convert the product-convolution approximation to other matrix formats that are amenable to fast linear algebra operations. Wavelet compression methods have been used for this purpose \cite{PRODCONVWAVELETS}. Here we follow our previous work \cite{PRODCONVMYPAPER}, in which we convert the product-convolution approximation to H-matrix format. 

Conventional H-matrix construction methods require access to matrix entries of the matrix being approximated, and therefore cannot be used to efficiently form H-matrix approximations of operators that are only available through matrix-vector products. There are matrix-free methods for H-matrix approximation \cite{LEXINGPEELINGPROCESS}, and these methods have been used to form H-matrix compressions of Hessians in PDE constrained inverse problems \cite{ILONAHMATRIX,NOEMIHMATRIX}. While these matrix-free H-matrix construction methods are asymptotically scalable in theory, the required number of matrix vector products can be large in practice. 


Product-convolution type approximations of Hessians have been used in a seismic inverse problem in \cite{GEORGSEISMICPRODCONV}, and in an advection-diffusion inverse problem in \cite{PRODCONVMYPAPER}. In \cite{DEMANETSEISMIC}, matrix probing \cite{MATRIXPROBING} is used to approximate the Hessian in a seismic inverse problem as the sum of simple pseudodifferential operators. Although it is not explicitly mentioned, the approximation in \cite{DEMANETSEISMIC} could be interpreted as a convolution-product interpolation, which is like a product-convolution approximation, except the order of pointwise multiplications and convolutions is reversed in \eqref{eq:product_convolution}.


\section{Constructing the approximation, $\widetilde{\mathcal{A}}$}
\label{sec:method}
This section describes how we compute the convolution kernels, $\varphi_i$, and weighting functions, $w_i$, for the product-convolution approximation, $\widetilde{\mathcal{A}} \approx \mathcal{A}$, given in \eqref{eq:product_convolution}. We also describe how we evaluate kernel entries of the approximation, $\widetilde{A}(y,x)$, and how we convert $\widetilde{\mathcal{A}}$ to H-matrix format.

We extract $\varphi_{x_i}$ for many points $x_i$ by applying $\mathcal{A}$ to Dirac combs associated with ``batches'' of points (Section \ref{sec:get_impulse_response}). To reduce memory usage, all kernels $\varphi_i$ associated with a given batch, $b$, are stored in a single function, $\eta^b$, rather than as distinct functions. A greedy algorithm is used to choose as many points as possible per batch, while ensuring that the points in each batch are not too close to each other, and are not too close to $\partial \Omega$ (Section \ref{sec:greedy_point_selection}). We ensure that the points $x_i$ are well separated from each other by forming a-priori estimates of the supports of the functions $\phi_x$ (Section \ref{eq:mean_and_covariance_estimation}). For the weighting functions, we use the smoothest functions (in a least-squares sense) that interpolate the points $x_i$; constructing the weighting functions requires solving two Poisson PDEs per point $x_i$ (Section \ref{sec:weighting_functions}).
%\begin{itemize}
%\item We extract $\varphi_{x_i}$ for many points $x_i$ by applying $\mathcal{A}$ to Dirac combs associated with ``batches'' of points (Section \ref{sec:get_impulse_response}). To reduce memory usage, all kernels $\varphi_i$ associated with a given batch, $b$, are stored in a single function, $\eta^b$, rather than as distinct functions.
%\item We use a greedy algorithm to choose as many points as possible per batch, while ensuring that the points in each batch are not too close to each other, and are not too close to $\partial \Omega$ (Section \ref{sec:greedy_point_selection}). 
%\item We ensure that the points $x_i$ are well separated from each other by forming a-priori estimates of the supports of the functions $\psi_x$ (Section \ref{eq:mean_and_covariance_estimation}). 
%\item For the weighting functions, we use the smoothest functions (in a least-squares sense) that interpolate the points $x_i$; constructing the weighting functions requires solving two Poisson PDEs per point $x_i$ (Section \ref{sec:weighting_functions}).
%\end{itemize}
The complete algorithm for constructing $\widetilde{\mathcal{A}}$ is shown in Algorithm \ref{alg:construct_Atilde}. The method for evaluating kernel entries of $\widetilde{A}$ is shown in Section \ref{sec:eval_matrix_entries}. The method for converting $\widetilde{\mathcal{A}}$ to H-matrix format is shown in Section \ref{sec:H_matrix_conversion}. 

\begin{algorithm2e}
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	{	
		\Input{Operator $\mathcal{A}$, parameter $\text{max\_batches}$}
		\Output{Dirac comb responses $\{\eta^b\}_{b=1}^{\text{max\_batches}}$, \\
			weighting functions $\{w_i\}_{i=1}^r$}
		
		Compute spatially varying mean, $\mu$, and covariance, $\Sigma$, using Algorithm \ref{alg:varhpi_mean_cov}.
		
		\For{$b=1,2,\dots,\text{max\_batches}$}{
			Choose a batch of sample points, $S_b$, using Algorithm \ref{alg:point_choice}
			
			Compute $\eta^b$ by applying $\mathcal{A}$ to the dirac comb associated with $S_b$ (Section \ref{sec:get_impulse_response})
			
			Compute weighting functions $w_i$ using Algorithm \ref{alg:weighting_functions_incremental}. In lines 2 and 3 of Algorithm \ref{alg:weighting_functions_incremental}, only solve for $\psi_i$ and $\theta_i$ associated with new points $x_i$ in $S_b$. The functions $\psi_i$ and $\theta_i$ associated with points in previous batches have already been computed in previous iterations of this for loop.
		}
		
	}
	\caption{Construct product-convolution approximation $\widetilde{\mathcal{A}}$}
	\label{alg:construct_Atilde}
\end{algorithm2e}


\subsection{Estimating impulse response supports}
\label{eq:mean_and_covariance_estimation}

Since $\phi_x$ is nonnegative, it is a scaled probability distribution. Let $\widehat{\phi}_x$ denote the normalized version of $\phi_x$, and let $\mu(x)$ and $\Sigma(x)$ denote the mean and covariance of $\widehat{\phi}_x$, respectively (see Definition \ref{defn:alpha_rho_mu_sigma}). We make the approximation that the support of $\phi_x$ is contained within the ellipsoid
\begin{equation}
\label{eq:support_ellipsoid}
E_x := \{x' \in \Omega: (x' - \mu(x))^T \Sigma(x)^{-1} (x' - \mu(x)) \le \tau^2\},
\end{equation}
where $\tau$ is a fixed constant. 
%The ellipsoid $E_{x}$ is the set of points within $\tau$ standard deviations of the mean of the Gaussian approximation of the normalized version of $\langle \mathcal{A}, \delta_{x}\rangle^{*}$ which has the same mean and covariance. 
The ellipsoid $E_x$ is the set of points within $\tau$ standard deviations from the mean of the Gaussian distribution with mean $\mu(x)$ and covariance $\Sigma(x)$, i.e., the Gaussian distribution which has the same mean and covariance as $\widehat{\phi}_x$ (see Figure FIG). The bigger $\tau$ is, the smaller the overlap between impulse responses within a batch will be, while the smaller $\tau$ is, the more impulse responses we can fit within each batch. In practice we find that $\tau=3$ provides a good balance between these competing goals. 
The fraction of the ``mass'' of $\phi_x$ residing outside of $E_x$ is less than $1/\tau^2$ by Chebyshev's inequality, though we find this bound is conservative and far less mass resides in this region in practice. 

In Algorithm \ref{alg:varhpi_mean_cov} we show how to compute $\alpha(x)$, $\mu(x)$, and $\Sigma(x)$ for all points $x$ simultaneously, by applying $\mathcal{A}^T$ to a constant function and a small number of linear and quadratic functions. Definitions required for Algorithm \ref{alg:varhpi_mean_cov} are given in Definition \ref{defn:alpha_rho_mu_sigma}, Definition \ref{defn:C_L_Q}, and Definition \ref{defn:pointwise}. The theoretical basis for Algorithm \ref{alg:varhpi_mean_cov} is shown in Theorem \ref{thm:vol_mean_cov}.

%We compute the scaling factor, denoted $\alpha(x)$, for all $x$ simultaneously by applying $\mathcal{A}^T$ to a constant function. We compute the spatially varying mean, $\mu(x)$, and covariance, $\Sigma(x)$ of the normalized version of $\phi_x$, for all $x$ simultaneously, by applying $\mathcal{A}^T$ to a small number of linear and quadratic functions. Definitions of $\alpha$, $\mu$, and $\Sigma$ are given in Definition \ref{defn:alpha_rho_mu_sigma}. The procedure for computing these quantities is shown in Algorithm \ref{alg:varhpi_mean_cov}, and the theoretical basis for this procedure is shown in Theorem \ref{thm:vol_mean_cov}. 

\begin{defn}[Moments of $\phi_x$]
	\label{defn:alpha_rho_mu_sigma}
	We define the spatially varying scaling factor, $\alpha$, the scaled impulse response, $\widehat{\phi}_x$, the spatially varying mean, $\mu$, and the spatially varying covariance, $\Sigma$, as follows:
	\begin{align*}
	\alpha:\Omega \rightarrow \mathbb{R}, &\qquad \alpha(x) := \int_{\Omega} \phi_x(z) dz, \\
	\widehat{\phi}_x:\Omega \rightarrow \mathbb{R}, &\qquad \widehat{\phi}_x := \phi_x \big/ \alpha(x), \\
	\mu:\Omega \rightarrow \mathbb{R}^d, &\qquad \mu(x) := \int_\Omega z \widehat{\phi}_x(z) dz, \\
	\Sigma:\Omega \rightarrow \mathbb{R}^{d \times d}, &\qquad\Sigma(x) := \int_\Omega (z - \mu(x))(z - \mu(x))^T \widehat{\phi}_x(z) dz.
	\end{align*}
\end{defn}

\begin{defn}[Constant, linear, and quadratic functions]	
	\label{defn:C_L_Q}
	Let  $x^i$ denote the $i^\text{th}$ component of $x$, i.e., $x = \left(x^1, x^2, \dots, x^d\right)$. We define $C$, $\{L^i\}_{i=1}^d$, and ${\{Q^{ij}\}_{i=1}^d}_{j=1}^d$ be the following constant, linear, and quadratic functions, respectively:
	\begin{equation*}
	C(x) := 1, \qquad
	L^i(x) := x^i, \qquad
	Q^{ij}(x) = x^i x^j.
	\end{equation*}
\end{defn}

\begin{defn}[Pointwise operations]
	\label{defn:pointwise}
	We write $f/g$ to denote the pointwise division of functions, $\left(f/g\right)(x) = f(x)/g(x)$, and $f\cdot g$ to denote the pointwise multiplication of functions, $(f\cdot g)(x) = f(x)g(x)$.
\end{defn}

%\begin{theorem}
%	\label{thm:vol_mean_cov}
%	Let  $x^i$ denote the $i^\text{th}$ component of $x$, i.e., $x = \left(x^1, x^2, \dots, x^d\right)$, and let $C$, $\{L^i\}_{i=1}^d$, and ${\{Q^{ij}\}_{i=1}^d}_{j=1}^d$ be the following constant, linear, and quadratic functions, respectively:
%	\begin{equation*}
%		C(x) := 1, \qquad
%		L^i(x) := x^i, \qquad
%		Q^{ij}(x) = x^i x^j.
%	\end{equation*}
%	We have
%	\begin{align}
%		\alpha =& \left(\mathcal{A}^TC\right)^*, \label{eq:vol_mean_var_thm1}\\
%		\mu^i =& \left(\mathcal{A}^T L^i\right)^* / \alpha, \label{eq:vol_mean_var_thm2}\\
%		\Sigma^{ij} =& \left(\mathcal{A}^T Q^{ij}\right)^* / \alpha - \mu^i\cdot\mu^j, \label{eq:vol_mean_var_thm3}
%	\end{align}
%	We write $f/g$ to denote the pointwise division of functions, $\left(f/g\right)(x) = f(x)/g(x)$, and $f\cdot g$ to denote the pointwise multiplication of functions, $(f\cdot g)(x) = f(x)g(x)$.
%\end{theorem}
%
%The proof of Theorem \ref{thm:vol_mean_cov} is shown in Appendix \ref{app:proofs}.


\begin{algorithm2e}
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	{	
		\Input{Operator $\mathcal{A}$}
		\Output{$\alpha$, $\mu$, and $\Sigma$}
		
		\tcp{Compute scaling factor $\alpha$}
		
		Form constant function $C(x)=1$

		$\alpha = \left(\mathcal{A}^T C\right)^*$
		
		\tcp{Compute mean $\mu$}
		\For{$i=1,2,\dots,d$}{
			Form linear function $L^i(x) = x^i$
			
			$\mu^i = \left(\mathcal{A}^T L^i\right)^* / \alpha$
		}
		\tcp{Compute covariance $\Sigma$}
		\For{$i=1,2,\dots,d$}{
			\For{$j=1,\dots,i$}{
				Form quadratic function $Q^{ij} = x^i x^j$
				
				$\Sigma^{ij} = \left(\mathcal{A}^T Q^{ij}\right)^* / \alpha - \mu^i\cdot \mu^j$
				
				$\Sigma^{ij} = \Sigma^{ji}$
			
			}
		}
		
	}
	\caption{Compute scaling factor $\alpha$, mean $\mu$, and covariance $\Sigma$}
	\label{alg:varhpi_mean_cov}
\end{algorithm2e}


\begin{theorem}[Correctness of Algorithm \ref{alg:varhpi_mean_cov}]
	\label{thm:vol_mean_cov}
	Algorithm \ref{alg:varhpi_mean_cov} correctly computes $\alpha$, $\mu$, and $\Sigma$.
\end{theorem}

The proof of Theorem \ref{thm:vol_mean_cov} is shown in Appendix \ref{app:proofs}.




\subsection{Greedy sample point selection}
\label{sec:greedy_point_selection}

Now that we have estimates of the supports of the functions $\phi_x$, we use these estimates, combined with a greedy algorithm to choose sets of sample points, $S_b$. These sample points will be used in Section \ref{sec:get_impulse_response} to form Dirac combs, which are then used to compute impulse responses of $\mathcal{A}$. Starting from a finite set of candidate points $P$, we select batches of sample points, $S_b$, using a greedy algorithm. To build a batch $S_b$, first we initialize $S_b$ as an empty set. Then we select a candidate point $p \in P$ that is the farthest away from all points in previous sample point batches. That is, $p$ is a maximizer of the following optimization problem:
\begin{equation*}
\max_{p \in P} \min_{x \in S_1 \cup \dots \cup S_{b-1}} \|p - x\|.
\end{equation*}
Once $p$ is selected, we remove $p$ from $P$. If $p$ is sufficiently far from all of the previously chosen points in the current batch, in the sense that $E_p \cap E_q = \{\}$ for all $q \in S_b$, and if $p$ is sufficiently far from the boundary, in the sense that $E_p \cap \partial \Omega = \{\}$, then we add $p$ to $S_b$. Otherwise we discard $p$. Recall that $E_p$ and $E_q$ are ellipsoids defined in \eqref{eq:support_ellipsoid}. This process repeats until there are no more points in $P$.  This is detailed in Algorithm \ref{alg:point_choice}. We determine whether $E_p \cap E_q = \{\}$ using the fast ellipsoid intersection test described in Appendix \ref{sec:fast_ellipsoid_intersection_test}.

We repeat the process to construct several batches of points $S_1, S_2, \dots$, until the number of batches exceeds a desired threshold. In our implementation, for each batch the set of candidate points $P$ is initialized as the set of all Lagrange nodes for the finite element basis functions used to discretize the problem, minus all points in previously chosen batches.


\begin{algorithm2e}
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwProg{Fn}{Function}{}{}
	
	\Input{Finite set of candidate points $P\subset \Omega$, \\mean $\mu(x)$ and covariance $\Sigma(x)$, \\domain boundary information $\partial \Omega$, \\previous sample point batches $S_1, \dots, S_{b-1}$}
	\Output{Batch of new sample points $S_b$}
	
	Initialize empty new batch of sample points, $S_b = \{\}$
	
	\While{$P$ is not empty}{
		Determine the point $p \in P$ that is farthest from all points in the previous sample point batches $S_1,\dots,S_{b-1}$
		
		Remove $p$ from $P$

	
		\If{$E_p \cap E_q \neq \{\}$ for all $q \in S_b$ and $E_p \cap \partial \Omega = \{\}$}{
			\tcp{$E_p$ and $E_q$ are the ellipsoids defined in \eqref{eq:support_ellipsoid}}
			
			Add $p$ to $S_b$
			
			Remove all points $p'$ satisfying $\mu(p') \in E_p$ from $P$
		}
	}

    \SetKwFunction{FMain}{point\_is\_acceptable}
	\caption{Choosing one batch of sample points, $S_b$}
	\label{alg:point_choice}
\end{algorithm2e}


\subsection{Batched computation of impulse responses}
\label{sec:get_impulse_response}

We compute impulse responses of $\mathcal{A}$ in batches by applying $\mathcal{A}$ to a small number Dirac combs. The batches of impulse responses, $\eta^b$, will ultimately be interpolated with weighting functions described in Section \ref{sec:weighting_functions} to form the desired approximation $\widetilde{\mathcal{A}}\approx \mathcal{A}$.  The Dirac comb, $\xi_b$, associated with a batch of sample points $S_b$ is the sum of Dirac distributions (point sources) centered at the points $x_i \in S_b$. That is,
\begin{equation*}
	\xi^b := \sum_{x_i \in S_b} \delta_{x_i}.
\end{equation*}
For each batch $S_b$, we compute as the action of $\mathcal{A}$ on the associated Dirac comb:
\begin{equation}
\label{eq:dirac_comb_H_action}
	\eta^b := \langle\mathcal{A}, \xi^b\rangle^*.
\end{equation}
By linearity, $\eta^b$ may be written as
\begin{equation}
\label{eq:phi_b}
	\eta^b = \left\langle\mathcal{A},\sum_{x_i \in S_b} \delta_{x_i}\right\rangle^* = \sum_{x_i \in S_b} \langle\mathcal{A}, \delta_{x_i}\rangle^* = \sum_{x_i \in S_b} \phi_x.
\end{equation}
Since the points $x_i$ are chosen so that the support of $\phi_i$ and the support of $\phi_j$ do not overlap (or do not overlap much) when $i \neq j$, from \eqref{eq:convolution_kernel} and \eqref{eq:phi_b} we have
\begin{equation}
\label{eq:varphi_eval}
	\varphi_{x_i}(z) = \phi_i(z+x_i) = \begin{cases}
		\eta^b(z+x_i), & z+x_i \in E_{x_i} \\
		0, & \text{otherwise}.
		\end{cases}
\end{equation}
for all $x_i \in S_b$. By performing one matrix-vector product, \eqref{eq:dirac_comb_H_action}, we recover $\varphi_{x_i}$ for every point $x_i \in S_b$. To minimize memory usage, the functions $\varphi_i$ are not stored individually. Instead, we store the functions $\eta^b$, and use formula \eqref{eq:varphi_eval} to perform function evaluations $z \mapsto \varphi_i(z)$ as needed.


\subsection{Poisson interpolation weighting functions}
\label{sec:weighting_functions}

We choose the $i^\text{th}$ weighting function, $w_i(x)$, to be the solution to the following optimization problem:
\begin{equation}
\label{eq:wi_optimization_problem}
\begin{aligned}
\min_{w_i \in H^2_N(\Omega)} &\quad \frac{1}{2}\norm{\Delta w_i}_{L^2(\Omega)}^2 \\
\text{such that} &\quad w_i(x_j) = \delta_{ij}, \quad j=1,\dots,r.
\end{aligned}
\end{equation}
Here $\Delta$ is the Laplacian on $\Omega$, $\delta_{ij}$, is the Kronecker delta, which takes value one if $i=j$ and zero if $i \neq j$, and $H^2_N(\Omega)$ is the space of all functions in $H^2(\Omega)$ with Neumann zero boundary conditions. Using these weighting functions yields the ``flattest possible'' interpolation of the convolution kernels $\varphi_i$, in a least-squares sense (Theorem \ref{thm:smoothest_interpolant}). Also, these weighting functions sum to one pointwise (Proposition \ref{thm:wi_sum_to_one}). 

In Algorithm \ref{alg:weighting_functions_incremental}, we show how to efficiently solve optimization problem \eqref{eq:wi_optimization_problem}, and thus compute $w_i$.  Constructing all $r$ of the functions $w_i$ requires solving the Poisson equation $2r$ times with different right hand side sources, and these Poisson PDE solves can be performed cheaply with multigrid. All of the weighting functions, $\{w_i\}_{i=1}^r$, will change if a new point, $x_{r+1}$, is added. However, the dominant computational cost in constructing the weighting functions is the solution of the Poisson PDEs, and these Poisson PDE solves can be performed incrementally---we only need to perform two new Poisson PDE solves when a new point is added, rather than $2(r+1)$ new PDE solves.

It is important to note that functions in $H^2_N(\Omega)$ only have well-defined pointwise values if the spatial dimension is $d=1$, $2$, or $3$. Hence optimization problem \eqref{eq:wi_optimization_problem} is only well-defined in three spatial dimensions or fewer. When inverting for parameter fields defined on four or higher spatial dimensions, the solution to this optimization problem does not exist, and these weighting functions should not be used. Proofs for results in this section are given in Appendix \ref{app:proofs}.


\begin{thm}[Optimal interpolation]
	\label{thm:smoothest_interpolant}
Let $y \in \mathbb{R}^d$, and let $\gamma_y$ be the function
\begin{equation}
\label{eq:u_defn_in_thm}
\gamma_y(x) := \sum_{i=1}^r \varphi_i(y)w_i(x) = \widetilde{A}\left(y+x,x\right),
\end{equation}
where the weighting functions $w_i$ are the solutions to optimization problem \eqref{eq:wi_optimization_problem}. Then $\gamma_y$ solves the optimization problem
\begin{equation}
\label{eq:uy_optimization_problem}
\begin{aligned}
\min_{\gamma_y \in H^2_N(\Omega)} &\quad \frac{1}{2}\norm{\Delta \gamma_y}_{L^2(\Omega)}^2 \\
\text{such that} &\quad \gamma_y(x_i) = \varphi_i(y), \quad i=1,\dots,q.
\end{aligned}
\end{equation}
\end{thm}

\begin{prop}[Weighting functions sum to one]
	\label{thm:wi_sum_to_one}
	We have
	\begin{equation*}
		\sum_{i=1}^r w_i(x) = 1
	\end{equation*}
	for all $x \in \Omega$.
\end{prop}



\begin{algorithm2e}
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	{	
		\Input{Sample points $\{x_i\}_{i=1}^r$, domain geometry $\Omega$}
		\Output{Weighting functions $\{w_i\}_{i=1}^r$}
		
		Initialize constant function $C(x) = 1$
		
		Compute Lebesgue measure of the domain, $|\Omega|$
		
		\tcp{Compute Poisson impulse responses $\{\psi_i\}_{i=1}^r$ and Poisson squared impulse responses $\{\theta_i\}_{i=1}^r$}
		
		\For{$i=1,2, \dots, r$}{
			Initialize $\delta_{x_i}$ as the point source (delta distribution) centered at $x_i$.
			
			Compute $\psi_i$ by solving the Neumann Poisson problem
			\begin{equation}
			\label{eq:psi_eq}
			\begin{cases}
			\Delta \psi_i = \delta_{x_i} - 1/|\Omega|, & \text{in }\Omega, \\
			\nu \cdot \nabla \psi_i = 0 & \text{on } \partial \Omega,
			\end{cases}
			\end{equation}
			with condition $\int_\Omega \psi_i(x) dx = 0$, where $\nu$ is the normal vector to $\partial \Omega$.

			
			Compute $\theta_i$ by solving the Neumann Poisson problem
			\begin{equation}
			\label{eq:theta_eq}
						\begin{cases}
			\Delta \theta_i = \psi_i, & \text{in }\Omega, \\
			\nu \cdot \nabla \theta_i = 0 & \text{on } \partial \Omega,
			\end{cases}
			\end{equation}
			with condition $\int_\Omega \theta_i(x) dx = 0$, where $\nu$ is the normal vector to $\partial \Omega$.
			
		}
		
		Form the $r \times r$ matrix $S$ with entries
		\begin{equation}
		\label{eq:defn_of_S_alg}
		S_{ij} = \left(\psi_i, \psi_j\right)_{L^2(\Omega)}.
		\end{equation}
		
		\tcp{Compute weighting functions $\{w_i\}_{i=1}^r$}
		
		\For{$i=1,2,\dots,r$}{
			Compute
			\begin{equation}
			\label{eq:definition_of_alpha_alg}
			\beta = \frac{\mathbf{1}^T S^{-1} e_i}{\mathbf{1}^T S^{-1} \mathbf{1}},
			\end{equation}
			where $\mathbf{1} = (1,1,\dots,1) \in \mathbb{R}^r$, and $e_i = (0,\dots,0,1,0,\dots,0)\in \mathbb{R}^r$ is unit vector with $i^\text{th}$ component equal to one.
			
			Compute 
			\begin{equation}
			\label{eq:definition_of_mu_alg}
			\lambda = -S^{-1} \left(e_i - \beta \mathbf{1} \right).
			\end{equation}
			
			Form the weighting function $w_i$ given by
			\begin{equation}
			\label{eq:definition_of_wi_alg}
				w_i(x) = \beta - \sum_{k=1}^r \lambda_k \theta_k(x).
			\end{equation}
		}
	}
	\caption{Compute weighting functions $w_i$, $i=1,\dots,r$.}
	\label{alg:weighting_functions_incremental}
\end{algorithm2e}


\begin{thm}[Correctness of Algorithm \ref{alg:weighting_functions_incremental}]
\label{thm:wi_alg_is_correct}
	The weighting functions $w_i$ computed by Algorithm \ref{alg:weighting_functions_incremental} solve optimization problem \ref{eq:wi_optimization_problem}.
\end{thm}


\subsection{Evaluating kernel entries of the approximation}
\label{sec:eval_matrix_entries}

We now describe how to efficiently evaluate $\widetilde{A}(y,x)$ at an arbitrary point $(x,y) \in \Omega \times \Omega$. From \eqref{eq:kernel_entries}, we have
\begin{equation}
\label{eq:eval_y_x}
\widetilde{A}(y,x) = \sum_{i=1}^r w_i(x) \varphi_{x_i}(y-x),
\end{equation}
and from \eqref{eq:varphi_eval}, we have
\begin{equation}
\label{eq:kernel_entry_formua}
\varphi_{x_i}(y-x) = \begin{cases}
\eta^{b(i)}(y - x + x_i), & y - x + x_i \in E_{x_i} \\
0, & \text{otherwise},
\end{cases}
\end{equation}
where $b(i)$ denotes the batch index such that $x_i \in S^{b(i)}$. To compute $z=\widetilde{A}(y,x)$, we start with $z=0$, then loop through all batches $b$, and all sample points in each batch. For each sample point, we add one term in the sum in \eqref{eq:eval_y_x} to $z$, using \eqref{eq:kernel_entry_formua} as needed to evaluate $\varphi_{x_i}$. This is shown in Algorithm \ref{alg:eval_y_x}.


\begin{algorithm2e}
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\Input{points $x \in \Omega$, $y \in \Omega$, \\sample point batches $S_1, \dots, S_\text{max\_batches}$, \\Dirac comb responses $\{\eta^b\}_{b=1}^{\text{max\_batches}}$, \\weighting functions $\{w_i\}_{i=1}^r$}
	\Output{Kernel entry $z = \widetilde{A}(y,x)$}

	$z \gets 0$
	
	\For{$b=1,2,\dots,\text{}max\_batches$}{
		\For{$x_i \in S_b$}{
			\If{$y - x + x_i \in E_{x_i}$}{
				$z \gets z + w_i(x)\eta^b(y-x+x_i)$
			}
		}
	}	
	\caption{Compute approximate kernel entry $z = \widetilde{A}(y,x)$}
	\label{alg:eval_y_x}
\end{algorithm2e}


\subsection{Conversion to hierarchical matrix format}
\label{sec:H_matrix_conversion}

We convert $\widetilde{\mathbf{A}}$ (the discretized version of $\widetilde{\mathcal{A}}$) to hierarchical matrix (H-matrix) format. Once in H-matrix format, useful matrix operations such as matrix factorization and inversion may be performed quickly using fast H-matrix methods. We use H1 matrices in our numerical results, but any of the other H-matrix formats (such as H2, HODLR, HSS, HBS, and others \cite{HMATRIX}) could be used instead. The basic ideas behind H-matrices are as follows:
\begin{itemize} 
	\item Often, large dense matrices of practical interest may be permuted, then partitioned into blocks recursively, in such a way that many off-diagonal blocks of the matrix are numerically low rank, even if the matrix is high rank.
	\item Recursive algorithms can take advantage of this low rank block structure to perform matrix arithmetic fast. Conventional algorithms for matrix inversion, matrix factorization, matrix-vector products, matrix-matrix products, and matrix addition require either $O(N^2)$ or $O(N^3)$ time and memory, while the aforementioned recursive algorithms can perform these matrix operations in $O(N \log^a N)$ time and memory for hierarchical matrices. Here $N$ is the number of degrees of freedom for the discretized version of the problem, and $a=0, 1, 2$, or $3$ depending on the operation and type of H-matrix.
\end{itemize}
For more details on H-matrices, we recommend \cite{HMATRIXGOOD}. 

We convert $\widetilde{\mathbf{A}}$ into H1 matrix format using the standard geometrical clustering/adaptive cross method implemented within the HLIBpro software package \cite{HLIBPRO}. Briefly, this method uses geometric information about the spatial locations of the degrees of freedom associated with the rows and columns of the matrix to construct a recursive block partitioning of $\widetilde{\mathbf{A}}$, such that all blocks at the finest level of the partitioning are either low rank, or are small. Then, the method forms low rank approximations of the blocks that are expected to be low rank by sampling specially chosen rows and columns of those blocks. The small high rank blocks are formed directly by evaluating their entries. The primary cost of constructing the approximation is the evaluation of $O(N \log N)$ entries of the kernel $\widetilde{A}$, which we perform using Algorithm \ref{alg:eval_y_x}. We describe the H-matrix construction in detail in Appendix \ref{app:h_matrix}.


\subsection{Rational positive semi-definite modification}

In many problems of practical interest (e.g., Hessian approximation) $\mathcal{A}$ is symmetric positive semi-definite. However, $\widetilde{\mathbf{A}}$ is generally non-symmetric and indefinite because of errors in the product-convolution approximation. 
This is undesirable. Symmetry and positive semi-definiteness are important properties which should be preserved by the approximation, if possible. Also, a lack of symmetry and/or positive semi-definiteness in $\widetilde{\mathbf{A}}$ may prevent one from using popular and highly effective algorithms, such as the conjugate gradient method, to perform further useful operations involving $\widetilde{\mathbf{A}}$.
In this section we describe a method for modifying $\widetilde{\mathbf{A}}$ to make it symmetric positive semi-definite.

First, we use fast H-matrix addition to symmetrize $\widetilde{\mathbf{A}}$:
$$\widetilde{\mathbf{A}}_\text{sym} := \frac{1}{2}\left(\widetilde{\mathbf{A}} + \widetilde{\mathbf{A}}^T\right).$$
Then we modify $\widetilde{\mathbf{A}}_\text{sym}$ to make it positive definite by forming a rational matrix function of the following form: 
\begin{equation}
\label{eq:rational_matrix_function}
\widetilde{\mathbf{A}}_\text{sym}^+ := c_0 + c_1 \widetilde{\mathbf{A}}_\text{sym} + c_2 \left(\widetilde{\mathbf{A}}_\text{sym} + \mu \mathbf{I}\right)^{-1},
\end{equation}
where $\mathbf{I}$ is the identity matrix which has the same shape as $\widetilde{\mathbf{A}}_\text{sym}$. The scalars $c_0$, $c_1$, $c_2$, and $\mu$ are chosen so that the spectrum of $\widetilde{\mathbf{A}}_\text{sym}^+$ has certain desirable properties, which we will describe later in this section. Forming $\widetilde{\mathbf{A}}_\text{sym}^+$ requires computing an H-matrix inverse, $\left(\widetilde{\mathbf{A}}_\text{sym} + \mu \mathbf{I}\right)^{-1}$. While explicit computation of matrix inverses should typically be avoided, here it is fine to compute the inverse because $\widetilde{\mathbf{A}}_\text{sym} + \mu \mathbf{I}$ is an H-matrix, and there are fast, stable, scalable, and widely used algorithms for inverting H-matrices \cite{Hmatrixinverse}.

The rational matrix approximation \eqref{eq:rational_matrix_function} may be written as $\widetilde{\mathbf{A}}_\text{sym}^+ = f(\widetilde{\mathbf{A}}_\text{sym})$, where $f$ is the rational function 
\begin{equation}
\label{eq:rational_function_scalar}
f(\lambda) := c_0 + c_1 \lambda + \frac{c_2}{\lambda+\mu}.
\end{equation}
We choose the four unknown constants, $c_0, c_1, c_2$, and $\mu$, to be the solution to the following system of four equations:
\begin{align}
f(0) &= 0 \label{eq:desired_property_1} \\
f'(0) &= 0 \label{eq:desired_property_2} \\
f(\lambda_\text{min}) &= |\lambda_\text{min}| \label{eq:desired_property_3} \\
f(\lambda_\text{max}) &= \lambda_\text{max}, \label{eq:desired_property_4}
\end{align}
where $\lambda_\text{min}$ and $\lambda_\text{max}$ are the smallest and largest eigenvalues of $\widetilde{\mathbf{A}}_\text{sym}$, respectively. We also require $\mu > |\lambda_\text{min}|$, so that the pole of $f$ is not in $[\lambda_\text{min},\lambda_\text{max}]$. Note that $\lambda_\text{min} < 0$ since $\widetilde{\mathbf{A}}_\text{sym}$ is indefinite. 
We estimate $\lambda_\text{min}$ and $\lambda_\text{max}$ using the implicitly restarted Lanczos method \cite{lanczos,scipy}, a matrix-free Krylov method. This is fast because computing matrix-vector products with an H-matrix is fast, and because the eigenvalues of $\widetilde{\mathbf{A}}_\text{sym}$ do not cluster near $\lambda_\text{min}$ or $\lambda_\text{max}$ (though the eigenvalues may cluster at zero). 

Together, equations \eqref{eq:desired_property_1}, \eqref{eq:desired_property_2}, \eqref{eq:desired_property_3},  and \eqref{eq:desired_property_4}, and the condition $\mu > |\lambda_\text{min}|$, imply that $f(0)=0$ is the unique minimum of $f$ on $[\lambda_\text{min},\lambda_\text{max}]$, which implies that $f$ is non-negative on $[\lambda_\text{min},\lambda_\text{max}]$, which implies that  $f(\widetilde{\mathbf{A}}_\text{sym}) = \widetilde{\mathbf{A}}_\text{sym}^+$ is positive semi-definite. The equation $f(0)=0$ implies that the null space of $\widetilde{\mathbf{A}}_\text{sym}^+$ contains the null-space of $\widetilde{\mathbf{A}}_\text{sym}$. This is important when the operator $\mathcal{A}$ has a large (or infinite) dimensional null space, or a spectrum that clusters at zero. Hessians in ill-posed distributed parameter inverse problems typically have both of these properties. The equation $f(\lambda_\text{max})=\lambda_\text{max}$ forces the moderate and large positive eigenvalues of $\widetilde{\mathbf{A}}_\text{sym}^+$ to be close to the corresponding moderate and large positive eigenvalues of $\widetilde{\mathbf{A}}_\text{sym}$. I.e., $f$ modifies the ``important'' part of the spectrum of $\widetilde{\mathbf{A}}_\text{sym}$ as little as possible. The equation $f(\lambda_\text{min})=|\lambda_\text{min}|$ ensures that the negative eigenvalues of $\widetilde{\mathbf{A}}_\text{sym}$, which result from error in the product-convolution approximation, do not get amplified in magnitude by the function $f$. There is a tradeoff: if we choose a larger value for $f(\lambda_\text{min})$, then the positive eigenvalues of $\widetilde{\mathbf{A}}_\text{sym}^+$ will be closer to the corresponding positive eigenvalues of $\widetilde{\mathbf{A}}_\text{sym}$ (good), but the erroneous negative eigenvalues of $\widetilde{\mathbf{A}}_\text{sym}$ will be amplified more in magnitude (bad). We find that $f(\lambda_\text{min})=|\lambda_\text{min}|$ is an appropriate balance between these competing interests.

Using the definition of $f$ in \eqref{eq:rational_function_scalar}, and basic calculus and linear algebra, we may write Equations \eqref{eq:desired_property_1}, \eqref{eq:desired_property_2}, and \eqref{eq:desired_property_4} in the following matrix form:
\begin{equation}
\label{eq:rational_3_linear_system}
\begin{bmatrix} 1 & 0 & 1/\mu \\ 0 & 1 & -1/\mu^2 \\ 1 & \lambda_\text{max} & 1/(\lambda_\text{max}+\mu) \end{bmatrix} \begin{bmatrix} c_0 \\ c_1 \\ c_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ \lambda_\text{max} \end{bmatrix},
\end{equation}
and we may write Equation \eqref{eq:desired_property_3} as follows: 
\begin{equation}
\label{eq:rational_extra_eq}
c_0 + c_1 \lambda_\text{min} + \frac{c_2}{\lambda_\text{min}+\mu} - |\lambda_\text{min}| = 0.
\end{equation}
Let $g(\mu)$ denote the left hand side of \eqref{eq:rational_extra_eq}, but with $c_0$, $c_1$, and $c_2$ now solving \eqref{eq:rational_3_linear_system} for the given value of $\mu$. I.e., within $g(\mu)$ the quantities $c_0$, $c_1$, and $c_2$ are no longer free variables; they now depend on $\mu$ implicitly. We solve the combined system, \eqref{eq:rational_3_linear_system} and $\eqref{eq:rational_extra_eq}$, by solving the one-dimensional nonlinear root finding problem
\begin{equation}
\label{eq:g_mu_zero}
g(\mu)=0
\end{equation}
with bound $\mu > |\lambda_\text{min}|$ and initial guess $\mu_0 = 2 |\lambda_\text{min}|$. Each evaluation of $g$ requires solving the $3 \times 3$ linear system \eqref{eq:rational_3_linear_system}, and once the solution $\mu$ is found, we solve \eqref{eq:rational_3_linear_system} one more time to get the final values of $c_0$, $c_1$, and $c_2$.
%
%Row 1 corresponds to the condition $f(M)=M$, row 2 corresponds to the condition $f(0)=0$, and row 3 corresponds to the condition $f'(0)=0$. To find the third row of $A$, we used the fact that the derivative of $f$ is given by $$f'(\lambda) = c_1 - \frac{c_2}{(\lambda+\mu)^2}.$$
%
%
%We want $f(\lambda)$ to have the following desirable properties:
%\begin{enumerate}
%	\item $f(\lambda)$ is nonnegative on $[m, M]$.
%	\item $f(\lambda) \approx \lambda$ when $\lambda$ is positive and not small.
%	\item $f(0)=0$ is the minimum of $f(\lambda)$ on $[m, M]$.
%	\item $f(\lambda) \le |\lambda|$ when $\lambda$ is negative.
%\end{enumerate}
%Property 1 ensures that $\widetilde{\mathbf{A}}_\text{sym}^+$ is positive semi-definite. Property 2 ensures that $f$ modifies the ``important'' part of the spectrum of $\widetilde{\mathbf{A}}_\text{sym}$ as little as possible. That is, the part of the spectrum of $\mathcal{A}$ that we expect to be well-approximated by the product-convolution approximation. Property 3 ensures that $\widetilde{\mathbf{A}}_\text{sym}^+$ preserves the null-space (and near null-space) of $\widetilde{\mathbf{A}}_\text{sym}$. This is important because in practice, we often want to approximate operators $\mathcal{A}$ that have a large (possibly infinite) number of zero eigenvalues, and/or a sequence of eigenvalues that clusters at zero. Hessians in ill-posed inverse problems typically have both of these properties. Property 4 ensures that $f$ does not amplify erroneous negative eigenvalues $\widetilde{\mathbf{A}}_\text{sym}$. If $\mathcal{A}$ is positive semi-definite, then eigenvalues of $\widetilde{\mathbf{A}}_\text{sym}$ can only be negative because of errors in the product-convolution approximation.



\subsection{Computational cost}

\begin{itemize}
	\item computational complexity in terms of PDE solves:
	\item cost of global low rank approximation (data scalability)
	\item cost to build approximation
	\item cost without compression to do a solve
	\item cost with compression to do a solve
	\item computational complexity for H-matrix alone vs. prod-conv + h-matrix $O(C k^2 \log N)$ PDE solves, C is big, k is big. Here $10$
	\item HODLR: C is smaller, but k is bigger, vs. H1 peeling process
	\item H-matrix operations cost for our method solve
	\item irregular grid
\end{itemize}

\section{Rational positive definite modification}
WRITE THIS



\section{Numerical results}
\label{sec:numerical_results}

\textbf{Heat equation}
\begin{itemize}
	\item Hessian per-column error plots for 1, 5, and 15 batches
	\item $H-P$ error vs number of batches for interior and whole domain
	\item $P^{-1}H-I$: error vs rank for $P=R$ and $P=P_\text{pch}$ diffusion time, for 1, 5, and 15 batches
	\item Krylov iterations to tols 1e-1 and 1e-6 vs diffusion parameter
	\item GLR vs PCH vs diffusion time
	\item Krylov method convergence plot: $R$ vs $P_\text{pch}$ vs no preconditioner
	\item Krylov iterations to tols 1e-6 vs mesh size $h$, PCH vs R vs no preconditioner (hold)
	\item true parameter (H)
	\item deterministic reconstruction (H)
	\item noisy measurements (1\%, 5\%, 10\% noise). $u|_\text{top}$ (H)
	\item recovered state at top (H)
	\item mesh scalability of PCH (CC) (H)
	\item put plot data into data directory
	\item save function .pvd files in paraview  directory
\end{itemize}

\begin{itemize}
	\item GLR vs PCH number of obs (Computational cost in PDE solves) (S)
	\item GLR vs PCH aspect ratio (CC) (S)
	\item turn on preconditioner after number of krylov iterations exceeds 15 in an iteration. Rebuild "as needed". (S)
\end{itemize}



Some numerical results here.

\section{Conclusions}
\label{sec:conclusions}

Some conclusions here. 

\appendix

\section{Hierarchical matrix construction}
\label{app:h_matrix}

The H-matrix construction process that we use proceeds as follows. First, we construct hierarchical partitionings of the degrees of freedom for the columns and rows of the matrix (cluster trees, Section \ref{sec:cluster_trees}). Second, we construct a hierarchical partitioning of the blocks of the matrix, in such a way that many of the blocks in the partitioning are expected to be low rank, and the remaining high rank blocks are small (block cluster tree, Section \ref{sec:block_cluster_tree}). Finally, we form low rank approximations of the blocks of the matrix that are expected to be low rank (adaptive cross approximation, Section \ref{sec:adaptive_cross}), and fill in the remaining high rank  blocks with their numerical values. The first two steps require geometric information about the spatial locations of the degrees of freedom associated with the rows and columns of the matrix, but these steps do not depend on the particular values of matrix entries. The third step requires us to evaluate $O(N \log N)$ specially-chosen entries of $\widetilde{A}$, and we evaluate these entries using the method described in Section \ref{sec:eval_matrix_entries}. 

\subsection{Cluster trees}
\label{sec:cluster_trees}

We use recursive hyperplane splitting to hierarchically cluster the degrees of freedom associated with the columns and rows of the matrix into a \emph{column cluster tree} and a \emph{row cluster tree}, respectively. Here we describe construction of the column cluster tree; the row cluster tree is constructed similarly. 

Since we use finite elements to discretize the problem, the $i^\text{th}$ column of $\widetilde{\mathbf{A}}$ corresponds to the nodal point in $\mathbb{R}^d$ associated with the $i^\text{th}$ finite element basis function. The columns of the matrix thus correspond to a point cloud in $\mathbb{R}^d$. We split this point cloud into two equally sized \emph{child} point clouds, using a hyperplane which is perpendicular to the coordinate axis direction in which the point cloud is widest (e.g., either the $x$, $y$, or $z$ axis in $3$D). The two child point clouds are split in the same way. This splitting process repeats until the point clouds have less than a preset number of points (we use $32$ points). This hierarchical partitioning of the point cloud into smaller and smaller point clouds corresponds to a hierarchical partitioning of the columns of the matrix into smaller and smaller \emph{clusters} of columns. This hierarchical partitioning of the columns forms a binary tree, which we call the column cluster tree. The root of the tree is the set of all columns, and the leaves of the tree are clusters of columns that are not subdivided any further. 

A depth-first search ordering of the column cluster tree leaves is then generated. When the columns of the matrix are permuted into this depth-first ordering, the columns associated with each cluster in the cluster tree are contiguous.

In the same way, the degrees of freedom associated with the rows of the matrix are hierarchically clustered into another cluster tree, and an ordering for the rows is generated. In our examples, the degrees of freedom for the columns coincide with the degrees of freedom for the rows, so the cluster trees for the rows and columns are the same, but this is not required in general.

\subsection{Block cluster tree} 
\label{sec:block_cluster_tree}

We partition the matrix into a recursive hierarchy of mostly low rank blocks called the \emph{block cluster tree}. The idea is that a block of the matrix is likely to be low rank if the point cloud associated with the rows of the block is far away from the point cloud associated with the columns of the block. This is reasonable to expect here because of the locality property of $\mathcal{A}$. Indeed, locality implies that many blocks of the matrix corresponding to far away point cloud clusters will be rank zero.

After reordering the rows and columns of $\widetilde{\mathbf{A}}$ via the depth-first ordering described above, we partition the reordered version of $\widetilde{\mathbf{A}}$ into a tree of $2 \times 2$ block matrices recursively. We use a geometric admissibility condition (discussed below) to decide which blocks to subdivide, and use the cluster trees for the rows and columns to determine how to subdivide those blocks. For the first stage of subdivision, let $r_1$ and $r_2$ be the children row clusters for the root of the row cluster tree, let $c_1$ and $c_2$ be the children column clusters for the root of the column cluster tree. The matrix $\widetilde{\mathbf{A}}$ is partitioned into blocks as follows:
\begin{equation*}
\begin{bmatrix}
\widetilde{\mathbf{A}}_{11} & \widetilde{\mathbf{A}}_{12} \\
\widetilde{\mathbf{A}}_{21} & \widetilde{\mathbf{A}}_{22},
\end{bmatrix}
\end{equation*}
where $\widetilde{\mathbf{A}}_{11}$ denotes the block of $\widetilde{\mathbf{A}}$ with rows $r_1$ and columns $c_1$, $\widetilde{\mathbf{A}}_{12}$ denotes the block of $\widetilde{\mathbf{A}}$ with rows $r_1$ and columns $c_2$, and so on for $\widetilde{\mathbf{A}}_{21}$ and $\widetilde{\mathbf{A}}_{22}$.

We now loop through the four blocks, $\widetilde{\mathbf{A}}_{11}$, $\widetilde{\mathbf{A}}_{12}$, $\widetilde{\mathbf{A}}_{21}$, and $\widetilde{\mathbf{A}}_{22}$, and decide which blocks should be subdivided further. For the purpose of explanation, consider $\widetilde{\mathbf{A}}_{12}$. If
\begin{equation}
\label{eq:weak_admissibility_cond}
\dist\left(r_1, c_2\right) \ge \eta \min\left(\diam\left(r_1\right), \diam\left(c_2\right)\right),
\end{equation}
then we mark $\widetilde{\mathbf{A}}_{12}$ as \emph{admissible} (expected to be low rank) and leave it alone. Here $\dist\left(r_1, c_2\right)$ is the Euclidean distance between the axis-aligned bounding box for the point cloud associated with the row cluster $r_1$, and the axis aligned bounding box for the point cloud associated with the column cluster $c_2$. The quantity $\diam\left(r_1\right)$ is the diameter of the axis aligned bounding box for the point cloud associated with the row cluster $r_1$, and $\diam\left(c_2\right)$ is the analogous diameter associated with the column cluster $c_2$. The quantity $\eta$ is a scalar constant; we use $\eta=2.0$. Basically, if the point clouds associated with $r_1$ and $c_2$ are far away from each other relative to their diameters, then we expect that the corresponding block of the matrix will be low rank. This process is repeated for the other blocks to determine which blocks are admissible and which are not. For us, the diagonal blocks $\widetilde{\mathbf{A}}_{11}$ and $\widetilde{\mathbf{A}}_{22}$ are not admissible because the distance between a point cloud and itself is zero. 

Next, we subdivide all blocks that are not admissible and are larger than a predetermined size (we use size $32 \times 32$), using the same process that was used to subdivide $\mathbf{\widetilde{A}}$. But now we subdivide a block based on the two child row clusters and two child column clusters for the rows and columns of that block. This subdivision process continues recursively until all blocks are either admissible, or smaller than the predetermined size mentioned above. The resulting hierarchical partitioning of matrix blocks forms a tree (quad-tree in $2$D, or oct-tree in $3$D), which is called the \emph{block cluster tree}. The root of the tree is the whole matrix, internal nodes in the tree are blocks that are subdivided, and the leaves of the tree are blocks that are either expected to be low rank, or are small.


\subsection{Adaptive cross low rank approximation of blocks}
\label{sec:adaptive_cross}

Once the block cluster tree has been constructed, low rank approximations of the admissible (low rank) blocks are formed using the adaptive cross method \cite{ACA}. Let $X \in \mathbb{R}^{m \times m}$ be an admissible block of $\widetilde{\mathbf{A}}$. The idea of the adaptive cross method is to form a low rank approximation of $X$ by sampling a small number of rows and columns of $X$. 
\begin{itemize}
	\item Let $C \in \mathbb{R}^{m \times r}$ be a matrix consisting of a subset of $r$ columns of $X$, such that the span of the columns in $C$ approximates the column space of $X$. 
	\item Let $R\in\mathbb{R}^{r \times m}$ be a subset of the rows of $X$, such that the span of the rows in $R$ approximates the row space of $X$.
	\item Let $U \in \mathbb{R}^{r \times r}$ be the block of $X$ consisting of the intersection of the rows from $R$ with the columns from $C$.
\end{itemize}
Then it is well-established that
\begin{equation}
\label{eq:CUR}
X \approx C U^+ R,
\end{equation}
where $U^+$ is the pseudoinverse of $U$ \cite{CUR}. The quality of approximation \eqref{eq:CUR} depends on how well the columns of $C$ approximate the column space of $X$, and how well the rows of $R$ approximate the row space of $X$. 

In the adaptive cross method, ``good'' columns, $C$, and rows, $R$, are chosen via an alternating iterative process. An initial set of columns $C$ is chosen. Keeping $C$ fixed, a set of rows $R$ is chosen so that the so that determinant of the submatrix $U$ within $C$ is as large as possible. This may be done via either the maxvol procedure \cite{GOODSUBMATRIX}, or via QR factorization. Now, keeping $R$ fixed, a set of new columns $C$ is chosen so that the submatrix $U$ within $R$ is as large as possible. This process repeats a small number of times. This results in matrices $R$ and $C$ such that the error in the approximation, \eqref{eq:CUR}, is small. The dominant cost of this procedure is the cost of computing $kr$ columns of $X$ and $kr$ rows of $X$, where $k$ is the number of alternating iterations. There is also a small linear algebra overhead cost for the determinant maximization process that is performed at each step. For more details on adaptive cross low rank approximation, see \cite{ACA}. The key point is that the adaptive cross method allows us to form a rank-$r$ approximation of an $m \times m$ block of the matrix via a process that only requires accessing $O(mr)$ entries of that block.

We use the adaptive cross method to form low rank approximations for each admissible block of $\widetilde{\mathbf{A}}$. We directly compute all entries of the small dense blocks of $\widetilde{\mathbf{A}}$ that are not admissible. This process requires us to compute $O(r^2 N \log N)$ entries of $\widetilde{\mathbf{A}}$, which is relatively cheap compared to the matrix-vector products with $\mathbf{A}$ that are required to form the product-convolution approximation $\widetilde{\mathbf{A}}$.



\section{Fast ellipsoid intersection test}
\label{sec:fast_ellipsoid_intersection_test}
The procedure for choosing sample points relies on quickly determining whether two ellipsoids intersect. Let $E_p$ and $E_q$ be the ellipsoids defined as
\begin{align*}
	E_p :=& \{x : (x - \mu_p)^T \Sigma_p^{-1} (x - \mu_p) \le \tau^2\} \\
	E_q :=& \{x : (x - \mu_q)^T \Sigma_q^{-1} (x - \mu_q) \le \tau^2\}, \\
\end{align*}
where $\mu_p, \mu_q \in \mathbb{R}^d$, and $\Sigma_p, \Sigma_q \in \mathbb{R}^{d \times d}$ are positive definite. Let $K$ be the following one dimensional convex function:
\begin{equation*}
	K(s) := 1 - \frac{1}{\tau^2} (\mu_p - \mu_q)^T \left(\frac{1}{1-s}\Sigma_p + \frac{1}{s}\Sigma_q\right)^{-1}(\mu_p - \mu_q)	
\end{equation*}
In \cite{ELLIPSOIDINTERSECT} it is shown that $E_p \cap E_q = \{\}$ if and only if $K(s) < 0$ for some $s\in (0,1)$. We check whether $E_p$ and $E_q$ intersect by minimizing $K(s)$ on $(0,1)$. If $K(s^*) <0$ at the minimizer $s^*$, then $E_p \cap E_q = \{\}$. Otherwise $E_p \cap E_q \neq \{\}$.

The function $K(s)$ may be evaluated quickly for many $s$ by pre-computing the solution to the generalized eigenvalue problem
\begin{equation*}
	\Sigma_p \Phi = \Sigma_q \Phi \Lambda,
\end{equation*}
where $\Phi \in \mathbb{R}^{d \times d}$ is the matrix of generalized eigenvectors (which may be non-orthogonal), and $\Lambda = \diag(\lambda_1,\lambda_2,\dots,\lambda_d)$ is the diagonal matrix of generalized eigenvalues $\lambda_i$. The matrix $\Phi$ simultaneously diagonalizes $\Sigma_p$ and $\Sigma_q$, in the sense that $\Phi^T\Sigma_p\Phi = \Lambda$, and $\Phi^T\Sigma_q\Phi = I$, where $I$ is the $d \times d$ identity matrix. Using this diagonalization, and some algebraic manipulations, we may write $K(s)$ as
\begin{equation}
\label{eq:Ks_generalized}
K(s) = 1 - \frac{1}{\tau^2} \sum_{i=1}^d \frac{s(1-s)}{1 + s (\lambda_i - 1)}v_i^2,
\end{equation}
where $v := \Phi^T\left(\mu_p - \mu_q\right)$. We compute the generalized eigenvalue decomposition of $\Sigma_p$ and $\Sigma_q$, then minimize $K(s)$ in the form \eqref{eq:Ks_generalized} on the interval $(0,1)$ using Brent's algorithm \cite{BRENT} (any fast 1 dimensional convex optimization routine may be used). The resulting algorithm for checking whether $E_p$ and $E_q$ intersect is summarized in Algorithm \ref{alg:ellipsoid_intersection_test}.

\begin{algorithm2e}
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwProg{Fn}{Function}{}{}
	\Input{Ellipsoid $E_p$ with mean $\mu_p$ and covariance $\Sigma_p/\tau^2$\\
		Ellipsoid $E_q$ with mean $\mu_q$ and covariance $\Sigma_q/\tau^2$ 
	}
	
	\Output{Boolean $\text{ellipsoids\_intersect}$ which is true if $E_p \cap E_q \neq \{\}$ and false otherwise}
	
	
	Solve generalized eigenvalue problem $\Sigma_p \Phi = \Sigma_q \Phi \Lambda$
		
	$v \gets \Phi^T\left(\mu_p - \mu_q\right)$
		
	$\displaystyle K^* \gets \min_{s \in (0,1)}~1 - \frac{1}{\tau^2} \sum_{i=1}^d \frac{s(1-s)}{1 + s (\lambda_i - 1)}v_i^2$
		
	\If{$K^* < 0$}{
			
		$\text{ellipsoids\_intersect} \gets \text{False}$
			
	}
	\Else{
			
		$\text{ellipsoids\_intersect} \gets \text{True}$
			
	}
	\caption{Determining whether two ellipsoids intersect}
	\label{alg:ellipsoid_intersection_test}
\end{algorithm2e}

\section{Proofs}
\label{app:proofs}

%These results rely on straightforward prerequisites that we present in Lemma \ref{lem:T_is_linear}. We prove correctness of Algorithm \ref{alg:weighting_functions_incremental} in Theorem \ref{thm:wi_alg_is_correct}. Theorem \ref{thm:wi_alg_is_correct} relies on a prerequisite result which we present in Lemma \ref{lem:psi_delta_eval}.

\begin{lem}[Preparation for Theorem \ref{thm:smoothest_interpolant}]
	\label{lem:T_is_linear}
	Let $\Omega \subset \mathbb{R}^d$, $d\in\{1,2,3\}$, be an open bounded domain with a bounded extension operator, let $x_i \in \Omega$ for $i=1,\dots,r$, and let $f \in \mathbb{R}^r$. Consider the optimization problem
	\begin{equation}
	\label{eq:generic_optimization_initial}
	\begin{aligned}
	\min_{\gamma \in H^2_N(\Omega)} &\quad \frac{1}{2}\norm{\Delta \gamma}_{L^2(\Omega)}^2 \\
	\text{such that} &\quad \gamma(x_i) = f_i, \quad i=1,\dots,r.
	\end{aligned}
	\end{equation}
	A minimizer, $\gamma^*$, of \eqref{eq:generic_optimization_initial} is a function defined on $\Omega$ that interpolates the data pairs $(x_i,f_i)$, and is as flat as possible in a least squares sense.
	The following statements hold.
	\begin{enumerate}
		\item[(a)] There exists a unique solution to \eqref{eq:generic_optimization_initial}.
		\item[(b)] 	Let $\mathcal{T}:\mathbb{R}^r \rightarrow H^2(\Omega)$ denote the solution operator for optimization problem \eqref{eq:generic_optimization_initial}	as a function of $f$. That is, 
		\begin{equation}
		\label{eq:opt_soln_operator}
		\mathcal{T}(f) := \gamma^*,
		\end{equation}
		where $\gamma^*$ is the minimizer of \eqref{eq:generic_optimization_initial}. The operator $\mathcal{T}$ is a linear operator.
	\end{enumerate}
\end{lem}
\begin{proof}
	By the Rellich-Kondrachov theorem (see, e.g., Theorem 7.20 in \cite{ARBOGAST}), $H^2_N(\Omega)$ is compactly embedded in $C^0(\Omega)$. WORK IN PROGRESS
\end{proof}


\begin{proof}[Proof of Theorem \ref{thm:smoothest_interpolant}]
	Let $f = (\varphi_1(y), \varphi_2(y), \dots, \varphi_r(y))$ and let $e_i = (0,\dots,0,1,0,\dots,0)$ be the length $r$ vector with $i^\text{th}$ component equal to one and all other components equal to zero. The solution to optimization problem \eqref{eq:uy_optimization_problem} is given by
	\begin{align*}
	\mathcal{T}(f) &= \varphi_1(y) \mathcal{T}(e_1) + \varphi_2(y) \mathcal{T}(e_2) + \dots +\varphi_r(y) \mathcal{T}(e_r) \\
	&= \varphi_1(y) w_1 + \varphi_2(y) w_2 + \dots +\varphi_r(y) w_r \\
	&= \gamma_y.
	\end{align*}
	In the first line we used linearity of the solution map, $\mathcal{T}$ (Lemma \ref{lem:T_is_linear}), in the second line we used the fact that $w_i = \mathcal{T}(e_i)$, and in the third line we used the definition of $\gamma_y$ in \eqref{eq:u_defn_in_thm}.
\end{proof}

\begin{proof}[Proof of Proposition \ref{thm:wi_sum_to_one}]
	By linearity of $\mathcal{T}$ (Lemma \ref{lem:T_is_linear}), we have
	\begin{equation*}
	\sum_{i=1}^r w_i(x) = \sum_{i=1}^r \mathcal{T}(e_i)(x) = \mathcal{T}\left(\sum_{i=1}^r e_i\right)(x) = \mathcal{T}(\mathbf{1})(x) = 1,
	\end{equation*}
	where $\mathbf{1} = (1,1,\dots,1)$ is the length $r$ vector which has all components equal to one. The last equality ($\mathcal{T}(\mathbf{1})(x) = 1$) holds because the constant function $C \in H^2(\Omega)$, $C(x)=1$, satisfies the constraint of optimization problem \eqref{eq:generic_optimization_initial} with $f = \mathbf{1}$, and yields the minimum possible value of zero for the objective function.
\end{proof}

\begin{lem}[Preparation for Theorem \ref{thm:wi_alg_is_correct}]
	\label{lem:psi_delta_eval}
	We have
	\begin{equation}
	\label{eq:inline_wi_lemma}
	h(x_j) = \left(\psi_j, \Delta h\right)_{L^2(\Omega)} + \avg(h)
	\end{equation}
	for all $h\in H^2_N(\Omega)$, where
	\begin{equation*}
	\avg(h) :=	 \frac{1}{|\Omega|} \int_\Omega h ~dx,
	\end{equation*}
	and $|\Omega| = \int_\Omega 1 ~dx$ is the Lebesgue measure of $\Omega$.
\end{lem}

\begin{proof}
	The result follows from multiplying \eqref{eq:psi_eq} by $h$, integrating both sides of the equation over $\Omega$, performing integration by parts to move derivatives from $\psi_j$ onto $h$, and using the Neumann zero boundary conditions for $\psi_j$ and $h$ to eliminate boundary integral terms.
\end{proof}

\begin{proof}[Proof of Theorem \ref{alg:weighting_functions_incremental}]
	For notational convenience, in this proof we write $w$, $\lambda$, and $e$ instead of $w_i$, $\lambda_i$, and $e_i$, respectively. Here subscripts for $\lambda$, and $e$ instead denote components of those vectors. For example, here $\lambda_j$ denotes the $j^\text{th}$ component of the vector $\lambda$, which corresponds to the $j^\text{th}$ component of the vector $\lambda_i$ in Algorithm \ref{alg:weighting_functions_incremental}.
	
	From standard optimization theory, the solution to optimization problem \ref{eq:wi_optimization_problem} is the stationary point of the Lagrangian,
	\begin{equation*}
	\mathcal{L}(w,\lambda) = \frac{1}{2}\|\Delta w\|^2 + \sum_{j=1}^r \lambda_j\left(w(x_j) - e_j\right).
	\end{equation*}
	The plan of this proof is to show that $\nabla \mathcal{L}(w,\lambda)=0$, which will verify that $w$ solves optimization problem \ref{eq:wi_optimization_problem}.
	
	Using the definition of $w_i$ in \eqref{eq:definition_of_wi_alg}, we may write the gradient of $\mathcal{L}$ with respect to $\lambda_j$ as
	\begin{equation*}
	\nabla_{\lambda_j} \mathcal{L}(w, \lambda) = w(x_j) - e_j
	= \beta - \sum_{k=1}^r \lambda_k \theta_k(x_j) - e_j.
	\end{equation*}
	Using Lemma \ref{lem:psi_delta_eval} with $h=\theta_k$, the equation for $\theta_k$ in \eqref{eq:theta_eq}, and the definition of $S$ in \eqref{eq:defn_of_S_alg}, we have
	\begin{align*}
	\sum_{k=1}^r \lambda_k \theta_k(x_j) &= \sum_{k=1}^r \lambda_k \left(\psi_j, \Delta \theta_k\right)_{L^2(\Omega)}\\
	&= \sum_{k=1}^r \lambda_k \left(\psi_j, \psi_k\right)_{L^2(\Omega)} = \left(S \lambda\right)_j.
	\end{align*}
	Combining all of these components of $\nabla_\lambda \mathcal{L}$ into a single vector and using the definition of $\lambda$ in \eqref{eq:definition_of_mu_alg} yields
	\begin{align*}
	\nabla_\lambda \mathcal{L}(w, \lambda) &= \beta \mathbf{1} - e - S \lambda \\
	&= \beta \mathbf{1} - e - S \left(-S^{-1}\left(e - \beta \mathbf{1}\right)\right) = 0.
	\end{align*}
	
	Taking the directional derivative of $\mathcal{L}$ with respect to $w$ in direction $h$ and using Lemma \ref{lem:psi_delta_eval} yields
	\begin{align}
	\nabla_w \mathcal{L}(w, \lambda)h &= \left(\Delta w, \Delta h\right)_{L^2(\Omega)} + \sum_{j=1}^r \lambda_j h(x_j) \nonumber \\
	&= \left(\Delta w, \Delta h\right)_{L^2(\Omega)} + \sum_{j=1}^r \lambda_j \left(\psi_j, \Delta h\right)_{L^2(\Omega)} + \lambda_j \avg(h) \nonumber \\
	&= \left(\Delta w + \sum_{j=1}^r \lambda_j \psi_j,~ \Delta h\right)_{L^2(\Omega)} + \avg(h) \mathbf{1}^T \lambda. \label{eq:L_w_38}
	\end{align}
	Using the definition of $w$ in Algorithm \ref{alg:weighting_functions_incremental}, and the fact that the Laplacian of a constant function is zero, we have
	\begin{equation*}
	\Delta w = -\sum_{j=1}^r \lambda_j \Delta \theta_j 
	= -\sum_{j=1}^r \lambda_j \psi_j,
	\end{equation*}
	which implies that the first term in \eqref{eq:L_w_38} is zero. By the definition of $\lambda$ in \eqref{eq:definition_of_mu_alg} and $\beta$ in \eqref{eq:definition_of_alpha_alg}, we have
	\begin{align*}
	\mathbf{1}^T \lambda &= -\mathbf{1}^T S^{-1} \left(e - \beta \mathbf{1}\right) \\
	&= -\mathbf{1}^T S^{-1} e + \beta \mathbf{1}^T S^{-1} \mathbf{1} \\
	&= -\mathbf{1}^T S^{-1} e + \frac{\mathbf{1}^T S^{-1} e}{\mathbf{1}^T S^{-1} \mathbf{1}} \mathbf{1}^T S^{-1} \mathbf{1} = 0,
	\end{align*}
	which implies that the second term in \eqref{eq:L_w_38} is zero. Thus $\nabla_w \mathcal{L}(w, \lambda) = 0$, so Algorithm \ref{alg:weighting_functions_incremental} produces the solution to optimization problem \ref{eq:wi_optimization_problem}.
\end{proof}



\begin{proof}[Proof of Theorem \ref{thm:vol_mean_cov}]
	Let $v \in L^2$ be arbitrary. We have
	\begin{align*}
		\left(v, \alpha\right)_{L^2(\Omega)} =& \int_\Omega v(x) \int_{\Omega} \langle\mathcal{A},\delta_x\rangle^*(z) dz dx\\
		=& \int_\Omega v(x) \int_{\Omega} A(z,x) dz dx \\
		=& \int_\Omega \int_\Omega v(x) A(z,x) C(z) dz dx \\
		=& \left(\mathcal{A}^T C\right)(v),
	\end{align*}
	which implies $\alpha = \left(\mathcal{A}^TC\right)^*$.
	
	Using similar techniques, we have
	\begin{align*}
		\left(v, \alpha \cdot \mu^i\right)_{L^2(\Omega)} =& \int_\Omega v(x) \alpha(x) \int_{\Omega} z^i \langle\mathcal{A},\delta_x\rangle^*(z)/\alpha(x) dz dx\\
		=& \int_\Omega \int_\Omega v(x) L^i(z) A(z,x) dz dx \\
		=& \left(\mathcal{A}^T L^i\right)(v),
	\end{align*}
	which implies $\mu^i = \left(\mathcal{A}^T L^i\right)^* / \alpha$. 
	
	Using the identity
	\begin{equation*}
		\Sigma(x) = \int_\Omega zz^T \widehat{\phi}_x(z) dz - \mu(x) \mu(x)^T,
	\end{equation*}
	we have
	\begin{align*}
		\left(v, \alpha \cdot \left(\Sigma^{ij}+\mu^i \cdot \mu^j\right)\right)_{L^2(\Omega)} =& \int_\Omega v(x) \alpha(x) \int_\Omega z^i z^j \langle\mathcal{A},\delta_x\rangle^*(z) / \alpha(x) dz dx \\
		=& \int_\Omega \int_\Omega v(x) Q^{ij}(z) A(z,x) dz dx \\
		=& \left(\mathcal{A}Q^{ij}\right)(v),
	\end{align*}
	which implies $\Sigma^{ij} = \left(\mathcal{A}^T Q^{ij}\right)^* / \alpha - \mu^i\cdot\mu^j$.
	
\end{proof}

\section{Background: distributions}
\label{app:distributions}

Let $\overline{\Omega}$ be the closure of $\Omega$, and let $C(\overline{\Omega})$ be the space of continuous functions mapping $\overline{\Omega}\rightarrow \mathbb{R}$. The action of $\mathcal{A}$ is extended to distributions $\mu:C\left(\overline{\Omega}\right) \rightarrow \mathbb{R}$ via the formula
\begin{equation*}
\langle\mathcal{A},\mu\rangle(v) := \int_\Omega v(y) \mu\left(A(y, \cdot)\right) dy, 
\end{equation*}
where $A(y,\cdot)$ is the function $x \mapsto A(y,x)$. This is derived formally as follows:
\begin{align*}
\langle\mathcal{A},\mu\rangle(v) &= \int_\Omega \int_\Omega v(y) A(y,x) \textrm{``} \mu(x) \textrm{''} dx dy \\
&= \int_\Omega v(y) \int_\Omega A(y,x) \textrm{``} \mu(x) \textrm{''} dx \\
&= \int_\Omega v(y) \mu\left(A(y,\cdot)\right) dx.
\end{align*}
For example, the delta distribution $\delta_x$ is defined by $\delta_x(v) = v(x)$, and the action of $\mathcal{A}$ on $\delta_x$ is given by
\begin{equation}
\label{eq:appendix_impulse_response}
\langle\mathcal{A},\delta_x\rangle(v)	= \int_\Omega v(y) A(y,x) dy.
\end{equation}
Recall that the impulse response of $\mathcal{A}$ to $\delta_x$, denoted $\phi_x$, is the Riesz representation of $\langle\mathcal{A},\delta_x\rangle$. From \eqref{eq:appendix_impulse_response} and the definition of the Riesz representation, we have $\phi_x = \langle\mathcal{A},\delta_x\rangle^* = A(~\cdot~,x)$. That is, $\phi_x$ is the function $y \mapsto A(y,x)$.


\section{Discretization}
\label{sec:discretization}

Let $f_i$, $i=1,\dots,N$ be a set of finite element basis functions, let $h$ denote the mesh size parameter for the finite element mesh, and let
\begin{equation*}
V_h = \Span\left(f_1, f_2, \dots, f_N\right)
\end{equation*}
be the corresponding  finite element space with the $L^2$ inner product. Functions $u\in L^2(\Omega)$ are approximated by functions $u_h \in V_h$. In turn, functions $u_h\in V_h$ are represented in computations by length $N$ arrays $\mathbf{u}$, such that the array entries of $\mathbf{u}$ are the coordinates of $u_h$ in the finite element basis. That is,
\begin{equation}
\label{eq:finite_bijection}
u_h(x) = \sum_{i=1}^N \mathbf{u}_i f_i(x).
\end{equation}

We write $\mathbf{M} \in \mathbb{R}^{N \times N}$ to denote the finite element \emph{mass matrix}, which has entries
\begin{equation*}
\mathbf{M}_{ij} = \left(f_i, f_j\right)_{L^2(\Omega)} = \int_{\Omega} f_i(x) f_j(x) dx,
\end{equation*}
and we write $\mathbb{R}^N_\mathbf{M}$ to denote the space $\mathbb{R}^N$ with the matrix-weighted inner product
\begin{equation*}
\left(\mathbf{u},\mathbf{v}\right)_\mathbf{M} := \mathbf{v}^T\mathbf{M} \mathbf{u}.
\end{equation*}
Direct calculation shows that $V_h$ is isometrically isomorphic $\mathbb{R}^N_\mathbf{M}$. That is, equation \eqref{eq:finite_bijection} establishes a bijection between functions $u_h \in V_h$ and coefficient vectors $\mathbf{u} \in \mathbb{R}^N_\mathbf{M}$, and this bijection preserves the inner product in the sense that $\left(u_h, v_h\right)_{L^2(\Omega)} = \left(\mathbf{u},\mathbf{v}\right)_\mathbf{M}$. Likewise, the dual space, $V_h'$, is isometrically isomorphic to $\mathbb{R}^N_{\mathbf{M}^{-1}}$. 

Here vectors in $\mathbf{u} \in \mathbb{R}^N_{\mathbf{M}}$ and $\boldsymbol{\sigma} \in \mathbb{R}^N_{\mathbf{M}^{-1}}$ are viewed as column vectors, and $\mathbf{u}^T$ and $\boldsymbol{\sigma}^T$ are the row vectors that result from taking the standard matrix transposes of $\mathbf{u}$ and $\boldsymbol{\sigma}$, respectively. The discrete dual of a vector $\mathbf{v}$ with respect to the $\mathbf{M}$ inner product is written as $\mathbf{v}^*$, and $\mathbf{v}^*$ is viewed as a column vector.
%These isomorphisms allow us to identify functions $u_h \in V_h$ with their corresponding coefficient arrays $\mathbf{u} \in \mathbb{R}^N_\mathbf{M}$, and linear functionals $\sigma_h \in V_h'$ with their corresponding coefficient arrays $\boldsymbol{\sigma} \in \mathbb{R}^N_{\mathbf{M}^{-1}}$.


\subsection{Discretized operations}
\label{app:discretized_operations}

Here we list the function space operations on $V_h$ and $V_h'$ that are required for this paper, and show how to perform the corresponding matrix and vector operations on $\mathbb{R}^N_{\mathbf{M}}$ and $\mathbb{R}^N_{\mathbf{M}^{-1}}$.

%We summarize coefficient vector versions of the function space operation that are used in this paper in Appendix \ref{app:discretized_operations}.
%
%As described in Section \ref{sec:discretization}, a function $u_h$ in a finite element space $V_h$ may be written in terms of its vector of coefficients, $\mathbf{u} \in \mathbb{R}^N_\mathbf{M}$, with respect the finite element basis $\{\phi_i\}_{i=1}^N$. Recall that $\mathbf{M}$ is the mass matrix. We have
%\begin{equation*}
%	u(x) \approx u_h(x) = \sum_{i=1}^N \mathbf{u}_i \phi_i(x).
%\end{equation*}
%The finite element function space $V_h$ is isometrically isomorphic to the coefficient space $\mathbb{R}^N_\mathbf{M}$, and the dual space $V_h'$ is isometrically isomorphic to $\mathbb{R}^N_{\mathbf{M}^{-1}}$. In computations, we perform operations with coefficient vectors in $\mathbb{R}^N_\mathbf{M}$ and coefficient dual vectors in $\mathbb{R}^N_{\mathbf{M}^{-1}}$, rather than functions in $V_h$ and functionals in $V_h'$. Here we describe how to perform common function space operations in terms of coefficient vectors and coefficient dual vectors.

\begin{description}
	\item[Coefficients of a functional:] The coefficient dual vector, $\boldsymbol{\sigma} \in \mathbb{R}^N_{\mathbf{M}^{-1}}$, corresponding to a linear functional $\sigma_h \in V_h'$ has components given by
	\begin{equation*}
		\boldsymbol{\sigma}_i = \sigma_h(f_i), \quad i=1,\dots,N.
	\end{equation*}
	\item[Action of a functional:] The action of a linear functional $\sigma_h \in V_h'$ on a function $u_h \in V_h$ may be computed as follows:
	\begin{equation*}
		\sigma_h(v_h) = \boldsymbol{\sigma}^T \mathbf{u}.
	\end{equation*}
	\item[Riesz representation:] Let $\sigma_h \in V_h'$. The Riesz representation of $\sigma_h$ is the unique  function $\sigma_h^* \in V_h$ satisfying 
	\begin{equation*} 
	\left(\sigma_h^*, v_h\right)_{L^2(\Omega)} = \sigma_h(v_h) \quad\forall v_h \in V_h.
	\end{equation*}
	The coefficient dual vector $\boldsymbol{\sigma}^*$ corresponding to $\sigma_h$ is given by
	\begin{equation*} 
		\boldsymbol{\sigma}^* = \mathbf{M}^{-1} \boldsymbol{\sigma}.
	\end{equation*}
	\item[Finite element $L^2$ projection:] Let $g \in L^2(\Omega)$. The orthogonal projection of $g$ onto $V_h$ with respect to the $L^2$ inner product is the unique function $g^\pi_h \in V_h$ satisfying
	\begin{equation*}
		\left(g^\pi_h, v_h\right)_{L^2(\Omega)} = \left(g, v_h\right)_{L^2(\Omega)} \quad\forall v_h \in V_h.
	\end{equation*}
	 Let $\left(\mathbf{g}^\pi\right)^* \in \mathbb{R}^N_{\mathbf{M}^{-1}}$ be the vector with components
	\begin{equation*}
		\left(\mathbf{g}^\pi\right)^*_i = \int_\Omega g(x) f_i(x) dx, \quad i=1,\dots,N.
	\end{equation*}
	The coefficient vector $\mathbf{g}^\pi \in \mathbb{R}^N_{\mathbf{M}}$ corresponding to $g^\pi_h$ is given by
	\begin{equation*}
		\mathbf{g}^\pi = \mathbf{M}^{-1}\left(\mathbf{g}^\pi\right)^*.
	\end{equation*}
	\item[Finite element interpolation:] Let $p_i \in \mathbb{R}^d$, $i=1,\dots,N$, be the Lagrange nodes associated with the finite element basis functions $f_i$. The finite element interpolant of a function $g \in C\left(\overline{\Omega}\right)$ is the function $g^I \in V_h$ given by
	\begin{equation*}
		g_h^I(x) = \sum_{i=1}^N g(p_i) f_i(x).
	\end{equation*}
	The coefficient vector, $\mathbf{g}$, for $g_h^I$ has components 
	\begin{equation*}
	\mathbf{g}_i^I = g(p_i), \quad i=1.,\dots,N.
	\end{equation*}
	\item[Projection vs. interpolation:]
	Typically, $g^\pi$ is a more accurate approximation of $g$ than $g^I$, but $g^I$ is cheaper to compute than $g^\pi$ because computing $g^\pi$ requires solving a linear system, while computing $g^I$ does not. If the finite element basis satisfies the following Kronecker property,
	\begin{equation*}
		f_i(p_j) = \begin{cases}1 & i=j \\
		0 & \text{otherwise},\end{cases}
	\end{equation*}
	then $g^I(p_i) = g(p_i)$ for $i=1,\dots,N$ (i.e., the interpolant equals the original function at the Lagrange nodes). Interpolation is typically sufficiently accurate for practical purposes if the Kronecker property is satisfied. Projection should be used if the Kronecker property is not satisfied.
	\item[Matrix representation of an operator:] Let $\mathcal{B}_h : V_h \rightarrow V_h'$ be a linear operator, and let $\mathbf{B} \in \mathbf{R}^{N \times N}$ be the matrix with entries
	\begin{equation*}
		\mathbf{B}_{ij} = \left(\mathcal{B}_h f_j\right)(f_i).
	\end{equation*}
	The matrix $\mathbf{B}$ maps $\mathbb{R}^N_\mathbf{M} \rightarrow \mathbb{R}^N_{\mathbf{M}^{-1}}$ by matrix multiplication, and
	\begin{equation*}
		\sigma_h = \mathcal{B}_h u_h \quad \Leftrightarrow \quad \boldsymbol{\sigma} = \mathbf{B} \mathbf{u}.
	\end{equation*}
	\item[Transpose:] The matrix $\mathbf{B}^T$ maps $\mathbb{R}^N_\mathbf{M} \rightarrow \mathbb{R}^N_{\mathbf{M}^{-1}}$ via matrix multiplication, and
	\begin{equation*}
		\sigma_h = \mathcal{B}_h^T u_h \quad \Leftrightarrow \quad \boldsymbol{\sigma} = \mathbf{B}^T \mathbf{u}.
	\end{equation*}

	\item[Distributions:] Let $\mu:C\left(\overline{\Omega}\right)\rightarrow \mathbb{R}$ be a distribution. If $\mu(f_i)$ is well-defined for $i=1,\dots,N$, then the restriction of $\mu$ to domain $V_h$ is a linear functional $\mu_h\in V_h'$. The functional $\mu_h$ has a coefficient dual vector $\boldsymbol{\mu} \in \mathbb{R}^N_{\mathbf{M}^{-1}}$ with entries 
	\begin{equation*}
	\boldsymbol{\mu}_i=\mu(f_i), \quad i=1,\dots,N.
	\end{equation*} 
	For example, let $\delta_p$ be the Dirac distribution centered at a point $p\in\Omega$. If the finite element basis functions, $f_i$, are continuous at $p$, then restricting the domain of $\delta_p$ to the space $V_h$ yields a linear functional in $V_h'$. The corresponding coefficient dual vector, $\boldsymbol{\delta}_p$, has entries 
	\begin{equation*}
	\left(\boldsymbol{\delta}_p\right)_i = f_i(p), \quad i=1,\dots,N.
	\end{equation*}
	
	\item[Action of an operator on a distribution:] Since $V_h$ is finite-dimensional, the restriction, $\mu_h \in V_h$, of a distribution, $\mu$, to the space $V_h$ has a a Riesz representation $\mu_h^* \in V_h$. Let $B_h$ be the integral kernel associated with an operator $\mathcal{B}_h:V_h \rightarrow V_h'$. For $v_h \in V_h$, we have
	\begin{align*}
		\langle \mathcal{B}_h, \mu_h \rangle(v) &= \int_\Omega v_h(y) \mu_h\left(B_h(y, \cdot)\right) dy \\
		&= \int_\Omega v_h(y) \left(\int_\Omega B_h(y,x) \mu_h^*(x) dx\right) dy \\
		&= \int_\Omega \int_\Omega v_h(y) B_h(y,x) \mu_h^*(x) dx dy = \left(\mathcal{B}_h \mu_h^*\right)(v).
	\end{align*}
	Going from the first to the second line we used the definition of the Riesz representation. The action of $\mathcal{B}_h$ on $\mu_h$ in the sense of distributions is therefore equal to the action of $\mathcal{B}_h$ on $\mu^*_h$ in the conventional sense. We have
	\begin{equation*}
		\sigma_h = \langle \mathcal{B}_h, \mu_h \rangle \quad \Leftrightarrow \quad \sigma_h = \mathcal{B}_h \mu_h^* \quad \Leftrightarrow \quad \boldsymbol{\sigma} = \mathbf{B} \mathbf{M}^{-1} \boldsymbol{\mu}.
	\end{equation*}
\end{description}

\subsection{Discretized mean and covariance estimation} 

\begin{algorithm2e}
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	{	
		\Input{Operator $\mathbf{A}$}
		\Output{$\boldsymbol{\alpha}\in \mathbb{R}^N$, $\boldsymbol{\mu}\in \mathbb{R}^{d \times N}$, and $\boldsymbol{\Sigma} \in \mathbb{R}^{d \times d \times N}$}
		
		Form vector $\mathbf{C} \in \mathbb{R}^N_{\mathbf{M}}$ by either projecting or interpolating constant function $C(x)=1$ onto $V_h$
		
		$\boldsymbol{\alpha} = \left(\mathbf{M}^{-1}\mathbf{A}^T \mathbf{C}\right)^*$
		
		\For{$i=1,2,\dots,d$}{
			Form vector $\mathbf{L}^i \in \mathbb{R}^N_{\mathbf{M}}$ by either projecting or interpolating linear function $L^i(x) = x^i$ onto $V_h$
			
			$\mathbf{\mu}^i = \left(\mathbf{M}^{-1}\mathbf{A}^T \mathbf{L}^i\right)^* / \boldsymbol{\alpha}$
		}
		\For{$i=1,2,\dots,d$}{
			\For{$j=1,\dots,i$}{
				Form quadratic function $\mathbf{Q}^{ij} = x^i x^j$
				
				$\boldsymbol{\Sigma}^{ij} = \left(\mathbf{M}^{-1}\mathbf{A}^T \mathbf{Q}^{ij}\right)^* / \boldsymbol{\alpha} - \boldsymbol{\mu}^i\cdot \boldsymbol{\mu}^j$
				
				$\boldsymbol{\Sigma}^{ij} = \boldsymbol{\Sigma}^{ji}$
				
			}
		}
		
	}
	\caption{Discretized version of Algorithm \ref{alg:varhpi_mean_cov}}
	\label{alg:varhpi_mean_cov_discrete}
\end{algorithm2e}


In computations, $\alpha$, $\mu^{i}$, and $\Sigma^{ij}$ are replaced with finite element approximations $\alpha_h, \mu_h^i, \Sigma_h^{ij}\in V_h$, respectively, which have coefficient vectors $\boldsymbol{\alpha}, \boldsymbol{\mu}^{ij}, \boldsymbol{\Sigma}^{ij}\in\mathbb{R}^N_\mathbf{M}$, respectively. The functions $C$, $L^i$, and $Q^{ij}$ are replaced with either finite element projections onto $V_h$ (if the finite element basis does not satisfy the Kronecker property), or finite element interpolations onto $V_h$ (if the finite element basis satisfies the Kronecker property). In our numerical examples we use finite element basis functions that satisfy the Kronecker property, and we use finite element interpolations. The coefficient vectors corresponding to either the interpolations or projections are denoted $\mathbf{C}$, $\mathbf{L}^i$, and $\mathbf{Q}^{ij}$, respectively. The discretized versions of \eqref{eq:vol_mean_var_thm1}, \eqref{eq:vol_mean_var_thm2}, and \eqref{eq:vol_mean_var_thm3} are
\begin{align*}
\boldsymbol{\alpha} &= \mathbf{M}^{-1}\mathbf{A}^T\mathbf{C} \\
\boldsymbol{\mu}^i &= \left(\mathbf{M}^{-1}\mathbf{A}^T \mathbf{L}^i\right) / \boldsymbol{\alpha}\\
\boldsymbol{\Sigma}^{ij} &= \left(\mathbf{M}^{-1}\mathbf{A}^T \mathbf{Q}^{ij}\right) / \boldsymbol{\alpha} - \boldsymbol{\mu}^i\cdot \boldsymbol{\mu}^j,
\end{align*}
respectively. Here $\mathbf{f} \cdot \mathbf{g}$ and $\mathbf{f} / \mathbf{g}$ denote the component-wise multiplication and division of vectors, respectively.


\subsection{Discretized sampling of impulse responses}
In computations, the Dirac comb coefficient dual vector for $\xi_h^b$ is the vector $\boldsymbol{\xi}^b \in \mathbb{R}^N_{\mathbf{M}^{-1}}$ which has entries
\begin{equation*}
\boldsymbol{\xi}^b_i = \sum_{x_j \in S_b} f_i(x_j), \quad i=1,\dots,N.
\end{equation*}
The Dirac comb impulse response coefficient vector for $\eta_h^b$ is the vector $\boldsymbol{\eta}^b \in \mathbb{R}^N_\mathbf{M}$ given by
\begin{equation*}
\boldsymbol{\eta}^b = \mathbf{M}^{-1} \mathbf{A}^T \mathbf{M}^{-1}\boldsymbol{\xi}^b.
\end{equation*}


\section{DISORGANIZED OLD STUFF}




\begin{defn}
	\label{def:stiffness_mass}
	We define $\mathcal{K}:H^2(\Omega)\rightarrow L^2(\Omega)'$ and $\mathcal{M}:L^2(\Omega) \rightarrow L^2(\Omega)'$ to be the following linear operators:
	\begin{align*}
	\left(\mathcal{K}u\right)(v) :=& \int_\Omega v \Delta u ~dx \\
	\left(\mathcal{M}u\right)(v) :=& \int_\Omega u v ~dx,
	\end{align*}
	and $\mathcal{P}:H^2(\Omega)\rightarrow \mathbb{R}^r$ to be the following pointwise observation operator:
	\begin{equation*}
	(\mathcal{P}u)_i = u(x_i)
	\end{equation*}
	for $i=1,\dots,r$.
\end{defn}

\begin{prop}
	\label{prop:solution_operator_is_linear}
	Let $f \in \mathbb{R}^r$ be arbitrary. The optimization problem
	\begin{equation}
	\label{eq:generic_optimization_initial_old}
	\begin{aligned}
	\min_{u \in H^2(\Omega)} &\quad \frac{1}{2}\norm{\Delta u}_{L^2(\Omega)}^2 \\
	\text{such that} &\quad u(x_i) = f_i, \quad i=1,\dots,r
	\end{aligned}
	\end{equation}
	may be rewritten as follows:
	\begin{equation}
	\label{eq:generic_optimization}
	\begin{aligned}
	\min_{u \in H^2(\Omega)} &\quad \frac{1}{2} \left(\mathcal{K}^T\mathcal{M}^{-1}\mathcal{K}u\right)(u) \\
	\text{such that} &\quad 
	\mathcal{P} u = f.
	\end{aligned}
	\end{equation}
\end{prop}

\begin{prop}
	There exists a unique solution to optimization problem \eqref{eq:generic_optimization}. The solution to \eqref{eq:generic_optimization}, $u^*$, and the adjoint variable for enforcing the constraint, $\lambda^*$, solve the following Karun-Kush-Tucker (KKT) system:
	\begin{equation}
	\label{eq:weighting_kkt}
	\begin{bmatrix}
	\mathcal{K}^T\mathcal{M}^{-1}\mathcal{K} & \mathcal{P}^T \\
	\mathcal{P} & 0
	\end{bmatrix}
	\begin{bmatrix}
	u^* \\ \lambda^*
	\end{bmatrix}
	=
	\begin{bmatrix}
	0 \\ f
	\end{bmatrix},
	\end{equation}
	where we use block-matrix notation to denote block-operators.
\end{prop}

\begin{cor}
	\label{cor:linearity_of_solution_operator}
	Let $\mathcal{T}:\mathbb{R}^r \rightarrow H^2(\Omega)$ denote the solution operator for optimization problem \eqref{eq:generic_optimization}	as a function of $f$. That is, 
	\begin{equation*}
	\mathcal{T}(f) := u^*,
	\end{equation*}
	where $u^*$ is the minimizer of \eqref{eq:generic_optimization}. The operator $\mathcal{T}$ is a linear operator.
\end{cor}


\footnote{The reason for this dependence on spatial dimension is related to the fact that the fundamental solution of the biharmonic equation is continuous in $3$ or less dimensions, but singular in $4$ or more dimensions. In the unusual scenario where one is inverting for a parameter field in $4$ or more spatial dimensions, one could use a modified Poisson interpolation where $\Delta$ is replaced with $\Delta^\alpha$ in the objective function of \eqref{eq:wi_optimization_problem}, where $\alpha>1$ is chosen so that the fundamental solution of $\Delta^{2\alpha}$ is continuous.}


The \emph{point spread function} (PSF) of $\mathcal{A}$ at $x\in \Omega$ is
\begin{equation*}
\varphi_x(y) := \langle \mathcal{A},\delta_x\rangle^*(y+x) = A(y+x,x).
\end{equation*}
Here $\langle \mathcal{A}, \delta_x \rangle \in L^2(\Omega)'$ is the result of applying $\mathcal{A}$ to a point source $\delta_x$ (delta distribution) centered at $x$, and $\langle\mathcal{A},\delta_x\rangle^* \in L^2(\Omega)$ is the Riesz representation of $\langle\mathcal{A},\delta_x\rangle$. The PSF, $\varphi_x$, is the result of translating $\langle\mathcal{A},\delta_x\rangle^*$ to re-center it at zero instead of $x$. Recall the Riesz representation of a linear functional $\psi \in L^2(\Omega)'$ is the unique vector $\psi^* \in L^2(\Omega)$ satisfying $\left(\psi^*, v\right)_{L^2(\Omega)} = \psi(v)$ for all $v \in \L^2(\Omega)$.


The approximation $\widetilde{A}(y,x)$ may not be accurate if both $x$ and $y$ are near the boundary. Other approximation methods, if available, should be considered for the block of the operator associated with boundary-boundary interactions. Nevertheless, in our numerical results we see that approximation works well for the the inverse problem considered, without any modifications to the boundary-boundary block.

The solution to this system of equations is found by the following process:
\begin{enumerate}
	\item Compute 
	\begin{equation*}
	\Psi := K^+ E^T
	\end{equation*}
	by solving Neumann poisson problems for point source right hand sides, with point sources located at the points $x_i$, $i=1,\dots,q$.
	\item Compute 
	\begin{equation*}
	\Theta := K^+ M \Psi
	\end{equation*}
	by solving Neumann Poisson problems with the columns of $\Psi$ as distributed sources.
	\item Form 
	\begin{equation*}
	S = \Psi^T M \Psi
	\end{equation*}.
	\item Compute \begin{equation*}
	\alpha = \left(\mathbf{1}^T S^{-1} \mathbf{1}\right)^{-1}\mathbf{1}^T S^{-1} \mathbf{y}.
	\end{equation*}
	\item Compute 
	\begin{equation*}
	\boldsymbol{\lambda} = -S^{-1} \left(\mathbf{y} - \alpha \mathbf{1} \right).
	\end{equation*}
	\item Compute
	\begin{equation*}
	\mathbf{u} = -\Theta \boldsymbol{\lambda} + \alpha \mathbf{c}
	\end{equation*}
\end{enumerate}


Consider the Pine island glacier geometry shown in Figure REF. Regions of the glacier are separated by thin inlets of ocean. With radial basis function interpolation, points on one side of the inlet inappropriately influence the interpolation on the other side of the inlet. With Poisson interpolation, the PSF samples only influence the interpolation on their own side of the inlet.


\subsection{Computation of Poisson weighting functions}
\label{sec:computation_of_weighting_functions}

After discretization, optimization problem \eqref{eq:wi_optimization_problem} becomes
\begin{equation}
\label{eq:discrete_poisson_optimization_1}
\begin{aligned}
\min_{\mathbf{w}} &\quad \frac{1}{2}\mathbf{w}^T K^T M^{-1} K \mathbf{w} \\
\text{such that} &\quad E \mathbf{w} = \mathbf{d}.
\end{aligned}
\end{equation}
Here $\mathbf{w}$ is the discretization of $w_i$, $K$ is the stiffness matrix for the discretized Poisson problem with pure Neumann boundary conditions, $M$ is the mass matrix, and $\mathbf{d}=(0,\dots,0,1,0,\dots,0)$ is the length-$q$ vector with $i^\text{th}$ entry one and all other entries zero. Furthermore, $E$ is the pointwise observation matrix defined by $\left(E\mathbf{u}\right)_j = u(x_j)$, where $u$ denotes the function represented by the vector $\mathbf{u}$. Here we dropped the subscripts $i$ from variables for notational convenience. One must solve $q$ optimization problems of the form \eqref{eq:discrete_poisson_optimization_1}; one for each $i=1,\dots,q$. 

In the remainder of this section we will derive an efficient algorithm for solving \eqref{eq:discrete_poisson_optimization_1}. The algorithm (Algorithm REF) is based on explicit formula for the solution of \eqref{eq:discrete_poisson_optimization_1} which we present in Proposition \ref{prop:poisson_interpolation_formula}. The formulas in Proposition \ref{prop:poisson_interpolation_formula} rely on certain matrices that we define in Definition \ref{defn:poisson_interpolation_matrices}. The primary computational cost of the algorithm is the solution of Poisson problems with pure Neumann boundary conditions.

\begin{defn}
	\label{defn:poisson_interpolation_matrices}
	Let 
	\begin{align*}
	\Psi :=& K^+ E^T, \\
	\Theta :=& K^+ M \Psi \\
	S :=& \Psi^T M \Psi,
	\end{align*}
	where $K^+$ is the Moore-Penrose pseudoinverse of $K$. Further, let $\mathbf{c}$ dente the discretization of the constant function $c(x)=1$ on $\Omega$, and let $\mathbf{1}:=(1,\dots,1)$ denote the vector of length $q$ with all entries equal to one.
\end{defn}

\begin{remark}
	Computing $\mathbf{g} = K^+\mathbf{f}$ is equivalent to solving the discretized Poisson problem with $f$ as the right hand side source, with pure Neumann boundary conditions, and with the constraint that the solution has average value equal to zero. That is,
	\begin{equation}
	\label{eq:pure_neumann_poisson}
	\mathbf{g} = K^+ \mathbf{f} \quad \Leftrightarrow \quad 
	\begin{cases}
	\Delta g = f, & \text{in }\Omega,\\
	\nu \cdot \nabla g = 0, & \text{on }\partial \Omega,\\
	\int_\Omega g(x) dx = 0,
	\end{cases}
	\end{equation}
	where $\nu$ denotes the normal to the boundary.
	
	Hence, the $i^\text{th}$ column of $\Psi$ is the result of solving the discretized pure Neumann Poisson problem, with a point source at location $x_i$. The $i^\text{th}$ column of $\Theta$ is the result of solving the discretized pure Neumann Poisson problem with distributed source given by the $i^\text{th}$ column of $\Psi$.
\end{remark}

\begin{prop}
	\label{prop:poisson_interpolation_formula}
	The solution to optimization problem \eqref{eq:discrete_poisson_optimization_1} is given by
	\begin{equation*}
	\mathbf{w} = -\Theta \boldsymbol{\lambda} + \alpha \mathbf{c},
	\end{equation*}
	where
	\begin{align*}
	\alpha =& \left(\mathbf{1}^T S^{-1} \mathbf{1}\right)^{-1}\mathbf{1}^T S^{-1} \mathbf{d} \\
	\boldsymbol{\lambda} =& -S^{-1} \left(\mathbf{d} - \alpha \mathbf{1} \right). \\
	\end{align*}
\end{prop}

\begin{proof}
	Let
	\begin{equation*}
	v := \mathcal{K} w_i,
	\end{equation*}
	and for the remainder of this proof let $e := e_i$.
	Since the null-space of the Laplacian with pure Neumann boundary conditions is the set of all constant functions, we have
	\begin{equation}
	\label{eq:definition_of_w}
	w_i = \mathcal{K}^+ v + \alpha C
	\end{equation}
	for some scalar $\alpha$.
	
	Writing optimization problem \eqref{eq:wi_optimization_problem} in terms of $v$ and $\alpha$ instead of $w_i$ yields the following equivalent optimization problem:
	\begin{equation}
	\label{eq:discrete_poisson_optimization_2}
	\begin{aligned}
	\min_{v \in L^2(\Omega)', \alpha \in \mathbb{R}} &\quad \frac{1}{2} v\left(\mathcal{M}^{-1} v\right) \\
	\text{such that} &\quad \mathcal{P}\mathcal{K}^+ v + \alpha \mathbf{1} = e.
	\end{aligned}
	\end{equation}
	The Lagrangian for optimization problem \eqref{eq:discrete_poisson_optimization_2} is
	\begin{equation*}
	\mathcal{L} = \frac{1}{2}v\left( \mathcal{M}^{-1} v\right) + \lambda^T\left(\mathcal{P}\mathcal{K}^+ v + \alpha \mathbf{1} - e\right).
	\end{equation*}
	The solution to this optimization problem is the stationary point of the Lagrangian, i.e., the point at which the gradient of the Lagrangian is zero:
	\begin{equation*}
	0 = \nabla \mathcal{L} = 
	\begin{bmatrix}
	\nabla_{v} \mathcal{L} \\
	\nabla_{\lambda} \mathcal{L} \\
	\nabla_\alpha \mathcal{L}
	\end{bmatrix} 
	= \begin{bmatrix}
	\mathcal{M}^{-1} v + \left(\mathcal{K}^+\right)^T \mathcal{P}^T \lambda \\
	\mathcal{P}\mathcal{K}^+v + \alpha \mathbf{1} - e \\
	\mathbf{1}^T \lambda
	\end{bmatrix}.
	\end{equation*}
	This equation may be rewritten as the following block $3 \times 3$ linear system:
	\begin{equation*}
	\begin{bmatrix}
	\mathcal{M}^{-1} & \left(\mathcal{K}^+\right)^T \mathcal{P}^T & 0 \\
	\mathcal{P}\mathcal{K}^+ & 0 & \mathbf{1} \\
	0 & \mathbf{1}^T & 0
	\end{bmatrix}
	\begin{bmatrix}
	v \\ \lambda \\ \alpha
	\end{bmatrix}
	=
	\begin{bmatrix}
	0 \\ e \\ 0
	\end{bmatrix}.
	\end{equation*}
	Performing block Gaussian elimination on this system allows us to reduce the system to the following block triangular form
	\begin{equation*}
	\begin{bmatrix}
	\mathcal{M}^{-1} & \left(\mathcal{K}^+\right)^T \mathcal{P}^T & 0 \\
	& -S & \mathbf{1} \\
	0 & 0 & \mathbf{1}^T S^{-1} \mathbf{1}
	\end{bmatrix}
	\begin{bmatrix}
	v \\ \lambda \\ \alpha
	\end{bmatrix}
	=
	\begin{bmatrix}
	0 \\ e \\ \mathbf{1}^T S^{-1} e
	\end{bmatrix}
	\end{equation*}
	where we recall that $S := \Psi^T \mathcal{M} \Psi = \mathcal{P}\mathcal{K}^+ \mathcal{M} \left(\mathcal{K}^+\right)^T \mathcal{P}^T$. From this block triangular system, we read off the solution as:
	\begin{align*}
	\alpha =& \left(\mathbf{1}^T S^{-1} \mathbf{1}\right)^{-1}\mathbf{1}^T S^{-1} \mathbf{d} \\
	\boldsymbol{\lambda} =& -S^{-1} \left(\mathbf{d} - \alpha \mathbf{1} \right) \\
	\mathbf{v} =& -M K^+ E^T \boldsymbol{\lambda}
	\end{align*}
	Substituting these results into the definition of $\mathbf{w}$ in \eqref{eq:definition_of_w}, we have
	\begin{equation*}
	\mathbf{w} = -K^+ M K^+ E^T \boldsymbol{\lambda} + \alpha \mathbf{c} = -\Theta \boldsymbol{\lambda} + \alpha \mathbf{c},
	\end{equation*}
	as required.
\end{proof}

We now design an incremental algorithm for constructing interpolants $\mathbf{u}$, in which we can add points $x_i$ more efficiently. Let
\begin{align*}
\Psi :=& K^+ E^T \\
\Theta :=& K^+ M \Psi
\end{align*}
so that 
\begin{equation*}
S = \Psi^T M \Psi
\end{equation*}
and
\begin{equation*}
\mathbf{u} = -\Theta \boldsymbol{\lambda} + \alpha \mathbf{c}
\end{equation*}
The columns of $\Psi$ are the responses of $A^+$ to point sources (delta distributions) centered at the sample points $x_i$. The columns of $\Theta$ are the responses of $A^+M^{-1} A$ to these point sources. The matrices $\Psi$ and $\Theta$ may be constructed incrementally as new points are added. 


\section*{Acknowledgments}
We thank J.J. Alger and Longfei Gao for helpful discussions.

\bibliographystyle{siamplain}
\bibliography{references}
\end{document}
