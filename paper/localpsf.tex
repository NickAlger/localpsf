% SIAM Article Template
\documentclass[review,onefignum,onetabnum]{siamart190516}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{localpsf_shared}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={Fast matrix-free approximation of smoothly varying blur operators, with application to Hessians in PDE-constrained inverse problems with highly informative data},
  pdfauthor={N. Alger, N. Petra, T. Hartland, and O. Ghattas}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

\externaldocument{localpsf_supplement}

% FundRef data to be entered by SIAM
%<funding-group specific-use="FundRef">
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
	We present an efficient matrix-free method for approximating locally translation invariant operators that have locally supported non-negative integral kernels. Such operators arise, for example, as Schur complements in Schur complements methods for solving partial differential equations (PDEs), Poincare-Steklov operators in domain decomposition methods, covariance operators in spatial statistics, blurring operators in imaging, and Hessians in distributed parameter PDE-constrained optimization and inverse problems. The method computes impulse responses of the operator at a collection of scattered points, then interpolates these computed impulse responses to form a product-convolution approximation of the operator. The product-convolution approximation is converted to hierarchical matrix format in order to solve linear systems and perform other matrix operations efficiently. Impulse responses are computed by applying the operator to a small number of Dirac combs associated with ``batches'' of point sources. The key innovation of our method is a matrix-free procedure for choosing as many points as possible per batch, while ensuring that the supports of the impulse responses within each batch do not overlap. We apply the method to approximate Hessians in large-scale PDE-constrained inverse problems with highly informative data. Numerical results demonstrate that our method substantially outperforms existing state-of-the-art Hessian approximation methods which are based on low-rank approximation. Our method is able to form high quality approximations of high rank Hessians using only a small number of Hessian matrix-vector products.
\end{abstract}

% REQUIRED
\begin{keywords}
  example, \LaTeX
\end{keywords}

% REQUIRED
\begin{AMS}
  ASDF
\end{AMS}

\section{Introduction}
\label{sec:intro}

We present a fast matrix free method for approximating locally translation-invariant operators, $\Aop$, that have locally supported non-negative integral kernels. Such operators arise throughout computational science. Examples include Schur complements in Schur complements methods for solving partial differential equations (PDEs) \cite{TOBYPAPER}, Poincare-Steklov operators in domain decomposition methods (e.g., Dirichlet-to-Neumann maps), covariance operators in spatial statistics, blurring operators in imaging, and Hessians in distributed parameter PDE-constrained optimization and inverse problems.

Let $\Omega \subset \mathbb{R}^d$ be a bounded domain in dimension $\gdim=1$, $2$, or $3$. We consider integral operators $\Aop:L^2(\Omega)\rightarrow L^2(\Omega)'$ of the form
\begin{equation}
	\label{eq:kernel_representation}
	(\Aop u)(v) := \int_\Omega \int_\Omega v(y) \Aker(y,x) u(x) dx dy,
\end{equation}
where $\Aker:\Omega \times \Omega \rightarrow \mathbb{R}$ is the integral kernel. Here $\Aop u \in L^2(\Omega)'$ is the linear functional that results from applying $\Aop$ to $u\in L^2(\Omega)$, and $\left(\Aop u\right)(v)$ is the scalar that results from applying that linear functional to $v \in L^2(\Omega)$. We further narrow our focus to operators which possess the following four properties:
\begin{description}
\item[1) Matrix-free:] One cannot easily evaluate kernel entries $\Aker(y,x)$. Instead, access to $\Aop$ is available through ``operator actions.'' That is, we have a black-box computational procedure (which is typically expensive) through which we may evaluate the maps
\begin{equation*}
	u \mapsto\Aop u \quad \text{and} \quad v \mapsto \Aop^Tv
\end{equation*} 
for arbitrary functions $u$ and $v$. After discretization (see Appendix \ref{app:discretized_operations}), $\Aop$ becomes a dense matrix, $\mathbf{A}$, that is typically too large to be built and stored. This property is called ``matrix-free'' because at the discrete level it means that one can perform matrix-vector products of $\mathbf{A}$ and $\mathbf{A}^T$ with arbitrary vectors, but one cannot easily\footnote{One could compute $\mathbf{v} = \mathbf{A}\mathbf{e}_j$, where $\mathbf{e}_j=(0,\dots,0,1,0,\dots,0)$ is the unit vector which has $j^\text{th}$ component equal to one and all other components equal to zero, then extract $\mathbf{A}_{ij} = \mathbf{v}_i$. However, this process is wasteful because one computes the entire vector $\mathbf{v}$, then discards all but one component.} access matrix entries $\mathbf{A}_{ij}$. In applications, applying $\mathbf{A}$ to a vector may involve iteratively solving a large linear system, timestepping, or performing some other nontrivial computational procedure. For example, in reduced space approaches to PDE-constrained optimization and inverse problems, Hessian information is available only through the application of the Hessian to vectors, and one such Hessian application requires solving two PDEs \cite{HESSIANADJOINT}. 
\item[2) Local support:] Let $\impulseresponse_x$ be the following \emph{impulse response}:
\begin{equation}
\label{eq:impulse_response_defn}
\impulseresponse_x(y) := \Aker(y, x).
\end{equation} 
We say that $\Aop$ has local support if, for all $x\in \Omega$, the support of $\impulseresponse_x$ is contained (or approximately contained) in a neighborhood of $x$. The smaller these supports are, the better our algorithm will perform.  Let $\delta_x$ denote the delta distribution (point source) centered at $x$. The function $\impulseresponse_x$ is called the impulse response because straightforward analysis shows that it may be expressed as follows:
\begin{equation}
	\label{eq:impulse_response_delta_action}
	\impulseresponse_x = \left( \Aop \delta_x^* \right)^*,
\end{equation}
where $\Aop \delta_x^* \in L^2(\Omega)'$ is an abuse of notation that denotes the result of applying the operator $\Aop$ to the distribution $\delta_x$, and $\left( \Aop \delta_x^* \right)^* \in L^2(\Omega)$ denotes the Riesz representation of $\Aop \delta_x^*$ with respect to the $L^2$ inner product.\footnote{Recall that the Riesz representative of a functional $\sigma \in L^2(\Omega)'$ with respect to the $L^2$ inner product is the unique function $\sigma^* \in L^2(\Omega)$ such that $\sigma(v) = \left(\sigma^*,v\right)_{L^2(\Omega)}$ for all $v \in L^2(\Omega)$.} Note that while the domain of $\Aop$ is defined as $L^2(\Omega)$, which does not contain $\delta_x$, if $\Aker$ is sufficiently regular then the action of $\Aop$ may be extended to distributions. This is described in Appendix \ref{app:distributions}. From \eqref{eq:impulse_response_delta_action}, we see that this local support property means that the response of $\Aop$ to a point source at $x$ is zero (or small) at points $y$ that are far from $x$.
\item[3) Local translation invariance:] We say that $\Aop$ is locally translation invariant if
\begin{equation}
	\label{eq:local_translation_invariance}
	\Aker(y+h, x+h) \approx \Aker(y,x)
\end{equation}
when $h$ is not too large. One can imagine the kernel entry $\Aker(y,x)$ as representing the strength of an interaction between a source at point $x$ and a target at point $y$. Local translation invariance means that the interaction strength remains roughly the same if the source and target points are both translated by the same vector. This is illustrated in Figure FIG.
\item[4) Non-negative kernel:] For all $(y,x) \in \Omega \times \Omega$, we have
\begin{equation*}
\Aker(y,x) \ge 0.
\end{equation*}
\end{description}

Our overall strategy is to form a product-convolution approximation, $\AopPc$, of $\Aop$ using only operator actions, then convert $\AopPc$ into a hierarchical matrix (H-matrix), which we denote by $\AmatPc$. Schematically,
\begin{equation*}
	\Aop \longrightarrow \underbrace{\AopPc}_{\substack{\text{product}\\ \text{convolution}}} \longrightarrow \underbrace{\AmatPc}_{\text{H-matrix}}.
\end{equation*}
We will define product-convolution approximations in Section \ref{sec:prodconv_intro}. H-matrices are discussed in Appendix \ref{app:h_matrix}. Our method is summarized in Section \ref{sec:overview_intro}, and described in detail in Section \ref{sec:method}. 

The novel contributions of our method are in the way that we form the product-convolution approximation ($\Aop \rightarrow \AopPc$). This is the part of the method where we use the four properties of $\Aop$ described above. Fast conversion of $\AopPc$ to H-matrix format ($\AopPc \rightarrow \AmatPc$) is possible because integral kernel entries for $\AopPc$ are easily accessible (unlike integral kernel entries for $\Aop$). Once in H-matrix format, one may use H-matrix methods to perform further useful linear algebra operations involving $\AmatPc$, including matrix-matrix addition, matrix-matrix multiplication, matrix-vector products, matrix factorization, and matrix inversion. These H-matrix methods are fast and scale well to large problems.


\subsection{Motivation: inverse problems with highly informative data}
\label{sec:PDE_hessian_motivation}

Our motivation for this work is approximation of Hessians in distributed parameter inverse problems governed by partial differential equations (PDEs). That is, inverse problems in which one seeks to reconstruct an unknown parameter field, $m$, from noisy observations, 
\begin{equation*}
	y = f(m, u(m)) + \text{noise},
\end{equation*}
which depend on  a state variable $u$. In turn, $u$ depends on $m$ implicitly through the solution of a PDE, which we write generically as follows:
\begin{equation}
	\label{eq:state_pde}
	0 = g(m,u).
\end{equation}
Here $u(m)$ denotes the solution of the PDE \eqref{eq:state_pde} as a function of $m$. In the deterministic approach to inverse problems, one typically finds $m$ as the solution to the minimization problem 
\begin{equation}
	\label{eq:minimization_problem}
	\min_m \quad \frac{1}{2}\|y - f(m,u(m))\|_W^2 + R(m),
\end{equation}
where $\|\cdot\|_W$ is weighted norm which depends on the noise covariance, and $R(m)$ is a regularization term. The objective function in optimization problem \eqref{eq:minimization_problem} is defined indirectly via the implicit function theorem. As a result, the Hessian of the objective function, $\mathcal{H}$, is only accessible matrix-free. In particular, applying $\mathcal{H}$ to a vector requires solving two PDEs \cite{HESSIANACTION}. More accessible approximations of $\mathcal{H}$ are highly desirable, because they allow for fast solution of \eqref{eq:minimization_problem} via Newton-type methods. More accessible Hessian approximations are also central to many methods for uncertainty quantification in Bayesian statistical approaches to the inverse problem, because $\mathcal{H}^{-1}$ locally approximates the Bayesian posterior covariance for $m$. 

The most popular existing Hessian approximation methods are based on forming a low rank approximation of the data misfit term in the Hessian, or the data misfit term preconditioned by the prior term \cite{CCGOPAPERS}. Conventionally, either the Lanczos method or the randomized singular value decomposition \cite{HMTRANDOM} are used to perform the low rank approximation using only matrix-vector products. These methods suffer from a ``data predicament''---if the data are highly informative about the unknown parameter, then the numerical rank of the data misfit term in the Hessian is large, so a large number of matrix-vector products are required to form the aforementioned low rank approximation. The ideal scenario from a scientific perspective (highly informative data) is therefore the worst case scenario from a computational perspective (large computational cost) \cite{MYDISSERTATION}. Although the method we present in this paper is not applicable to all Hessians, it is applicable to several Hessians of practical interest. For these Hessians, our method offers a \emph{data-scalable} alternative to conventional low-rank Hessian approximation methods, because our method can form high-rank approximations of an operator using a small number of matrix-vector products.

\subsection{Product-convolution approximation}
\label{sec:prodconv_intro}

If $\Aop$ were perfectly translation invariant (i.e., if equality held in \eqref{eq:local_translation_invariance} for all $x$, $y$, $h$) then it is straightforward to show that $\Aop$ would be the convolution operator $u \mapsto \left(\convkernel_x \ast u\right)^*$, where the convolution kernel is given by
\begin{equation}
	\label{eq:convolution_kernel}
	\convkernel_x(y) := \impulseresponse_x(y+x).
\end{equation}
The convolution kernel $\convkernel_x$ is the result of translating the impulse response $\impulseresponse_x$ to re-center it at zero instead of $x$. Locally translation invariant operators act like convolution operators locally, but the convolution kernel, $\convkernel_x$, varies as $x$ changes. We therefore approximate $\Aop$ by a spatially varying weighted sum of convolution operators, $\AopPc \approx \Aop$, of the form
\begin{equation}
\label{eq:product_convolution}
	\left(\AopPc~u\right)(v) := \left( v,~ \sum_{i=1}^\convrank \convkernel_i \ast \left(w_i \cdot u\right) \right)_{L^2(\Omega)},
\end{equation}
where $f \cdot g$ denotes pointwise multiplication, $f \ast g$ denotes convolution, and $\left(f, g\right)_{L^2(\Omega)}$ denotes the $L^2$ inner product on $\Omega$, for functions $f,g \in L^2(\Omega)$. Approximations of the form \eqref{eq:product_convolution} are known as \emph{product-convolution} approximations, because the action of each term in the sum consists of a pointwise product, followed by a convolution. To avoid nested subscripts, we write 
\begin{equation*}
	\convkernel_i := \convkernel_{x_i} \quad \text{and} \quad \impulseresponse_i := \impulseresponse_{x_i},
\end{equation*}
with $\convkernel_{x_i}$ defined in \eqref{eq:convolution_kernel} and $\impulseresponse_{x_i}$ defined in \eqref{eq:impulse_response_defn}, to denote local convolution kernels and impulse responses corresponding to a collection of points $\{x_i\}_{i=1}^\convrank \subset \Omega$ scattered throughout the domain. The functions $w_i$ are spatially varying weighting functions that are used to interpolate the convolution kernels $\convkernel_i$. Our operator approximation method is defined by how we compute the convolution kernels, $\convkernel_i$, how we choose the points, $x_i$, and what weighting functions, $w_i$, that we use. In Section \ref{sec:overview_intro} we summarize the answers to these questions; we will provide detailed answers to these questions in Section \ref{sec:method}.

The more locally translation invariant an operator is (i.e., the smaller the discrepancy between the left hand side and right hand side in \eqref{eq:local_translation_invariance}), the smaller the number of terms that are required in \eqref{eq:product_convolution} to achieve an accurate product-convolution approximation. 

\subsection{Overview of the method}
\label{sec:overview_intro}

We compute one ``batch'' of $\convkernel_i$'s by applying $\Aop$ to a sum of point sources (Dirac comb) associated with a collection of points $x_i$ scattered throughout $\Omega$. The batch of points $x_i$ are chosen so that the support of $\impulseresponse_i$ and the support of $\impulseresponse_j$ do not overlap (or do not overlap much) if $i \neq j$. Because these supports do not overlap, we can post-process the response of $\Aop$ to the Dirac comb to recover the functions $\impulseresponse_i$, and therefore the functions $\convkernel_i$, associated with all points $x_i$ in the batch---with one application of $\Aop$, we recover many $\convkernel_i$. This is illustrated in Figure FIG. The process is repeated to get more batches of $\convkernel_i$'s, until a desired number of batches is reached.

In order to choose the points $x_i$, we need to estimate the support of $\impulseresponse_i$ \emph{before} we compute it. The supports of the functions $\impulseresponse_x$ are estimated a-priori, for all $x \in \Omega$ simultaneously, via a procedure that involves applying $\Aop^T$ to a small number of specially chosen functions (three functions if $\gdim=1$, six if $\gdim=2$, ten if $\gdim=3$), then post processing the results (Section \ref{eq:mean_and_covariance_estimation}). This procedure is the only part of the paper that requires non-negativity of the integral kernel. 

We choose the $i$th weighting function, $w_i$, to be the flattest function, in a least-squares sense, that takes the value one at the sample point $x_i$, and the value zero at the other sample points $x_j$, $j \neq i$. Computing the weighting functions $w_i$ requires solving two Poisson PDEs per weighting function, and these PDEs are solved cheaply using multigrid. We call this interpolation scheme \emph{Poisson interpolation}. The advantage of Poisson interpolation over other interpolation methods, most notably radial basis function interpolation \cite{ESCANDE}, is that Poisson interpolation incorporates information about the domain geometry when constructing the weighting functions. This is illustrated in Figure FIG. 

Once the $\convkernel_i$ and $w_i$ are computed, we convert $\AopPc$ to hierarchical matrix (H-matrix) format. Let $\Aker_\text{pc}$ denote the integral kernel associated with $\AopPc$. While $\Aker(y,x)$ is not easily computable, $\Aker_\text{pc}(y,x)$ is given by the formula
\begin{equation}
	\label{eq:kernel_entries}
	\AkerPc(y,x) = \sum_{i=1}^\convrank \convkernel_i(y-x) w_i(x).
\end{equation}
This follows from~\eqref{eq:product_convolution} and the fact that the convolution operator $u \mapsto \convkernel_i \ast u$ has $(y,x)$ kernel entry given by $\convkernel(y-x)$.
Formula~\eqref{eq:kernel_entries} allows us to construct a H-matrix representation of $\AopPc$ using the conventional adaptive cross H-matrix construction method, in which one forms low-rank approximations of blocks of the matrix by sampling rows and columns of those blocks \cite{HACA}. Once in H-matrix format, fast H-matrix arithmetic is used to invert $\AopPc$, or perform other useful matrix operations.

Often we are interested in approximating operators $\Aop$ that are symmetric positive semi-definite. Unfortunately, $\AopPc$ is, in general, non-symmetric and indefinite due to errors in the product convolution approximation. In this case, we modify the H-matrix representation of $\AopPc$ to make it positive definite, by symmetrizing it, then applying a specially chosen rational matrix function to the symmetrized H-matrix.


\subsection{Existing work}

In general, product-convolution approximations take the form shown in \eqref{eq:product_convolution}, but $\convkernel_i$ and $w_i$ may be arbitrary functions. Product-convolution approximations have been used in a wide variety of fields going back several decades \cite{PRODCONVLIST}.
For background on product-convolution approximations, we recommend reading the following papers: \cite{PRODCONVGOOD}. 

We build upon the class of product convolution approximations in which the functions, $\convkernel_i$, are impulse responses of $\Aop$ to point sources at a collection of points $x_i$ \cite{PRODCONVLIST}. A popular choice for methods in this class is to choose the points $x_i$ to be nodes in a regular grid, and interpolate the functions $\convkernel_i$ with piecewise linear interpolation \cite{NAGY}, or splines \cite{PRODCONVSPLINES}. With a regular grid, a large number of points $x_i$ is typically required to achieve an accurate product-convolution approximation. In our previous work, we reduced the computational cost by starting with a coarse grid of points $x_i$, then adaptively refining the grid in the regions where the error in the approximation is large \cite{PRODCONVMYPAPER}. But even with adaptive refinement, many matrix vector products with $\Aop$ may be required. In this paper, rather than using adaptive refinement, we instead reduce the computational cost by picking points $x_i$ such that many $\convkernel_i$ are computed with each matrix vector product. 

Our method of estimating the support of the functions $\convkernel_i$ was inspired by resolution analysis in seismic imaging \cite{RESOLUTION}. In resolution analysis, $\Aop$ is the Hessian for a seismic inverse problem, and the width of $\convkernel_p$ is used to estimate of the minimum length scale on which features of the parameter can be inferred near the point $p$. If the width of $\convkernel_p$ is ten meters, then features of the parameter near $p$ can be accurately inferred from data if those features are larger than roughly ten meters in size. In \cite{RESOLUTION}, the width of $\convkernel_p$ is estimated to be the local autocorrelation length of the function $\Aop^T \zeta$ near $p$, where $\zeta$ is a random noise funtion. In this paper, rather than probing $\Aop^T$ with random noise functions, we use specific constant, linear, and quadratic functions. Our method estimates the support of $\convkernel_p$ more accurately and reliably than resolution analysis, but our method requires that $\Aop$ has a positive integral kernel, while the resolution analysis method does not have this requirement.

Matrix-vector products with product-convolution approximations can be performed using the fast Fourier transform if the problem is discretized on a regular grid \cite{PRODCONVFFT}. The product-convolution approximation can then replace the original operator when using Krylov methods to solve linear systems with the original operator as the coefficient operator \cite{PRODCONVKRYLOV}. However, for complex geometries, problems are rarely discretized using regular grids, so we cannot easily use the fast Fourier transform. Furthermore, in big-data inverse problems the required number of Krylov iterations is large \cite{MYDISSERTATION,OTHERS}. Thus, after constructing the product-convolution approximation, it is desirable to convert the product-convolution approximation to other matrix formats that are amenable to fast linear algebra operations. Wavelet compression methods have been used for this purpose \cite{PRODCONVWAVELETS}. Here we follow our previous work \cite{PRODCONVMYPAPER}, in which we convert the product-convolution approximation to H-matrix format. 

Conventional H-matrix construction methods require access to matrix entries of the matrix being approximated, and therefore cannot be used to efficiently form H-matrix approximations of operators that are only available through matrix-vector products. There are matrix-free methods for H-matrix approximation \cite{LEXINGPEELINGPROCESS}, and these methods have been used to form H-matrix representations of Hessians in PDE constrained inverse problems \cite{ILONAHMATRIX,NOEMIHMATRIX}. While these matrix-free H-matrix construction methods are asymptotically scalable in theory, the required number of matrix vector products can be large in practice. 


Product-convolution type approximations of Hessians have been used in a seismic inverse problem in \cite{GEORGSEISMICPRODCONV}, and in an advection-diffusion inverse problem in \cite{PRODCONVMYPAPER}. In \cite{DEMANETSEISMIC}, matrix probing \cite{MATRIXPROBING} is used to approximate the Hessian in a seismic inverse problem as the sum of simple pseudodifferential operators. Although it is not explicitly mentioned, the approximation in \cite{DEMANETSEISMIC} could be interpreted as a convolution-product interpolation, which is like a product-convolution approximation, except the order of pointwise multiplications and convolutions is reversed in \eqref{eq:product_convolution}.


\section{Constructing the product-convolution H-matrix approximation}
\label{sec:method}
In this section we describe how we choose the sample points, $x_i$ (Section \ref{sec:sample_point_selection}), how we form the convolution kernels, $\convkernel_i$ (Section \ref{sec:get_impulse_response}), and how we form the weighting functions, $w_i$ (Section \ref{sec:weighting_functions}), for the product-convolution approximation, $\AopPc \approx \Aop$, given in \eqref{eq:product_convolution}. We also describe how we convert $\AopPc$ to H-matrix format (Section \ref{sec:H_matrix_conversion}), and (optionally) how we modify the resulting H-matrix, $\AmatPc$, to force it to be symmetric positive semi-definite (Section \ref{sec:make_spd}). The complete algorithm for constructing $\AmatPc$ is shown in Algorithm \ref{alg:construct_Atilde}. 

\begin{algorithm2e}
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	{	
		\Input{Linear operator $\Aop$, parameter $\text{max\_batches}$}
		\Output{H-matrix $\AmatPc$ (optionally, $\AmatPcSymPlus$)}
		
		Compute $\spatialvol$, $\spatialmean$, and $\spatialcov$ using Algorithm \ref{alg:varhpi_mean_cov}.
		
		\For{$k=1,2,\dots,\text{max\_batches}$}{
			Choose a batch of sample points, $\pointbatch_k$, using Algorithm \ref{alg:point_choice}
			
			Compute $\combresponse_k$ by applying $\Aop$ to the dirac comb for $\pointbatch_k$ (Section \ref{sec:get_impulse_response})
			
		}

		Form convolution kernels, $\convkernel_{x_i}$, on regular grids (Section \ref{sec:convolution_kernel_regular_grid})
	
		Modify convolution kernels to address boundary artifacts (Section \ref{sec:boundary_extension})
	
		Compute all weighting functions $w_i$ using Algorithm 

		Form H-matrix $\AmatPc$ (Section \ref{sec:H_matrix_conversion})
		
		(optional) Modify $\AmatPc$ to make $\AmatPcSymPlus$ (Section \ref{sec:make_spd})
		
	}
	\caption{Construct product-convolution H-matrix}
	\label{alg:construct_Atilde}
\end{algorithm2e}


\subsection{Sample point selection}
\label{sec:sample_point_selection}

We choose sample points, $x_i$, in batches. A greedy algorithm is used to choose as many points as possible per batch, while ensuring that the points in each batch are not too close to each other. We ensure that the points $x_i$ are well separated from each other by forming a-priori estimates of the supports of the functions $\impulseresponse_x$ (Section \ref{eq:mean_and_covariance_estimation}). The support of $\impulseresponse_x$ is estimated to be contained within an ellipsoid, so the process of choosing batches of points is an ellipsoid packing problem (Section \ref{sec:greedy_point_selection}). 


\subsubsection{Estimating impulse response supports}
\label{eq:mean_and_covariance_estimation}

Because of the non-negative kernel property, for each $x \in \Omega$, the impulse response $\impulseresponse_x$ is a non-negative function $y \mapsto \impulseresponse_x(y)$. Hence, $\impulseresponse_x$ is a scaled probability distribution. Let $\spatialvol$ denote the scaling factor, let $\widehat{\impulseresponse}_x := \impulseresponse_x / \spatialvol(x)$ denote the normalized version of $\impulseresponse_x$, and let $\spatialmean(x)$ and $\spatialcov(x)$ denote the mean and covariance of $\widehat{\impulseresponse}_x$, respectively. We make the approximation that the support of $\impulseresponse_x$ is contained within the ellipsoid
\begin{equation}
	\label{eq:support_ellipsoid}
	E_x := \{x' \in \Omega: (x' - \spatialmean(x))^T \spatialcov(x)^{-1} (x' - \spatialmean(x)) \le \tau^2\},
\end{equation}
where $\tau$ is a fixed constant. The ellipsoid $E_x$ is the set of points within $\tau$ standard deviations from the mean of the Gaussian distribution with mean $\spatialmean(x)$ and covariance $\spatialcov(x)$, i.e., the Gaussian distribution which has the same mean and covariance as $\widehat{\impulseresponse}_x$ (see Figure FIG). The quantity $\tau$ is a parameter that must be chosen by the user of the algorithm. The larger $\tau$ is, the larger the ellipsoid $E_x$ is, and the more conservative the estimate is for the support of $\impulseresponse_x$. However, the cost of our algorithm will depend on how many non-overlapping ellipsoids $E_x$ we can ``pack'' in the domain $\Omega$ (more ellipsoids $\implies$ lower cost), and choosing a larger value of $\tau$ means that fewer ellipsoids will fit in $\Omega$. We find that $\tau=3$ yields a reasonable balance between these competing interests, and use $\tau=3$ in our numerical results. The fraction of the ``mass'' of $\impulseresponse_x$ residing outside of $E_x$ is less than $1/\tau^2$ by Chebyshev's inequality, though this bound is conservative and typically far less mass resides in this region. 

The ellipsoid $E_x$ is not immediately available. To use $E_x$, we must first compute the scalar $\spatialvol(x)$, the length-$\gdim$ vector $\spatialmean(x)$, and the $\gdim \times \gdim$ matrix $\spatialcov(x)$ (recall $\gdim \in \{1,2,3\}$ is the spatial dimension of $\Omega$). The direct approach to compute $\spatialvol(x)$, $\spatialmean(x)$, and $\spatialcov(x)$ is to apply $\Aop$ to a point source centered at $x$ to get $\impulseresponse_x$, as per \eqref{eq:impulse_response_delta_action}. Then one can post-process $\impulseresponse_x$ to determine $\spatialvol(x)$, $\spatialmean(x)$, and $\spatialcov(x)$. But this direct approach is computationally infeasible because and it requires one operator action of $\Aop$ per point $x$ that we wish to evaluate $E_x$ at, and we will need to evaluate $E_x$ at a large number of points. Fortunately, it is possible to compute $\spatialvol(x)$, $\spatialmean(x)$, and $\spatialcov(x)$, \emph{for all points $x$ simultaneously}, by applying $\Aop^T$ to one constant function, $\gdim$ linear functions, and $\gdim(\gdim+1)/2$ quadratic functions. We present this efficient method of computing $\spatialvol$, $\spatialmean$, and $\spatialcov$ in Theorem \ref{thm:vol_mean_cov} and summarize the method in Algorithm \ref{alg:varhpi_mean_cov}.


\begin{theorem}[Impulse response moments]
	\label{thm:vol_mean_cov}
	Let $\spatialvol:\Omega \rightarrow \mathbb{R}$, $\spatialmean:\Omega \rightarrow \mathbb{R}^\gdim$, and $\spatialcov:\Omega \rightarrow \mathbb{R}^{\gdim \times \gdim}$, be the following functions:
	\begin{equation*}
		\spatialvol(x) := \int_{\Omega} \impulseresponse_x(y) dy, \qquad
		\spatialmean(x) := \mathbb{E}(\widehat{\impulseresponse}_x), \qquad
		\spatialcov(x) := \Var(\widehat{\impulseresponse}_x),
	\end{equation*}
	where $\widehat{\impulseresponse}_x := \impulseresponse_x / \spatialvol(x)$ denotes the normalized version of $\impulseresponse_x$, $\mathbb{E}(\widehat{\impulseresponse}_x)$ denotes the mean of $\widehat{\impulseresponse}_x$, and $\Var(\widehat{\impulseresponse}_x)$ denotes the variance of $\widehat{\impulseresponse}_x$.	Let  $x^i$ denote the $i^\text{th}$ component of $x$, i.e., $x = \left(x^1, x^2, \dots, x^\gdim\right)$. Let $C$, $\{L^i\}_{i=1}^\gdim$, and ${\{Q^{ij}\}_{i=1}^\gdim}_{j=1}^\gdim$ be the following constant, linear, and quadratic functions:
	\begin{equation*}
		C(x) := 1, \qquad
		L^i(x) := x^i, \qquad
		Q^{ij}(x) = x^i x^j.
	\end{equation*}
	Let $f/g$ denote the pointwise division of functions, $\left(f/g\right)(x) = f(x)/g(x)$, and $f\cdot g$ denote the pointwise multiplication of functions, $(f\cdot g)(x) = f(x)g(x)$. The following results hold:
	\begin{subequations}
		\label{eq:vol_mean_var}
		\begin{align}
			\spatialvol =& \left(\Aop^T C\right)^* \\
			\spatialmean^i =& \left(\Aop^T L^i\right)^* / \spatialvol \\
			\spatialcov^{ij} =& \left(\Aop^T Q^{ij}\right)^* / \spatialvol - \spatialmean^i\cdot \spatialmean^j
		\end{align}
	\end{subequations}
	 for $i=1,\dots, \gdim$, $j=1,\dots,\gdim$.
\end{theorem}

The proof of Theorem \ref{thm:vol_mean_cov} is shown in Appendix \ref{app:proofs}. The proof is straightforward, and follows directly from the definitions.

\begin{algorithm2e}
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	{	
		\Input{Operator $\Aop$}
		\Output{$\spatialvol$, $\spatialmean$, and $\spatialcov$}
		
		\tcp{Compute scaling factor $\spatialvol$}
		
		Form constant function $C(x)=1$

		Compute $\spatialvol = \left(\Aop^T C\right)^*$
		
		\tcp{Compute mean $\spatialmean$}
		\For{$i=1,2,\dots,\gdim$}{
			Form linear function $L^i(x) = x^i$
			
			Compute $\spatialmean^i = \left(\Aop^T L^i\right)^* / \spatialvol$
		}
		\tcp{Compute covariance $\spatialcov$}
		\For{$i=1,2,\dots,\gdim$}{
			\For{$j=1,\dots,i$}{
				Form quadratic function $Q^{ij}(x) = x^i x^j$
				
				Compute $\spatialcov^{ij} = \left(\Aop^T Q^{ij}\right)^* / \spatialvol - \spatialmean^i\cdot \spatialmean^j$
				
				Set $\spatialcov^{ji} = \spatialcov^{ij}$
			
			}
		}
		
	}
	\caption{Compute scaling factor $\spatialvol$, mean $\spatialmean$, and covariance $\spatialcov$}
	\label{alg:varhpi_mean_cov}
\end{algorithm2e}


\subsubsection{Greedy ellipsoid packing}
\label{sec:greedy_point_selection}

We use a greedy ellipsoid packing algorithm to choose batches of sample points, $\pointbatch_k$, such that there is no overlap between the ellipsoids, $E_{x_i}$, associated with the sample points, $x_i$, within a batch. These sample point batches will be used in Section \ref{sec:get_impulse_response} to compute many impulse responses of $\Aop$ using only a small number of operator actions of $\Aop$. The support of $\impulseresponse_x$ is (approximately) contained in the ellipsoid $E_x$, so by applying $\Aop$ to the Dirac comb associated with all sample points in a batch, we will recover the impulse responses for all sample points in that batch.

We start with a finite set of candidate points $P$. To build $\pointbatch_k$, first we initialize $\pointbatch_k$ as an empty set. Then we select a candidate point $p \in P$ that is the farthest away from all points in previous sample point batches. That is, $p$ is a maximizer of the following optimization problem:
\begin{equation*}
\max_{p \in P} \min_{x \in \pointbatch_1 \cup \dots \cup \pointbatch_{k-1}} \|p - x\|.
\end{equation*}
Candidate points for the first batch are chosen arbitrarily from $P$.
Once $p$ is selected, we remove $p$ from $P$. If $p$ is sufficiently far from all of the previously chosen points in the current batch, in the sense that $E_p \cap E_q = \{\}$ for all $q \in \pointbatch_k$, then we add $p$ to $\pointbatch_k$. Otherwise we discard $p$. This process repeats until there are no more points in $P$.  This is detailed in Algorithm \ref{alg:point_choice}. We determine whether $E_p \cap E_q = \{\}$ using the fast ellipsoid intersection test described in Appendix \ref{sec:fast_ellipsoid_intersection_test}.

We repeat the process to construct several batches of points $\pointbatch_1, \pointbatch_2, \dots$, until the number of batches exceeds a desired threshold. In our implementation, for each batch the set of candidate points $P$ is initialized as the set of all Lagrange nodes for the finite element basis functions used to discretize the problem, except those points in previously chosen batches.

\begin{algorithm2e}
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwProg{Fn}{Function}{}{}
	
	\Input{Finite set of candidate points $P\subset \Omega$, \\spatialmean $\spatialmean(x)$ and covariance $\spatialcov(x)$, \\domain boundary information $\partial \Omega$, \\previous sample point batches $\pointbatch_1, \dots, \pointbatch_{k-1}$}
	\Output{Batch of new sample points $\pointbatch_k$}
	
	Initialize empty new batch of sample points, $\pointbatch_k = \{\}$
	
	\While{$P$ is not empty}{
		Determine the point $p \in P$ that is farthest from all points in the previous sample point batches $\pointbatch_1,\dots,\pointbatch_{k-1}$
		
		Remove $p$ from $P$

	
		\If{$E_p \cap E_q \neq \{\}$ for all $q \in \pointbatch_k$ and $E_p \cap \partial \Omega = \{\}$}{
			\tcp{$E_p$ and $E_q$ are the ellipsoids defined in \eqref{eq:support_ellipsoid}}
			
			Add $p$ to $\pointbatch_k$
			
			Remove all points $p'$ satisfying $\spatialmean(p') \in E_p$ from $P$
		}
	}

    \SetKwFunction{FMain}{point\_is\_acceptable}
	\caption{Choosing one batch of sample points, $\pointbatch_k$}
	\label{alg:point_choice}
\end{algorithm2e}


\subsection{Convolution kernels}
\label{sec:get_impulse_response}

We compute convolution kernels $\convkernel_{x_i}$ in batches by applying $\Aop$ to a small number Dirac combs. The Dirac comb, $\diraccomb_k$, associated with a batch of sample points, $\pointbatch_k$, is the sum of Dirac distributions (point sources) centered at the points $x_i \in \pointbatch_k$. That is,
\begin{equation*}
	\diraccomb_k := \sum_{x_i \in \pointbatch_k} \delta_{x_i}.
\end{equation*}
For each batch $\pointbatch_k$, we compute the action of $\Aop$ on the associated Dirac comb:
\begin{equation}
	\label{eq:dirac_comb_H_action}
	\combresponse_k := \left(\Aop \diraccomb_k^*\right)^*.
\end{equation}
By linearity, the resulting function $\combresponse_k$ may be written as follows:
\begin{equation}
	\label{eq:phi_b}
	\combresponse_k = \left(\Aop \left(\sum_{x_i \in \pointbatch_k} \delta_{x_i}\right)^*\right)^* = \sum_{x_i \in \pointbatch_k} \left(\Aop \delta_{x_i}^*\right)^* = \sum_{x_i \in \pointbatch_k} \impulseresponse_{x_i}.
\end{equation}
Since the points $x_i$ are chosen so that the ellipsoid $E_{x_i}$ that (approximately) supports $\impulseresponse_i$, and the ellipsoid $E_{x_j}$ that (approximately) supports $\impulseresponse_j$ do not overlap when $i \neq j$, from \eqref{eq:convolution_kernel} and \eqref{eq:phi_b} we have (approximately)
\begin{equation}
\label{eq:varphi_eval}
	\convkernel_{x_i}(z) = \impulseresponse_i(z+x_i) = 
	\begin{cases}
		\combresponse_k(z+x_i), & z+x_i \in E_{x_i} \\
		0, & \text{otherwise}
	\end{cases}
\end{equation}
for all $x_i \in \pointbatch_k$. By performing one matrix-vector product, \eqref{eq:dirac_comb_H_action}, we recover the convolution kernels $\convkernel_{x_i}$ associated with every point $x_i \in \pointbatch_k$. 


\subsubsection{Interpolation onto local regular grids}
\label{sec:convolution_kernel_regular_grid}

To convert $\AopPc$ to H-matrix format (Section \ref{sec:H_matrix_conversion}), we will need to evaluate $\convkernel_{x_i}(z)$ for a large number of points $z$. Per \eqref{eq:varphi_eval}, evaluating $\convkernel_{x_i}(z)$ requires evaluating $\combresponse_k(z+x_i)$. In computations, $\combresponse_k$ will be a finite element function formed on an irregular mesh, and $z+x_i$ will typically not be a gridpoint of the mesh. Evaluating $\combresponse_k(z+x_i)$ therefore requires finding which mesh cell that $z+x_i$ is in, then evaluating several polynomial functions to evaluate the finite element function on that cell. Although this procedure is asymptotically scalable, it is slow in practice \cite{FEMEVALSLOW}. 

To speed up evaluation of $\convkernel_{x_i}$, we interpolate $\convkernel_{x_i}$ onto a regular rectilinear grid on a coordinate axis aligned box that just barely contains the ellipsoid $E_{x_i}$. We transfer the function $\convkernel_{x_i}$ from the irregular mesh to the regular grid by evaluating $\convkernel_{x_i}$ at all gridpoints in the regular grid, using the slow process described in the previous paragraph. This is an upfront cost which is done ``offline.'' After $\convkernel_{x_i}$ has been transferred to the regular grid, we use fast regular grid interpolation to perform all subsequent ``online'' evaluations of $\convkernel_{x_i}$. This yields a substantial overall speedup because the number of finite element function evaluations required to transfer the functions $\convkernel_{x_i}$ to the regular grid is $O(\fedim)$, while the number of kernel evaluations required to convert $\AopPc$ to H-matrix format is $O(k \fedim \log \fedim)$. Here $\fedim$ is the number of finite element degrees of freedom, and $k$ is the H-matrix rank. Furthermore, transferring $\convkernel_{x_i}$ to a regular grid requires evaluating $\convkernel_{x_i}$ at a predetermined and highly structured collection of points. This structure allows for faster determination of which mesh cells that points are in, and allows one to evaluate the finite element function at all regular gridpoints in parallel. In contrast, constructing the H-matrix requires evaluating $\convkernel_{x_i}$ at an unpredictable sequence of scattered of points, which is an expensive task for finite element function evaluation, but a cheap task for regular grid interpolation. 
Having $\convkernel_{x_i}$ stored on a regular grid also simplifies the implementation of the method that we use to deal with boundary artifacts, which will be described in Section \ref{sec:boundary_extension}.

We incur interpolation error when transferring $\convkernel_{x_i}$ from the irregular mesh to the regular grid. To limit this interpolation error, the density of the regular grid should be as dense or denser than the local density of the finite element mesh. In our numerical results, the distance between neighboring points in the regular grid is chosen to be half the minimum distance between any pair of finite element Lagrange nodes that are contained in $E_{x_i}$. The grid transfer procedure is illustrated in Figure FIG. 

When $x_i$ is near a boundary, it may occur that some gridpoints for the regular grid are in locations where $\convkernel_{x_i}$ is undefined. In this case, we store a placeholder value for $\convkernel_{x_i}$ at those gridpoints. We use the IEEE standard value of ``NaN'' (not a number) for this placeholder. In the next section (Section \ref{sec:boundary_extension}) we replace these NaN values with more appropriate values.


\subsubsection{Addressing boundary artifacts}
\label{sec:boundary_extension}

Evaluating the kernel for our product-convolution approximation, $\AkerPc(y,x)$, requires evaluating $\convkernel_{x_i}(y-x)$. But $\convkernel_{x_i}(y-x)$ is undefined  when $y - x + x_i$ is outside $\Omega$, which may occur even if $x \in \Omega$ and $y \in \Omega$. The default approach to deal with this problem is to extend $\convkernel_{x_i}$ by zero wherever it is undefined. But this leads to substantial errors, known as boundary artifacts, which are especially significant when the sample point $x_i$ is near the boundary of the domain. In this case, the impulse response is artificially chopped off by the domain boundary. See Figure FIG for an illustration. 

While $\convkernel_{x_i}$ may be undefined at a point $z$, often there is a sample point $x_j$ that is near $x_i$, but further away from the boundary than $x_i$, such that $\convkernel_{x_j}$ is well defined at $z$. To address boundary artifacts, we fill in missing/undefined portions of convolution kernels using information from other convolution kernels associated with the $k$ nearest sample points (we use $k=8$ in our numerical results). Kernels associated with nearer sample points are given precedence over kernels associated with farther away sample points. 

In detail, let $p_1, p_2, \dots, p_k$ be the $k$ nearest sample points to $x_i$, ordered by nearness, including $x_i$ itself. That is, $p_1=x_i$, $p_2$ is the nearest neighbor to $x_i$, $p_3$ is the second nearest neighbor to $x_i$, and so on. Also, recall from the previous section (Section \ref{sec:convolution_kernel_regular_grid}) that the functions $\convkernel_{x_i}$ are stored on regular rectilinear grids. To extend $\convkernel_{x_i}$, we form an expanded regular rectilinear grid that has the same grid spacing as the grid used for $\convkernel_{x_i}$, and contains that grid as a subgrid, but is large enough to contain the boxes associated with all of the neighbor kernels, $\convkernel_{p_1}, \convkernel_{p_2}, \dots, \convkernel_{p_k}$ (recall that the functions $\convkernel_{x_i}$ are all centered at zero). We initialize an extended convolution kernel, $\convkernel_{x_i}^E$, to zero on this expanded grid. Then we interpolate the functions $\convkernel_{p_j}$ onto this expanded grid in reverse order. First we interpolate $\convkernel_{p_k}$ onto the grid, then $\convkernel_{p_{k-1}}$, and so on until the last step in which we interpolate the original kernel $\convkernel_{p_1} = \convkernel_{x_i}$. At each step, we overwrite existing values of $\convkernel_{x_i}^E$ with new values from the kernel currently being interpolated, except when the new value of $\convkernel_{p_j}$ is undefined (``NaN''). At gridpoints where the kernel being interpolated is undefined, we leave $\convkernel_{x_i}^E$ unchanged. In this way, we fill in undefined values of $\convkernel_{x_i}$ with values from successively further and further away kernels. This process is illustrated in Figure FIG. To avoid notational clutter, and since we will henceforth only work with the extended kernel, $\convkernel_{x_i}^E$, in the remainder of this paper the symbol $\convkernel_{x_i}$ refers to $\convkernel_{x_i}^E$.


\subsection{Poisson weighting functions}
\label{sec:weighting_functions}

Consider the following optimization problem,
\begin{equation}
	\label{eq:wi_optimization_problem_generic}
	\begin{aligned}
	\min_{v \in H^2_N(\Omega)} &\quad \frac{1}{2}\norm{\Delta v}_{L^2(\Omega)}^2 \\
	\text{such that} &\quad v(x_j) = \interpolatedvalues_j, \quad j=1,\dots,\convrank,
	\end{aligned}
\end{equation}
in which one seeks a smooth function $v$ that takes the values $\interpolatedvalues_j \in \mathbb{R}$ at the points $x_j$. Here $\Delta$ is the Laplacian on $\Omega$ and $H^2_N(\Omega)$ is the space of all functions in $H^2(\Omega)$ with Neumann zero boundary conditions. We choose the $i^\text{th}$ weighting function, $w_i(x)$, to be the solution to optimization problem \eqref{eq:wi_optimization_problem_generic}, with 
\begin{equation*}
	b_j = w_i(x_j) = 
	\begin{cases}
		1, & j=i\\
		0, & j \neq i.
	\end{cases}
\end{equation*}
In Theorem \ref{thm:smoothest_interpolant}, we show that using these weighting functions yields the ``flattest possible'' \cite{JACOBSEN} interpolation of the convolution kernels, $\convkernel_i$, in a least-squares sense. Also, these weighting functions sum to one pointwise (Proposition \ref{thm:wi_sum_to_one}).

We may efficiently solve optimization problem \eqref{eq:wi_optimization_problem_generic} via the following procedure:
\begin{enumerate}
	\item For each sample point $x_i$, we compute a function $\firstgreens_i$ by solving the following Neumann Poisson problem:
	\begin{equation}
		\label{eq:psi_eq}
		\begin{cases}
			\Delta \firstgreens_i = \delta_{x_i} - 1/|\Omega|, & \text{in }\Omega, \\
			\nu \cdot \nabla \firstgreens_i = 0 & \text{on } \partial \Omega,
		\end{cases}
	\end{equation}
	with condition $\int_\Omega \firstgreens_i(x) dx = 0$. Here $\delta_{x_i}$ is the point source centered at $x_i$, $|\Omega|$ is the measure of the domain $\Omega$, and $\nu$ is the normal vector to $\partial \Omega$.
	\item For each function $\firstgreens_i$, we compute a function $\secondgreens_i$ by solving another Neumann Poisson problem:
	\begin{equation}
		\label{eq:theta_eq}
		\begin{cases}
			\Delta \secondgreens_i = \firstgreens_i, & \text{in }\Omega, \\
			\nu \cdot \nabla \secondgreens_i = 0 & \text{on } \partial \Omega,
		\end{cases}
	\end{equation}
	with condition $\int_\Omega \secondgreens_i(x) dx = 0$.
	\item The solution to optimization problem \eqref{eq:wi_optimization_problem_generic}, which we denote by $v_*$, is formed as follows:
	\begin{equation}
		\label{eq:optimal_v}
		v_*(x) := \beta + \sum_{i=1}^\convrank c_i \secondgreens_i(x),
	\end{equation}
	where the constants $c_i \in \mathbb{R}$, $i=1,\dots,\convrank$, and $\beta \in \mathbb{R}$ are the solution to the following linear system:
	\begin{equation}
		\label{eq:interpolation_linear_system}
		\begin{bmatrix}
			\pointinteractionmatrix & \mathbf{1} \\ \mathbf{1}^T & 0
		\end{bmatrix}
		\begin{bmatrix}
			\mathbf{c} \\ \beta
		\end{bmatrix}
		=
		\begin{bmatrix}
			\mathbf{\interpolatedvalues} \\ 0
		\end{bmatrix}.
	\end{equation}
	Here $\mathbf{c} \in \mathbb{R}^\convrank$ is the vector with $i$th component $c_i$, $\mathbf{1} \in \mathbf{R}^\convrank$ is the column vector of all ones, $\mathbf{\interpolatedvalues} \in \mathbb{R}^\convrank$ is the vector with $i$th component $\interpolatedvalues_i$, and $\pointinteractionmatrix \in \mathbb{R}^{\convrank \times \convrank}$ is the matrix with entries given by
	\begin{equation}
		\label{eq:S_matrix}
		\pointinteractionmatrix_{ij} := \left( \firstgreens_i, \firstgreens_j \right)_{L^2(\Omega)}.
	\end{equation}
\end{enumerate}
Linear system \eqref{eq:interpolation_linear_system} is the KKT system that arises if one solves optimization problem \eqref{eq:wi_optimization_problem_generic} on the $\convrank+1$-dimensional subspace of $H^2_N(\Omega)$ spanned by the functions $\secondgreens_i$, $i=1,\dots,\convrank$, and the constant function $C(x)=1$. In Theorem \ref{thm:wi_no_breakdown}, we prove that this procedure does not break down, and in Theorem \ref{thm:wi_alg_is_correct}, we prove that the function $v_*$ produced by this procedure is the solution to optimization problem \eqref{eq:wi_optimization_problem_generic} on $H^2_N(\Omega)$.

Constructing all $\convrank$ of the functions $w_i$ thus requires solving the Poisson equation $2\convrank$ times with different right hand side sources to form the functions $\firstgreens_i$ and $\secondgreens_i$, then post-processing these functions. This is shown in Algorithm \ref{alg:weighting_functions_incremental}. The Poisson PDE solves, which are the most expensive part of this procedure, can be performed cheaply with multigrid. 

It is important to note that functions in $H^2_N(\Omega)$ only have well-defined pointwise values if the spatial dimension of the domain is $\gdim=1$, $2$, or $3$. Hence optimization problem \eqref{eq:wi_optimization_problem} is only well-posed in three spatial dimensions or fewer. In four or higher spatial dimensions, one could construct appropriate weighting functions using radial basis functions, though we do not discuss this here.

\begin{thm}[Procedure for computing $v_*$ does not break down]
	\label{thm:wi_no_breakdown}
	If all the sample points $x_i$, $i=1,\dots,\convrank$, are distinct, then PDE \eqref{eq:psi_eq} is uniquely solvable for $\firstgreens_i$, PDE \eqref{eq:theta_eq} is uniquely solvable for $\secondgreens_i$, and linear system \eqref{eq:interpolation_linear_system} is uniquely solvable for $\mathbf{c}$ and $\beta$. That is, the procedure for computing $v_*$ does not break down. Furthermore, $v_* \in H^2_N(\Omega)$.
\end{thm}

\begin{proof}
	Existence and uniqueness for the solutions to PDEs \eqref{eq:psi_eq} and \eqref{eq:theta_eq} follow from the standard elliptic PDE theory. By standard elliptic regularity theory, $\firstgreens_i \in H^1(\Omega)$, so $\firstgreens_i \in L^2(\Omega)$, and so all matrix entries $\pointinteractionmatrix_{ij}$ are finite. 
	
	Using the definition of $\pointinteractionmatrix$ and basic linear algebra, it is straightforward to show that if $\pointinteractionmatrix$ were singular, there would exist constants $a_1, a_2, \dots a_\convrank$, which are not all zero, such that
	\begin{equation*}
		0 = a_1 \firstgreens_1 + a_2 \firstgreens_2 + \dots a_\convrank \firstgreens_\convrank.
	\end{equation*}
	Applying the Laplacian to this equation yields
	\begin{equation*}
		0 = a_1 \delta_{x_1} + a_2 \delta_{x_2} + \dots a_\convrank \delta_{x_\convrank} - \left(\sum_{i=1}^\convrank a_i\right)\bigg/|\Omega|,
	\end{equation*}
	which is impossible because a nonzero linear combination of delta distributions at different points cannot equal a constant function. Thus we must conclude that $\pointinteractionmatrix$ is non-singular. By block matrix factorization, non-singularity of $\pointinteractionmatrix$ implies that linear system \eqref{eq:interpolation_linear_system} is uniquely solvable for $\mathbf{c}$ and $\beta$.
	
	By standard elliptic regularity theory, the functions $\secondgreens_i$ are in $H^3(\Omega)$, and therefore they are in $H^2(\Omega)$. By construction the functions $\secondgreens_i$ have Neumann zero boundary conditions, so $\secondgreens_i \in H^2_N(\Omega)$. Since $v_*$ is a linear combination of the functions $\secondgreens_i$ and the constant function $C(x)=1$, $v_* \in H^2_N(\Omega)$.
\end{proof}

\begin{lem}[Preparation for Theorem \ref{thm:wi_alg_is_correct}]
	\label{lem:psi_delta_eval}
	We have
	\begin{equation}
		\label{eq:inline_wi_lemma}
		h(x_j) = \left(\firstgreens_j, \Delta h\right)_{L^2(\Omega)} + \avg(h)
	\end{equation}
	for all $h\in H^2_N(\Omega)$, where
	\begin{equation*}
		\avg(h) :=	 \frac{1}{|\Omega|} \int_\Omega h ~dx,
	\end{equation*}
	and $|\Omega| = \int_\Omega 1 ~dx$ is the Lebesgue measure of $\Omega$.
\end{lem}

\begin{proof}
	The result follows from multiplying \eqref{eq:psi_eq} by $h$, integrating both sides of the equation over $\Omega$, performing integration by parts to move derivatives from $\firstgreens_j$ onto $h$, and using the Neumann zero boundary conditions for $\firstgreens_j$ and $h$ to eliminate boundary integral terms.
\end{proof}

\begin{thm}[Solution of optimization problem \eqref{eq:wi_optimization_problem_generic}]
	\label{thm:wi_alg_is_correct}
	The function $v_*$ given in \eqref{eq:optimal_v} is the unique solution to optimization problem \eqref{eq:wi_optimization_problem_generic}.
\end{thm}

\begin{proof}
	Let
	\begin{equation*}
		X := \{v \in H^2_N(\Omega) : v(x_i) = \interpolatedvalues_i \text{ for }i=1,\dots,\convrank\}
	\end{equation*}
	denote the feasible set for optimization problem \ref{eq:wi_optimization_problem_generic}, and let 
	\begin{equation*}
		X_0 := \{v \in H^2_N(\Omega) : v(x_i) = 0 \text{ for }i=1,\dots,\convrank\}
	\end{equation*}	
	denote the tangent space to $X$. Note that pointwise values of functions in $H_N^2(\Omega)$ are well-defined since the Sobolev imbedding theorem (see, e.g., Theorem 7.19 in \cite{Arbogast}) implies that $H^2(\Omega)$ is continuously imbedded in the space of continuous functions $C^0(\Omega)$ when $\gdim < 4$, and we are considering $\gdim \in \{1,2,3\}$. The feasible set $X$ is therefore a closed affine subspace of $H^2_N(\Omega)$, the tangent space $X_0$ is a closed subspace of $H^2_N(\Omega)$, and optimization problem \eqref{eq:wi_optimization_problem_generic} is well-defined.
	
	The proof now proceeds in three steps. First, we show that $v_*$ is feasible ($v_* \in X$). Second, we show that $v_*$ is the unique minimizer of \eqref{eq:wi_optimization_problem_generic} if it satisfies a certain variational equation. Third, we verify that $v_*$ satisfies this variational equation.
	
	\paragraph{Feasibility} To show that $v_*$ is feasible (i.e., $v_* \in X$), we must show that $v_*(x_i) = \interpolatedvalues_i$ for $i=1,\dots,\convrank$. We have
	\begin{align*}
		v_*(x_i) &= \beta + \sum_{j=1}^\convrank c_j \secondgreens_j(x_i) \\
		&= \beta + \sum_{j=1}^\convrank c_j \left(\firstgreens_i, \Delta \secondgreens_j\right)_{L^2(\Omega)} + \avg\left(\secondgreens_j\right) \\
		&= \beta + \sum_{j=1}^\convrank c_j \left(\firstgreens_i, \firstgreens_j\right)_{L^2(\Omega)} \\
		&= \beta + \sum_{j=1}^\convrank c_j \pointinteractionmatrix_{ij}
	\end{align*}
	In the first line we used the definition of $v_*$ in \eqref{eq:optimal_v}. In the second line we used Lemma \ref{lem:psi_delta_eval}. In the third line we used the facts that, by construction, $\Delta \secondgreens_j = \firstgreens_j$ and $\int_\Omega \secondgreens_j(x) dx = 0$. In the fourth line we used the definition of $\pointinteractionmatrix$ in \eqref{eq:S_matrix}. Hence the condition $v_*(x_i) = \interpolatedvalues_i$ for $i=1,\dots,\convrank$ may be written in matrix form as
	\begin{equation*}
		\pointinteractionmatrix \mathbf{c} + \beta \mathbf{1} = \mathbf{\interpolatedvalues},
	\end{equation*}
	and this must hold because it is the first block row of system \eqref{eq:interpolation_linear_system} which is solved to compute $\mathbf{c}$ and $\beta$. So $v_* \in X$.
	
	\paragraph{Optimality} Since $X$ is an affine subspace, and $X_0$ is the tangent space to $X$, any $v \in X$ may be written in the form $v = v_* + p$ for some $p \in X_0$. We may therefore reframe optimization problem \ref{eq:wi_optimization_problem_generic} as 
	\begin{equation}
		\label{eq:p_optimization}
		\min_{p \in X_0} J(p),
	\end{equation}
	where
	\begin{equation*}
		J(p) := \frac{1}{2}||\Delta (v_* + p)||^2.
	\end{equation*}
	Showing that $v_*$ is the unique minimizer of \eqref{eq:wi_optimization_problem_generic} is equivalent to showing that $0$ is the unique minimizer of \eqref{eq:p_optimization}. 
	
	By direct expansion, we have
	\begin{equation}
		\label{eq:Ju_expanded}
		J(p) = J(0) + \left(\Delta u, \Delta p\right)_{L^2(\Omega)} + \frac{1}{2}||\Delta p||^2.
	\end{equation}
	Since $p$ has Neumann zero boundary conditions, if $\Delta p = 0$, then $p$ is constant on each connected component of $\Omega$. Furthermore, since $p(x_i)$ is zero for at least one point $x_i$ per connected component of $\Omega$, if $\Delta p = 0$ then $p=0$. Hence, by contrapositive we have
	\begin{align*}
		\label{eq:contrapositive_laplacian_zero}
		p \neq 0 \quad \implies \quad& 0 < ||\Delta p||_{L^2(\Omega)}\\
		\implies \quad& J(0) < J(p) + \left(\Delta v_*, \Delta p\right)_{L^2(\Omega)}.
	\end{align*}
	If $v_*$ satisfies the following variational equation,
	\begin{equation}
		\label{eq:variational_up}
		(\Delta v_*, \Delta p)_{L^2(\Omega)} = 0 \quad \text{for all~} p \in X_0,
	\end{equation} 
	then
	\begin{equation*}
		p \neq 0 \quad \implies \quad J(0) < J(p).
	\end{equation*}
	Showing that $v_*$ is the unique minimizer of \eqref{eq:wi_optimization_problem_generic} therefore reduces to showing that $v_*$ satisfies variational equation \eqref{eq:variational_up}. 
	
	\paragraph{Variational equation} To show that $v_*$ satiafies variational equation \eqref{eq:variational_up}, notice that
	\begin{align*}
		(\Delta v_*, \Delta p)_{L^2(\Omega)} =& \sum_{i=1}^\convrank c_i(\Delta \secondgreens_i, \Delta p)_{L^2(\Omega)} \\
		=& \sum_{i=1}^\convrank c_i \left(p(x_i) - \avg\left(p\right)\right) \\
		=& -\avg\left(p\right) \sum_{i=1}^\convrank c_i. 
	\end{align*}
	In the first line we used the definition of $\secondgreens_i$ and the fact that the Laplacian of a constant is zero. In the second line we used Lemma \ref{lem:psi_delta_eval}. In the third line we used the fact that $p(x_i)=0$ because $p \in X_0$. Hence variational equation \eqref{eq:variational_up} holds if $\sum_{i=1}^\convrank c_i = 0$, which may be written in matrix form as $\mathbf{1}^T \mathbf{c} = 0$. This must hold because it is the second block row in system \eqref{eq:interpolation_linear_system}.
\end{proof}


\begin{thm}[Poisson weighting functions are optimal]
	\label{thm:smoothest_interpolant}
	Let $y \in \mathbb{R}^\gdim$, $\gdim \in \{1,2,3\}$, and let $\horizinterpolant_y$ be the function
	\begin{equation}
		\label{eq:u_defn_in_thm}
		\horizinterpolant_y(x) := \sum_{i=1}^\convrank \convkernel_i(y)w_i(x) = \AkerPc\left(y+x,x\right),
	\end{equation}
	where the weighting functions $w_i$ are the solutions to optimization problem \eqref{eq:wi_optimization_problem}. Then for each $y \in \Omega$, the function $\horizinterpolant_y$ solves the optimization problem
	\begin{equation}
		\label{eq:uy_optimization_problem}
		\begin{aligned}
			\min_{\horizinterpolant_y \in H^2_N(\Omega)} &\quad \frac{1}{2}\norm{\Delta \horizinterpolant_y}_{L^2(\Omega)}^2 \\
			\text{such that} &\quad \horizinterpolant_y(x_i) = \convkernel_i(y), \quad i=1,\dots,q.
		\end{aligned}
	\end{equation}
\end{thm}

\begin{prop}[Poisson weighting functions sum to one]
	\label{thm:wi_sum_to_one}
	We have
	\begin{equation*}
		\sum_{i=1}^\convrank w_i(x) = 1
	\end{equation*}
	for all $x \in \Omega$.
\end{prop}

\begin{algorithm2e}
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	{	
		\Input{Sample points $\{x_i\}_{i=1}^\convrank$, domain geometry $\Omega$}
		\Output{Weighting functions $\{w_i\}_{i=1}^\convrank$}

		\For{$k=1,2, \dots, \convrank$}{
			
			Solve the Neumann Poisson problem \eqref{eq:psi_eq} for $\firstgreens_k$.

			Solve the Neumann Poisson problem \eqref{eq:theta_eq} for $\secondgreens_k$.
			
		}
		
		Form the $\convrank \times \convrank$ matrix $\pointinteractionmatrix$ given in \eqref{eq:S_matrix}.
		
		\For{$i=1,2,\dots,\convrank$}{
			Solve the linear system
			\begin{equation*}
			\begin{bmatrix}
			\pointinteractionmatrix & \mathbf{1} \\ \mathbf{1}^T & 0
			\end{bmatrix}
			\begin{bmatrix}
			\mathbf{c}^{(i)} \\ \beta^{(i)}
			\end{bmatrix}
			=
			\begin{bmatrix}
			\mathbf{e}^{(i)} \\ 0
			\end{bmatrix}.
			\end{equation*}
			for $\mathbf{c}^{(i)}=(c_1^{(i)}, c_2^{(i)}, \dots, c_\convrank^{(i)})$ and $\beta^{(i)}$. Here $\mathbf{1} = (1,1,\dots,1) \in \mathbb{R}^\convrank$, and $\mathbf{e}^{(i)} = (0,\dots,0,1,0,\dots,0)\in \mathbb{R}^\convrank$ is the unit vector with $i^\text{th}$ component equal to one and all other components equal to zero.
			
			Form the weighting function $w_i$ given by
			\begin{equation*}
			w_i(x) = \beta^{(i)} + \sum_{k=1}^\convrank c_k^{(i)} \secondgreens_k(x).
			\end{equation*}
			
		}
	}
	\caption{Compute weighting functions $w_i$, $i=1,\dots,\convrank$.}
	\label{alg:weighting_functions_incremental}
\end{algorithm2e}


\subsection{Discretization and hierarchical matrix construction}
\label{sec:H_matrix_conversion}

Here we discuss how we construct an H-matrix representation of our product-convolution approximation, after discretizing the problem with finite elements. 


\subsubsection{Finite element discretization of the integral kernel}

Let $\febasis_1, \febasis_2, \dots, \febasis_\fedim$ be a set of finite element basis functions used to discretize the problem on a mesh with mesh size parameter $h$, and let $V_h := \Span\left(\febasis_1, \febasis_2, \dots, \febasis_\fedim\right)$ be the corresponding finite element space. Further, let $p_i \in \mathbb{R}^\gdim$, $i=1,\dots, \fedim$ be the Lagrange nodes associated with the functions $\febasis_i$. That is, the points such that $\febasis_i(p_j)$ equals one if $i=j$, and zero otherwise.

In Section \ref{sec:intro} we noted that the original integral kernel, $\Aker$, is inaccessible, while in Equation \eqref{eq:kernel_entries}  we saw that the integral kernel for our product-convolution approximation, $\AkerPc \approx \Aker$, is accessible, and may be cheaply evaluated at arbitrary pairs of points $(y,x) \in \Omega \times \Omega$ via the following formula:
\begin{equation}
	\label{eq:eval_y_x}
	\AkerPc(y,x) = \sum_{i=1}^\convrank w_i(x) \convkernel_{x_i}(y-x).
\end{equation}
We now define a further approximation, $\AkerPcMesh \approx \AkerPc$, as follows:
\begin{equation}
	\label{eq:defn_of_Akerpcmesh}
	\AkerPcMesh(y,x) := \sum_{i=1}^\fedim \sum_{j=1}^\fedim \AkerPc(p_i,p_j) \febasis_i(y) \febasis_j(x).
\end{equation}
The kernel $\AkerPcMesh$ is the interpolation of $\AkerPc$ onto $V_h \otimes V_h$. Error in the overall kernel approximation, $\AkerPcMesh \approx \Aker$, arises both because of error in the product-convolution approximation, and because of finite element discretization error incurred from interpolation onto $V_h \otimes V_h$. 

Replacing $\Aker$ with $\AkerPcMesh$ within the definition of $\Aop$ in \eqref{eq:kernel_representation} yields the following operator approximation, $\AopPcMesh$:
\begin{equation}
	\label{eq:kernel_representation_pcmesh}
	(\AopPcMesh u)(v) := \int_\Omega \int_\Omega v(y) \AkerPcMesh(y,x) u(x) dx dy.
\end{equation}
While $\AopPcMesh$ is well-defined as a mapping $L^2(\Omega) \rightarrow L^2(\Omega)'$, we are interested in the restriction of $\AopPcMesh$ to functions in the finite element space. That is, $\AopPcMesh : V_h \rightarrow V_h'$. Let $u_h \in V_h$ and $v_h \in V_h$, and let $\mathbf{u}$ and $\mathbf{v}$ be the coefficient vectors for $u_h$ and $v_h$, respectively, with respect to the finite element basis $\febasis_1, \dots, \febasis_\fedim$. I.e., 
\begin{equation}
	\label{eq:fem_coeff_basis}
	u_h(x) = \sum_{i=1}^\fedim \mathbf{u}_i \febasis_i(x),
\end{equation}
and similar for $v_h$. Further, let $\mathbf{M} \in \mathbb{R}^{\fedim \times \fedim}$ be the finite element mass matrix, and let $\AkerPcMat \in \mathbb{R}^{\fedim \times \fedim}$ be following matrix of approximate kernel entries:
\begin{equation}
	\label{eq:Akerpcmat_entries}
	\left(\AkerPcMat\right)_{ij} := \AkerPcMesh(p_i, p_j).
\end{equation}
Using \eqref{eq:defn_of_Akerpcmesh}, \eqref{eq:kernel_representation_pcmesh}, \eqref{eq:fem_coeff_basis}, and \eqref{eq:Akerpcmat_entries}, the definition of the mass matrix, and performing algebraic manipulations, it is straightforward but tedious to show that
\begin{equation*}
	\left(\AopPcMesh u_h \right)(v_h) = \mathbf{v}^T \mathbf{M} \AkerPcMat \mathbf{M} \mathbf{u}.
\end{equation*}
The finite element matrix representation of $\AopPcMesh$, which we denote by $\AmatPc$, is therefore given by
\begin{equation}
	\label{eq:Amatpc_defn}
	\AmatPc := \mathbf{M} \AkerPcMat \mathbf{M}.
\end{equation}


\subsubsection{Hierarchical matrix construction}

We form an H-matrix representation of $\AmatPc$ by converting $\AkerPcMat$ and $\mathbf{M}$ to H-matrix format, then using fast H-matrix methods to multiply these matrices according to \eqref{eq:Amatpc_defn}. Once $\AmatPc$ is in H-matrix format, useful matrix operations such as matrix-vector products, matrix-matrix addition, matrix-matrix multiplication, and matrix inversion may be performed using fast scalable H-matrix methods \cite{BORMHACKBUSCHBOOK}. We use H1 matrices in our numerical results, but any of the other H-matrix formats (such as H2, HODLR, HSS, HBS, and others \cite{HMATRIX}) could be used instead. For more details on H-matrices, we recommend \cite{HMATRIXGOOD}. 

We convert $\AkerPcMat$ into H1 matrix format using the standard geometrical clustering/adaptive cross method implemented within the HLIBPro software package \cite{HLIBPRO}. Although $\AkerPcMat$ is a dense $\fedim \times \fedim$ matrix, constructing the H-matrix approximation of $\AkerPcMat$ only requires evaluation of $O(\hmatrixrank^2 \fedim \log \fedim)$ entries of $\AkerPc$, and these entries are computed via the formula in Equation \eqref{eq:eval_y_x}. Here $\hmatrixrank$ is the rank of the highest-rank block in the H-matrix. A dense representation of $\AkerPcMat$ is never formed. We describe this H-matrix construction process in detail in Appendix \ref{app:h_matrix}. We convert $\mathbf{M}$ to H-matrix format using standard H-matrix methods for sparse matrices implemented within HLIBPro, using the same recursive block partitioning structure as was used for $\AkerPcMat$. 


\subsection{Rational positive semi-definite modification}
\label{sec:make_spd}

In many problems of practical interest (e.g., Hessian approximation) $\Amat$ is symmetric positive semi-definite. However, $\AmatPc$ is generally non-symmetric and indefinite because of errors in the product-convolution approximation. 
This is undesirable. Symmetry and positive semi-definiteness are important properties which should be preserved if possible. Also, lacking these properties may prevent one from using highly effective algorithms, such as the conjugate gradient method, to perform further useful operations involving $\AmatPc$.
In this section we describe a method for modifying $\AmatPc$ to make it symmetric positive semi-definite.

First, we use fast H-matrix addition to symmetrize $\AmatPc$:
\begin{equation*}
	\AmatPcSym := \frac{1}{2}\left(\AmatPc + \AmatPc^T\right).
\end{equation*}
Then we modify $\AmatPcSym$ to make it positive semi-definite by forming a rational matrix function of the following form: 
\begin{equation}
	\label{eq:rational_matrix_function}
	\AmatPcSymPlus := c_0 \mathbf{I} + c_1 \AmatPcSym + c_2 \left(\AmatPcSym + \ratpole \mathbf{I}\right)^{-1},
\end{equation}
where $\mathbf{I}$ is the identity matrix which has the same shape as $\AmatPcSym$. Approximation \eqref{eq:rational_matrix_function} may be written as $\AmatPcSymPlus = \ratfct(\AmatPcSym)$, where $\ratfct$ is the following rational function:
\begin{equation}
	\label{eq:rational_function_scalar}
	\ratfct(\lambda) := c_0 + c_1 \lambda + \frac{c_2}{\lambda+\ratpole}.
\end{equation}
The scalars $c_0$, $c_1$, $c_2$, and $\ratpole$ are chosen so that the moderate and large positive eigenvalues of $\AmatPcSymPlus$ approximately equal the corresponding moderate and large positive eigenvalues of $\AmatPcSym$, while the negative and small positive eigenvalues of $\AmatPcSym$ are modified so that $\AmatPcSymPlus$ is positive semi-definite. Later in this section we will explain how $c_0$, $c_1$, $c_2$, and $\ratpole$ are chosen and how our choice of these scalars affects the spectral properties of $\AmatPcSymPlus$. In Figure FIG we illustrate $\ratfct(\lambda)$. Forming $\AmatPcSymPlus$ requires computing an H-matrix inverse, $\left(\AmatPcSym + \ratpole \mathbf{I}\right)^{-1}$. While explicit computation of matrix inverses should usually be avoided, here it is computationally acceptable because $\AmatPcSym + \ratpole \mathbf{I}$ is an H-matrix. Inversion of H-matrices is a routine procedure which only requires $O(\fedim \log(\fedim)^2)$ work \cite{Hmatrixinverse}.

We choose $c_0, c_1, c_2$, and $\ratpole$, to be the solution to the following system of equations:
\begin{subequations}
	\label{eq:four_eq_system}
	\begin{align}
		\ratfct(0) &= 0 \label{eq:desired_property_1} \\
		\ratfct'(0) &= 0 \label{eq:desired_property_2} \\
		\ratfct(\lambda_\text{min}) &= |\lambda_\text{min}| \label{eq:desired_property_3} \\
		\ratfct(\lambda_\text{max}) &= \lambda_\text{max}, \label{eq:desired_property_4}
	\end{align}
\end{subequations}
where $\lambda_\text{min}$ and $\lambda_\text{max}$ are the smallest and largest eigenvalues of $\AmatPcSym$, respectively. We estimate $\lambda_\text{min}$ and $\lambda_\text{max}$ using the implicitly restarted Lanczos method \cite{lanczos,scipy}. We assume that $\lambda_\text{min} < 0$; if $\lambda_\text{min} \ge 0$, then $\AmatPcSym$ is already positive semi-definite, so we do not need to modify it with a rational function. We also require $\ratpole > |\lambda_\text{min}|$, so that the pole of $\ratfct$ is not in $[\lambda_\text{min},\lambda_\text{max}]$. We describe how we solve system \eqref{eq:four_eq_system} in Appendix \ref{app:solve_rational_system}. Solving \eqref{eq:four_eq_system} reduces to solving a smooth one-dimensional optimization problem, which is computationally trivial. 

Together, equations \eqref{eq:desired_property_1}, \eqref{eq:desired_property_2}, \eqref{eq:desired_property_3},  and \eqref{eq:desired_property_4}, and the condition $\ratpole > |\lambda_\text{min}|$, imply that $\ratfct(0)=0$ is the unique minimum of $\ratfct$ on $[\lambda_\text{min},\lambda_\text{max}]$, which implies that $\ratfct$ is non-negative on $[\lambda_\text{min},\lambda_\text{max}]$, which implies that  $\AmatPcSymPlus$ is positive semi-definite. Furthermore, \eqref{eq:desired_property_1} implies that the null space of $\AmatPcSymPlus$ contains the null-space of $\AmatPcSym$. This is important when the operator $\Aop$ has a large (or infinite) dimensional null space, or a spectrum that clusters at zero. Hessians in ill-posed distributed parameter inverse problems typically have both of these properties. Equation \eqref{eq:desired_property_4} forces the moderate and large positive eigenvalues of $\AmatPcSymPlus$ to be close to the corresponding moderate and large positive eigenvalues of $\AmatPcSym$. I.e., $\ratfct$ modifies the ``important'' part of the spectrum of $\AmatPcSym$ as little as possible. Equation \eqref{eq:desired_property_3} ensures that the negative eigenvalues of $\AmatPcSym$, which result from error in the product-convolution approximation, do not get amplified in magnitude by the function $\ratfct$. There is a tradeoff: if we choose a larger value for $\ratfct(\lambda_\text{min})$, then the positive eigenvalues of $\AmatPcSymPlus$ will be closer to the corresponding positive eigenvalues of $\AmatPcSym$ (good), but the erroneous negative eigenvalues of $\AmatPcSym$ will be amplified in magnitude (bad). This is shown in Figure FIG. We find that $\ratfct(\lambda_\text{min})=|\lambda_\text{min}|$ is an appropriate balance between these competing interests.


\subsection{Computational cost}

\begin{itemize}
	\item computational complexity in terms of PDE solves:
	\item cost of global low rank approximation (data scalability)
	\item cost to build approximation
	\item cost without compression to do a solve
	\item cost with compression to do a solve
	\item computational complexity for H-matrix alone vs. prod-conv + h-matrix $O(C k^2 \log \fedim)$ PDE solves, C is big, k is big. Here $10$
	\item HODLR: C is smaller, but k is bigger, vs. H1 peeling process
	\item H-matrix operations cost for our method solve
	\item irregular grid
\end{itemize}


\section{Numerical results}
\label{sec:numerical_results}

\textbf{Heat equation}
\begin{itemize}
	\item Hessian per-column error plots for 1, 5, and 15 batches
	\item $H-P$ error vs number of batches for interior and whole domain
	\item $P^{-1}H-I$: error vs rank for $P=R$ and $P=P_\text{pch}$ diffusion time, for 1, 5, and 15 batches
	\item Krylov iterations to tols 1e-1 and 1e-6 vs diffusion parameter
	\item GLR vs PCH vs diffusion time
	\item Krylov method convergence plot: $R$ vs $P_\text{pch}$ vs no preconditioner
	\item Krylov iterations to tols 1e-6 vs mesh size $h$, PCH vs R vs no preconditioner (hold)
	\item true parameter (H)
	\item deterministic reconstruction (H)
	\item noisy measurements (1\%, 5\%, 10\% noise). $u|_\text{top}$ (H)
	\item recovered state at top (H)
	\item mesh scalability of PCH (CC) (H)
	\item put plot data into data directory
	\item save function .pvd files in paraview  directory
\end{itemize}

\begin{itemize}
	\item GLR vs PCH number of obs (Computational cost in PDE solves) (S)
	\item GLR vs PCH aspect ratio (CC) (S)
	\item turn on preconditioner after number of krylov iterations exceeds 15 in an iteration. Rebuild "as needed". (S)
\end{itemize}

Some numerical results here.


\section{Conclusions}
\label{sec:conclusions}

Some conclusions here. 

\appendix


\section{Hierarchical matrix details}
\label{app:h_matrix}

Often, large dense matrices of practical interest may be permuted, then partitioned into blocks recursively, in such a way that many off-diagonal blocks of the matrix are numerically low rank, even if the matrix is high rank. Such matrices are known as hierarchical matrices (H-matrices). Many classes of H-matrices exist (H1, H2, HSS, HBS, and more), and all of these types of H-matrices could be used in conjunction with our product-convolution approximation. Here we use classical H1 matrices. For this section, when we say H-matrix, we are referring to H1 matrices. 

While a dense $\fedim \times \fedim$ matrix traditionally requires $O(\fedim^2)$ memory to store, H-matrices may be stored using $O(\fedim \log \fedim)$ memory, by storing only the low rank factors for the low rank blocks. Recursive algorithms can take advantage of the H-matrix low rank block structure to perform matrix arithmetic fast. Conventional dense matrix algorithms for matrix inversion, matrix factorization, matrix-vector products, matrix-matrix products, and matrix-matrix addition require either $O(\fedim^2)$ or $O(\fedim^3)$ time and memory, while the aforementioned recursive algorithms can perform these matrix operations in $O(\fedim \log(\fedim)^a)$ time and memory for H-matrices. Here $a=0, 1, 2$, or $3$ depending on the operation and type of H-matrix. For more details on H-matrices, we recommend \cite{HMATRIXGOOD}. 


\subsection{H-matrix construction}

In detail, the process of constructing an H-matrix representation of $\AkerPcMat$ proceeds as follows. First, we construct hierarchical partitionings of the degrees of freedom for the columns and rows of the matrix (cluster trees, Section \ref{sec:cluster_trees}). Second, we construct a hierarchical partitioning of the blocks of the matrix, in such a way that many of the blocks in the partitioning are expected to be low rank, and the remaining high rank blocks are small (block cluster tree, Section \ref{sec:block_cluster_tree}). Finally, we form low rank approximations of the blocks of the matrix that are expected to be low rank (adaptive cross approximation, Section \ref{sec:adaptive_cross}), and fill in the remaining high rank  blocks with their numerical values. The first two steps require geometric information about the spatial locations of the degrees of freedom associated with the rows and columns of the matrix, but these steps do not depend on the particular values of matrix entries. The third step requires us to evaluate $O(\fedim \log \fedim)$ specially-chosen entries of $\AkerPcMat$, and we evaluate these entries using \eqref{eq:Akerpcmat_entries}. 


\subsubsection{Cluster trees}
\label{sec:cluster_trees}

We use recursive hyperplane splitting to hierarchically cluster the degrees of freedom associated with the columns and rows of the matrix into a \emph{column cluster tree} and a \emph{row cluster tree}, respectively. Here we describe construction of the column cluster tree; the row cluster tree is constructed similarly. 

Since we use finite elements to discretize the problem, the $i^\text{th}$ column of $\AkerPcMat$ corresponds to the Lagrange node in $\mathbb{R}^\gdim$ associated with the $i^\text{th}$ finite element basis function. The columns of the matrix thus correspond to a point cloud in $\mathbb{R}^\gdim$. We split this point cloud into two equally sized \emph{child} point clouds, using a hyperplane which is perpendicular to the coordinate axis direction in which the point cloud is widest (e.g., either the $x$, $y$, or $z$ axis in $3$D). The two child point clouds are split in the same way. This splitting process repeats until the point clouds have less than a preset number of points (we use $32$ points). This hierarchical partitioning of the point cloud into smaller and smaller point clouds corresponds to a hierarchical partitioning of the columns of the matrix into smaller and smaller \emph{clusters} of columns. This hierarchical partitioning of the columns forms a binary tree, which is called the column cluster tree. The root of the tree is the set of all columns, and the leaves of the tree are clusters of columns that are not subdivided any further. 

A depth-first search ordering of the column cluster tree leaves is then generated. When the columns of the matrix are permuted into this depth-first ordering, the columns associated with each cluster in the cluster tree are contiguous.

In the same way, the degrees of freedom associated with the rows of the matrix are hierarchically clustered into another cluster tree, and a depth-first search ordering for the rows is generated. In our examples, the degrees of freedom for the columns coincide with the degrees of freedom for the rows, so the cluster trees for the rows and columns are the same, but this is not required in general.


\subsubsection{Block cluster tree} 
\label{sec:block_cluster_tree}

We partition the matrix into a recursive hierarchy of mostly low rank blocks called the \emph{block cluster tree}. The idea is that a block of the matrix is likely to be low rank if the point cloud associated with the rows of the block is far away from the point cloud associated with the columns of the block. This is reasonable to expect here because of the locality property of $\Aop$. Indeed, locality implies that many blocks of the matrix corresponding to far away point cloud clusters will be rank zero.

After reordering the rows and columns of $\AkerPcMat$ via the depth-first ordering described above, we partition the reordered version of $\AkerPcMat$ into a tree of $2 \times 2$ block matrices recursively. We use a geometric admissibility condition (discussed below) to decide which blocks to subdivide, and use the cluster trees for the rows and columns to determine how to subdivide those blocks. For the first stage of subdivision, let $r_1$ and $r_2$ be the children row clusters for the root of the row cluster tree, let $c_1$ and $c_2$ be the children column clusters for the root of the column cluster tree. The matrix $\AkerPcMat$ is partitioned into blocks as follows:
\begin{equation*}
	\begin{bmatrix}
		\left(\AkerPcMat\right)_{11} & \left(\AkerPcMat\right)_{12} \\
		\left(\AkerPcMat\right)_{21} & \left(\AkerPcMat\right)_{22},
	\end{bmatrix}
\end{equation*}
where $\left(\AkerPcMat\right)_{11}$ denotes the block of $\AkerPcMat$ with rows $r_1$ and columns $c_1$, $\left(\AkerPcMat\right)_{12}$ denotes the block of $\AkerPcMat$ with rows $r_1$ and columns $c_2$, and so on for $\left(\AkerPcMat\right)_{21}$ and $\left(\AkerPcMat\right)_{22}$.

We now loop through the four blocks, $\left(\AkerPcMat\right)_{11}$, $\left(\AkerPcMat\right)_{12}$, $\left(\AkerPcMat\right)_{21}$, and $\left(\AkerPcMat\right)_{22}$, and decide which blocks should be subdivided further. For the purpose of explanation, consider $\left(\AkerPcMat\right)_{12}$. If
\begin{equation}
	\label{eq:weak_admissibility_cond}
	\dist\left(r_1, c_2\right) \ge \weakadmconst \min\left(\diam\left(r_1\right), \diam\left(c_2\right)\right),
\end{equation}
then we mark $\left(\AkerPcMat\right)_{12}$ as \emph{admissible} (expected to be low rank) and leave it alone. Here $\dist\left(r_1, c_2\right)$ is the Euclidean distance between the axis-aligned bounding box for the point cloud associated with the row cluster $r_1$, and the axis aligned bounding box for the point cloud associated with the column cluster $c_2$. The quantity $\diam\left(r_1\right)$ is the diameter of the axis aligned bounding box for the point cloud associated with the row cluster $r_1$, and $\diam\left(c_2\right)$ is the analogous diameter associated with the column cluster $c_2$. Here the quantity $\weakadmconst$ is a scalar constant; we use $\weakadmconst=2.0$. Basically, if the point clouds associated with $r_1$ and $c_2$ are far away from each other relative to their diameters, then we expect that the corresponding block of the matrix will be low rank. This process is repeated for the other blocks to determine which blocks are admissible and which are not. For us, the diagonal blocks $\left(\AkerPcMat\right)_{11}$ and $\left(\AkerPcMat\right)_{22}$ are not admissible because the distance between a point cloud and itself is zero. 

Next, we subdivide all blocks that are not admissible and are larger than a predetermined size (we use size $32 \times 32$), using the same process that was used to subdivide $\AkerPcMat$. But now we subdivide a block based on the two child row clusters and two child column clusters for the rows and columns of that block. This subdivision process continues recursively until all blocks are either admissible, or smaller than the predetermined size mentioned above. The resulting hierarchical partitioning of matrix blocks forms a tree, which is called the \emph{block cluster tree}. The root of the tree is the whole matrix, internal nodes in the tree are blocks that are subdivided, and the leaves of the tree are blocks that are either expected to be low rank, or are small.


\subsubsection{Adaptive cross low rank approximation of blocks}
\label{sec:adaptive_cross}

Once the block cluster tree has been constructed, low rank approximations of the admissible (low rank) blocks are formed using the adaptive cross method \cite{ACA}. Let $X \in \mathbb{R}^{m \times m}$ be an admissible block of $\AkerPcMat$. The idea of the adaptive cross method is to form a low rank approximation of $X$ by sampling a small number of rows and columns of $X$. 
\begin{itemize}
	\item Let $C \in \mathbb{R}^{m \times \hmatrixrank}$ be a matrix consisting of a subset of $\hmatrixrank$ columns of $X$, such that the span of the columns in $C$ approximates the column space of $X$. 
	\item Let $R\in\mathbb{R}^{\hmatrixrank \times m}$ be a subset of the rows of $X$, such that the span of the rows in $R$ approximates the row space of $X$.
	\item Let $U \in \mathbb{R}^{\hmatrixrank \times \hmatrixrank}$ be the block of $X$ consisting of the intersection of the rows from $R$ with the columns from $C$.
\end{itemize}
Then it is well-established that
\begin{equation}
	\label{eq:CUR}
	X \approx C U^+ R,
\end{equation}
where $U^+$ is the pseudoinverse of $U$ \cite{CUR}. The quality of approximation \eqref{eq:CUR} depends on how well the columns of $C$ approximate the column space of $X$, and how well the rows of $R$ approximate the row space of $X$. 

In the adaptive cross method, ``good'' columns, $C$, and rows, $R$, are chosen via an alternating iterative process. An initial set of columns $C$ is chosen. Keeping $C$ fixed, a set of rows $R$ is chosen so that the so that determinant of the submatrix $U$ within $C$ is as large as possible. This may be done via either the maxvol procedure \cite{GOODSUBMATRIX}, or via QR factorization. Now, keeping $R$ fixed, a set of new columns $C$ is chosen so that the submatrix $U$ within $R$ is as large as possible. This process repeats a small number of times. This results in matrices $R$ and $C$ such that the error in the approximation, \eqref{eq:CUR}, is small. The dominant cost of this procedure is the cost of computing $a \hmatrixrank$ columns of $X$ and $a\hmatrixrank$ rows of $X$, where $a$ is the number of alternating iterations. There is also a small linear algebra overhead cost for the determinant maximization process that is performed at each step. For more details on adaptive cross low rank approximation, see \cite{ACA}. The key point is that the adaptive cross method allows us to form a rank-$\hmatrixrank$ approximation of an $m \times m$ block of the matrix via a process that only requires accessing $O(m\hmatrixrank)$ entries of that block.

We use the adaptive cross method to form low rank approximations for each admissible block of $\AkerPcMat$. We directly compute all entries of the small dense blocks of $\AkerPcMat$ that are not admissible. This process requires us to compute $O(\hmatrixrank^2 \fedim \log \fedim)$ entries of $\AkerPcMat$, which is relatively cheap compared to the operator actions of $\Aop$ that are required to form the product-convolution approximation.


\section{Fast ellipsoid intersection test}
\label{sec:fast_ellipsoid_intersection_test}
The procedure for choosing sample points relies on quickly determining whether two ellipsoids intersect. Let $E_p$ and $E_q$ be the ellipsoids defined as
\begin{align*}
	E_p :=& \{x : (x - \spatialmean_p)^T \spatialcov_p^{-1} (x - \spatialmean_p) \le \tau^2\} \\
	E_q :=& \{x : (x - \spatialmean_q)^T \spatialcov_q^{-1} (x - \spatialmean_q) \le \tau^2\}, \\
\end{align*}
where $\spatialmean_p, \spatialmean_q \in \mathbb{R}^\gdim$, and $\spatialcov_p, \spatialcov_q \in \mathbb{R}^{\gdim \times \gdim}$ are positive definite. Let $K$ be the following one dimensional convex function:
\begin{equation*}
	K(s) := 1 - \frac{1}{\tau^2} (\spatialmean_p - \spatialmean_q)^T \left(\frac{1}{1-s}\spatialcov_p + \frac{1}{s}\spatialcov_q\right)^{-1}(\spatialmean_p - \spatialmean_q)	
\end{equation*}
In \cite{ELLIPSOIDINTERSECT} it is shown that $E_p \cap E_q = \{\}$ if and only if $K(s) < 0$ for some $s\in (0,1)$. We check whether $E_p$ and $E_q$ intersect by minimizing $K(s)$ on $(0,1)$. If $K(s^*) <0$ at the minimizer $s^*$, then $E_p \cap E_q = \{\}$. Otherwise $E_p \cap E_q \neq \{\}$.

The function $K(s)$ may be evaluated quickly for many $s$ by pre-computing the solution to the generalized eigenvalue problem
\begin{equation*}
	\spatialcov_p P = \spatialcov_q P \Lambda,
\end{equation*}
where $P \in \mathbb{R}^{\gdim \times \gdim}$ is the matrix of generalized eigenvectors (which may be non-orthogonal), and $\Lambda = \diag(\lambda_1,\lambda_2,\dots,\lambda_\gdim)$ is the diagonal matrix of generalized eigenvalues $\lambda_i$. The matrix $P$ simultaneously diagonalizes $\spatialcov_p$ and $\spatialcov_q$, in the sense that $P^T\spatialcov_p P = \Lambda$, and $P^T\spatialcov_q P = I$, where $I$ is the $\gdim \times \gdim$ identity matrix. Using this diagonalization, and some algebraic manipulations, we may write $K(s)$ as
\begin{equation}
	\label{eq:Ks_generalized}
	K(s) = 1 - \frac{1}{\tau^2} \sum_{i=1}^\gdim \frac{s(1-s)}{1 + s (\lambda_i - 1)}v_i^2,
\end{equation}
where $v := \Phi^T\left(\spatialmean_p - \spatialmean_q\right)$. We compute the generalized eigenvalue decomposition of $\spatialcov_p$ and $\spatialcov_q$, then minimize $K(s)$ in the form \eqref{eq:Ks_generalized} on the interval $(0,1)$ using Brent's algorithm \cite{BRENT} (any fast 1 dimensional convex optimization routine may be used). The resulting algorithm for checking whether $E_p$ and $E_q$ intersect is summarized in Algorithm \ref{alg:ellipsoid_intersection_test}.

\begin{algorithm2e}
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwProg{Fn}{Function}{}{}
	\Input{Ellipsoid $E_p$ with mean $\spatialmean_p$ and covariance $\spatialcov_p/\tau^2$\\
		Ellipsoid $E_q$ with mean $\spatialmean_q$ and covariance $\spatialcov_q/\tau^2$ 
	}
	
	\Output{Boolean $\text{ellipsoids\_intersect}$ which is true if $E_p \cap E_q \neq \{\}$ and false otherwise}
	
	
	Solve generalized eigenvalue problem $\spatialcov_p \Phi = \spatialcov_q \Phi \Lambda$
		
	$v \gets \Phi^T\left(\spatialmean_p - \spatialmean_q\right)$
		
	$\displaystyle K^* \gets \min_{s \in (0,1)}~1 - \frac{1}{\tau^2} \sum_{i=1}^\gdim \frac{s(1-s)}{1 + s (\lambda_i - 1)}v_i^2$
		
	\If{$K^* < 0$}{
			
		$\text{ellipsoids\_intersect} \gets \text{False}$
			
	}
	\Else{
			
		$\text{ellipsoids\_intersect} \gets \text{True}$
			
	}
	\caption{Determining whether two ellipsoids intersect}
	\label{alg:ellipsoid_intersection_test}
\end{algorithm2e}


\section{Solving for rational function scalars}
\label{app:solve_rational_system}

Using the definition of $\ratfct$ in \eqref{eq:rational_function_scalar}, and basic calculus and linear algebra, we may write Equations \eqref{eq:desired_property_1}, \eqref{eq:desired_property_2}, and \eqref{eq:desired_property_4} in the following matrix form:
\begin{equation}
	\label{eq:rational_3_linear_system}
	\begin{bmatrix} 
		1 & 0 & 1/\ratpole \\ 0 & 1 & -1/\ratpole^2 \\ 1 & \lambda_\text{max} & 1/(\lambda_\text{max}+\ratpole) 
	\end{bmatrix}
	\begin{bmatrix} 
		c_0 \\ c_1 \\ c_2 
	\end{bmatrix}
	=
	\begin{bmatrix} 
		0 \\ 0 \\ \lambda_\text{max} 
	\end{bmatrix},
\end{equation}
and we may write Equation \eqref{eq:desired_property_3} as follows: 
\begin{equation}
	\label{eq:rational_extra_eq}
	c_0 + c_1 \lambda_\text{min} + \frac{c_2}{\lambda_\text{min}+\ratpole} - |\lambda_\text{min}| = 0.
\end{equation}
Let 
\begin{equation*}
	f(\ratpole) := c_0(\ratpole) + c_1(\ratpole) \lambda_\text{min} + \frac{c_2(\ratpole)}{\lambda_\text{min}+\ratpole} - |\lambda_\text{min}|
\end{equation*}
denote the left hand side of \eqref{eq:rational_extra_eq}, but with $c_0(\ratpole)$, $c_1(\ratpole)$, and $c_2(\ratpole)$ being the solution of \eqref{eq:rational_3_linear_system} for the given value of $\ratpole$. I.e., within $f(\ratpole)$ the quantities $c_0$, $c_1$, and $c_2$ are no longer free variables; they now depend on $\ratpole$ implicitly through the solution of a linear system. We solve the combined system, \eqref{eq:rational_3_linear_system} and $\eqref{eq:rational_extra_eq}$, by solving the one-dimensional nonlinear root finding problem
\begin{equation}
	\label{eq:g_mu_zero}
	f(\ratpole)=0
\end{equation}
with bound $\ratpole > |\lambda_\text{min}|$ and initial guess $\ratpole_0 = 2 |\lambda_\text{min}|$. Each evaluation of $g$ requires solving the $3 \times 3$ linear system \eqref{eq:rational_3_linear_system}, and once the solution $\ratpole$ is found, we solve \eqref{eq:rational_3_linear_system} one more time to get the final values of $c_0$, $c_1$, and $c_2$.


\section{Proofs}
\label{app:proofs}

\begin{lem}[Linearity of weighting function solution operator]
	\label{lem:T_is_linear}
	Let $\mathcal{T}:\mathbb{R}^\convrank \rightarrow H^2(\Omega)$ denote the solution operator for optimization problem \eqref{eq:generic_optimization_initial}	as a function of $f$. That is, 
	\begin{equation}
		\label{eq:opt_soln_operator}
		\mathcal{T}(f) := \horizinterpolant_*,
	\end{equation}
	where $\horizinterpolant_*$ is the minimizer of \eqref{eq:generic_optimization_initial}. The operator $\mathcal{T}$ is a linear operator.
\end{lem}

\begin{proof}
	asdf
\end{proof}


\begin{proof}[Proof of Theorem \ref{thm:smoothest_interpolant}]
	Let $f = (\convkernel_1(y), \convkernel_2(y), \dots, \convkernel_\convrank(y))$ and let $e_i = (0,\dots,0,1,0,\dots,0)$ be the length $\convrank$ vector with $i^\text{th}$ component equal to one and all other components equal to zero. The solution to optimization problem \eqref{eq:uy_optimization_problem} is given by
	\begin{align*}
		\mathcal{T}(f) &= \convkernel_1(y) \mathcal{T}(e_1) + \convkernel_2(y) \mathcal{T}(e_2) + \dots + \convkernel_\convrank(y) \mathcal{T}(e_\convrank) \\
		&= \convkernel_1(y) w_1 + \convkernel_2(y) w_2 + \dots + \convkernel_\convrank(y) w_\convrank \\
		&= \horizinterpolant_y.
	\end{align*}
	In the first line we used linearity of the solution map, $\mathcal{T}$ (Lemma \ref{lem:T_is_linear}), in the second line we used the fact that $w_i = \mathcal{T}(e_i)$, and in the third line we used the definition of $\horizinterpolant_y$ in \eqref{eq:u_defn_in_thm}.
\end{proof}

\begin{proof}[Proof of Proposition \ref{thm:wi_sum_to_one}]
	By linearity of $\mathcal{T}$ (Lemma \ref{lem:T_is_linear}), we have
	\begin{equation*}
		\sum_{i=1}^\convrank w_i(x) = \sum_{i=1}^\convrank \mathcal{T}(e_i)(x) = \mathcal{T}\left(\sum_{i=1}^\convrank e_i\right)(x) = \mathcal{T}(\mathbf{1})(x) = 1,
	\end{equation*}
	where $\mathbf{1} = (1,1,\dots,1)$ is the length $\convrank$ vector which has all components equal to one. The last equality ($\mathcal{T}(\mathbf{1})(x) = 1$) holds because the constant function $C \in H^2(\Omega)$, $C(x)=1$, satisfies the constraint of optimization problem \eqref{eq:generic_optimization_initial} with $f = \mathbf{1}$, and yields the minimum possible value of zero for the objective function.
\end{proof}



\begin{defn}[Moments of $\impulseresponse_x$]
	\label{defn:alpha_rho_mu_sigma}
	We define the spatially varying scaling factor, $\spatialvol$, the scaled impulse response, $\widehat{\impulseresponse}_x$, the spatially varying mean, $\spatialmean$, and the spatially varying covariance, $\spatialcov$, as follows:
	\begin{align*}
		\spatialvol:\Omega \rightarrow \mathbb{R}, &\qquad \spatialvol(x) := \int_{\Omega} \impulseresponse_x(z) dz, \\
		\widehat{\impulseresponse}_x:\Omega \rightarrow \mathbb{R}, &\qquad \widehat{\impulseresponse}_x := \impulseresponse_x \big/ \spatialvol(x), \\
		\spatialmean:\Omega \rightarrow \mathbb{R}^\gdim, &\qquad \spatialmean(x) := \int_\Omega z \widehat{\impulseresponse}_x(z) dz, \\
		\spatialcov:\Omega \rightarrow \mathbb{R}^{\gdim \times \gdim}, &\qquad\spatialcov(x) := \int_\Omega (z - \spatialmean(x))(z - \spatialmean(x))^T \widehat{\impulseresponse}_x(z) dz.
	\end{align*}
\end{defn}


\begin{proof}[Proof of Theorem \ref{thm:vol_mean_cov}]
	Let $v \in L^2$ be arbitrary. We have
	\begin{align*}
		\left(v, \spatialvol\right)_{L^2(\Omega)} =& \int_\Omega v(x) \int_{\Omega} \left(\Aop \delta_x^*\right)^*(z) dz dx\\
		=& \int_\Omega v(x) \int_{\Omega} A(z,x) dz dx \\
		=& \int_\Omega \int_\Omega v(x) A(z,x) C(z) dz dx \\
		=& \left(\Aop^T C\right)(v),
	\end{align*}
	which implies $\spatialvol = \left(\Aop^T C\right)^*$.
	
	Using similar techniques, we have
	\begin{align*}
		\left(v, \spatialvol \cdot \spatialmean^i\right)_{L^2(\Omega)} =& \int_\Omega v(x) \spatialvol(x) \int_{\Omega} z^i \left(\Aop \delta_x^*\right)^*(z)/\spatialvol(x) dz dx\\
		=& \int_\Omega \int_\Omega v(x) L^i(z) A(z,x) dz dx \\
		=& \left(\Aop^T L^i\right)(v),
	\end{align*}
	which implies $\spatialmean^i = \left(\Aop^T L^i\right)^* / \spatialvol$. 
	
	Using the identity
	\begin{equation*}
		\spatialcov(x) = \int_\Omega zz^T \widehat{\impulseresponse}_x(z) dz - \spatialmean(x) \spatialmean(x)^T,
	\end{equation*}
	we have
	\begin{align*}
		\left(v, \spatialvol \cdot \left(\spatialcov^{ij}+\spatialmean^i \cdot \spatialmean^j\right)\right)_{L^2(\Omega)} =& \int_\Omega v(x) \spatialvol(x) \int_\Omega z^i z^j \left(\Aop \delta_x^*\right)^*(z) / \spatialvol(x) dz dx \\
		=& \int_\Omega \int_\Omega v(x) Q^{ij}(z) A(z,x) dz dx \\
		=& \left(\Aop Q^{ij}\right)(v),
	\end{align*}
	which implies $\spatialcov^{ij} = \left(\Aop^T Q^{ij}\right)^* / \spatialvol - \spatialmean^i\cdot\spatialmean^j$.
\end{proof}


\section{Background: distributions}
\label{app:distributions}

Let $\overline{\Omega}$ be the closure of $\Omega$, and let $C(\overline{\Omega})$ be the space of continuous functions mapping $\overline{\Omega}\rightarrow \mathbb{R}$. The action of $\Aop$ is extended to distributions $\genericdistribution:C\left(\overline{\Omega}\right) \rightarrow \mathbb{R}$ via the formula
\begin{equation*}
	\left(\Aop \genericdistribution^*\right)(v) := \int_\Omega v(y) \genericdistribution\left(A(y, \cdot)\right) dy, 
\end{equation*}
where $A(y,\cdot)$ is the function $x \mapsto A(y,x)$. This is derived formally as follows:
\begin{align*}
	\left(\Aop \genericdistribution^*\right)(v) &= \int_\Omega \int_\Omega v(y) A(y,x) \textrm{``} \genericdistribution(x) \textrm{''} dx dy \\
	&= \int_\Omega v(y) \int_\Omega A(y,x) \textrm{``} \genericdistribution(x) \textrm{''} dx \\
	&= \int_\Omega v(y) \genericdistribution\left(A(y,\cdot)\right) dx.
\end{align*}
For example, the delta distribution $\delta_x$ is defined by $\delta_x(v) = v(x)$, and the action of $\Aop$ on $\delta_x$ is given by
\begin{equation}
	\label{eq:appendix_impulse_response}
	\left(\Aop \delta_x^*\right)(v)	= \int_\Omega v(y) A(y,x) dy.
\end{equation}
Recall that the impulse response of $\Aop$ to $\delta_x$, denoted $\impulseresponse_x$, is the Riesz representation of $\Aop \delta_x^*$. From \eqref{eq:appendix_impulse_response} and the definition of the Riesz representation, we have $\impulseresponse_x = A(~\cdot~,x)$. That is, $\impulseresponse_x$ is the function $y \mapsto A(y,x)$.


\section{Discretization}
\label{sec:discretization}

Let $\febasis_i$, $i=1,\dots,\fedim$ be a set of finite element basis functions, let $h$ denote the mesh size parameter for the finite element mesh, and let
\begin{equation*}
	V_h = \Span\left(\febasis_1, \febasis_2, \dots, \febasis_\fedim\right)
\end{equation*}
be the corresponding  finite element space with the $L^2$ inner product. Functions $u\in L^2(\Omega)$ are approximated by functions $u_h \in V_h$. In turn, functions $u_h\in V_h$ are represented in computations by length $\fedim$ arrays $\mathbf{u}$, such that the array entries of $\mathbf{u}$ are the coordinates of $u_h$ in the finite element basis. That is,
\begin{equation}
	\label{eq:finite_bijection}
	u_h(x) = \sum_{i=1}^\fedim \mathbf{u}_i \febasis_i(x).
\end{equation}

We write $\mathbf{M} \in \mathbb{R}^{\fedim \times \fedim}$ to denote the finite element \emph{mass matrix}, which has entries
\begin{equation*}
	\mathbf{M}_{ij} = \left(\febasis_i, \febasis_j\right)_{L^2(\Omega)} = \int_{\Omega} \febasis_i(x) \febasis_j(x) dx,
\end{equation*}
and we write $\mathbb{R}^\fedim_\mathbf{M}$ to denote the space $\mathbb{R}^\fedim$ with the matrix-weighted inner product
\begin{equation*}
	\left(\mathbf{u},\mathbf{v}\right)_\mathbf{M} := \mathbf{v}^T\mathbf{M} \mathbf{u}.
\end{equation*}
Direct calculation shows that $V_h$ is isometrically isomorphic $\mathbb{R}^\fedim_\mathbf{M}$. That is, equation \eqref{eq:finite_bijection} establishes a bijection between functions $u_h \in V_h$ and coefficient vectors $\mathbf{u} \in \mathbb{R}^\fedim_\mathbf{M}$, and this bijection preserves the inner product in the sense that $\left(u_h, v_h\right)_{L^2(\Omega)} = \left(\mathbf{u},\mathbf{v}\right)_\mathbf{M}$. Likewise, the dual space, $V_h'$, is isometrically isomorphic to $\mathbb{R}^\fedim_{\mathbf{M}^{-1}}$. 

Here vectors in $\mathbf{u} \in \mathbb{R}^\fedim_{\mathbf{M}}$ and $\boldsymbol{\sigma} \in \mathbb{R}^\fedim_{\mathbf{M}^{-1}}$ are viewed as column vectors, and $\mathbf{u}^T$ and $\boldsymbol{\sigma}^T$ are the row vectors that result from taking the standard matrix transposes of $\mathbf{u}$ and $\boldsymbol{\sigma}$, respectively. The discrete dual of a vector $\mathbf{v}$ with respect to the $\mathbf{M}$ inner product is written as $\mathbf{v}^*$, and $\mathbf{v}^*$ is viewed as a column vector.


\subsection{Discretized operations}
\label{app:discretized_operations}

Here we list the function space operations on $V_h$ and $V_h'$ that are required for this paper, and show how to perform the corresponding matrix and vector operations on $\mathbb{R}^\fedim_{\mathbf{M}}$ and $\mathbb{R}^\fedim_{\mathbf{M}^{-1}}$.

\begin{description}
	\item[Coefficients of a functional:] The coefficient dual vector, $\boldsymbol{\sigma} \in \mathbb{R}^\fedim_{\mathbf{M}^{-1}}$, corresponding to a linear functional $\sigma_h \in V_h'$ has components given by
	\begin{equation*}
		\boldsymbol{\sigma}_i = \sigma_h(\febasis_i), \quad i=1,\dots,\fedim.
	\end{equation*}
	\item[Action of a functional:] The action of a linear functional $\sigma_h \in V_h'$ on a function $u_h \in V_h$ may be computed as follows:
	\begin{equation*}
		\sigma_h(v_h) = \boldsymbol{\sigma}^T \mathbf{u}.
	\end{equation*}
	\item[Riesz representation:] Let $\sigma_h \in V_h'$. The Riesz representation of $\sigma_h$ is the unique  function $\sigma_h^* \in V_h$ satisfying 
	\begin{equation*} 
		\left(\sigma_h^*, v_h\right)_{L^2(\Omega)} = \sigma_h(v_h) \quad\forall v_h \in V_h.
	\end{equation*}
	The coefficient dual vector $\boldsymbol{\sigma}^*$ corresponding to $\sigma_h$ is given by
	\begin{equation*} 
		\boldsymbol{\sigma}^* = \mathbf{M}^{-1} \boldsymbol{\sigma}.
	\end{equation*}
	\item[Finite element $L^2$ projection:] Let $g \in L^2(\Omega)$. The orthogonal projection of $g$ onto $V_h$ with respect to the $L^2$ inner product is the unique function $g^\pi_h \in V_h$ satisfying
	\begin{equation*}
		\left(g^\pi_h, v_h\right)_{L^2(\Omega)} = \left(g, v_h\right)_{L^2(\Omega)} \quad\forall v_h \in V_h.
	\end{equation*}
	 Let $\left(\mathbf{g}^\pi\right)^* \in \mathbb{R}^\fedim_{\mathbf{M}^{-1}}$ be the vector with components
	\begin{equation*}
		\left(\mathbf{g}^\pi\right)^*_i = \int_\Omega g(x) \febasis_i(x) dx, \quad i=1,\dots,\fedim.
	\end{equation*}
	The coefficient vector $\mathbf{g}^\pi \in \mathbb{R}^\fedim_{\mathbf{M}}$ corresponding to $g^\pi_h$ is given by
	\begin{equation*}
		\mathbf{g}^\pi = \mathbf{M}^{-1}\left(\mathbf{g}^\pi\right)^*.
	\end{equation*}
	\item[Finite element interpolation:] Let $p_i \in \mathbb{R}^\gdim$, $i=1,\dots,\fedim$, be the Lagrange nodes associated with the finite element basis functions $\febasis_i$. The finite element interpolant of a function $g \in C\left(\overline{\Omega}\right)$ is the function $g^I \in V_h$ given by
	\begin{equation*}
		g_h^I(x) = \sum_{i=1}^\fedim g(p_i) \febasis_i(x).
	\end{equation*}
	The coefficient vector, $\mathbf{g}$, for $g_h^I$ has components 
	\begin{equation*}
	\mathbf{g}_i^I = g(p_i), \quad i=1.,\dots,\fedim.
	\end{equation*}
	\item[Projection vs. interpolation:]
	Typically, $g^\pi$ is a more accurate approximation of $g$ than $g^I$, but $g^I$ is cheaper to compute than $g^\pi$ because computing $g^\pi$ requires solving a linear system, while computing $g^I$ does not. If the finite element basis satisfies the following Kronecker property,
	\begin{equation*}
		\febasis_i(p_j) = \begin{cases}1 & i=j \\
		0 & \text{otherwise},\end{cases}
	\end{equation*}
	then $g^I(p_i) = g(p_i)$ for $i=1,\dots,\fedim$ (i.e., the interpolant equals the original function at the Lagrange nodes). Interpolation is typically sufficiently accurate for practical purposes if the Kronecker property is satisfied. Projection should be used if the Kronecker property is not satisfied.
	\item[Matrix representation of an operator:] Let $\mathcal{B}_h : V_h \rightarrow V_h'$ be a linear operator, and let $\mathbf{B} \in \mathbf{R}^{\fedim \times \fedim}$ be the matrix with entries
	\begin{equation*}
		\mathbf{B}_{ij} = \left(\mathcal{B}_h \febasis_j\right)(\febasis_i).
	\end{equation*}
	The matrix $\mathbf{B}$ maps $\mathbb{R}^\fedim_\mathbf{M} \rightarrow \mathbb{R}^\fedim_{\mathbf{M}^{-1}}$ by matrix multiplication, and
	\begin{equation*}
		\sigma_h = \mathcal{B}_h u_h \quad \Leftrightarrow \quad \boldsymbol{\sigma} = \mathbf{B} \mathbf{u}.
	\end{equation*}
	\item[Transpose:] The matrix $\mathbf{B}^T$ maps $\mathbb{R}^\fedim_\mathbf{M} \rightarrow \mathbb{R}^\fedim_{\mathbf{M}^{-1}}$ via matrix multiplication, and
	\begin{equation*}
		\sigma_h = \mathcal{B}_h^T u_h \quad \Leftrightarrow \quad \boldsymbol{\sigma} = \mathbf{B}^T \mathbf{u}.
	\end{equation*}

	\item[Distributions:] Let $\genericdistribution:C\left(\overline{\Omega}\right)\rightarrow \mathbb{R}$ be a distribution. If $\genericdistribution(\febasis_i)$ is well-defined for $i=1,\dots,\fedim$, then the restriction of $\genericdistribution$ to domain $V_h$ is a linear functional $\genericdistribution_h\in V_h'$. The functional $\genericdistribution_h$ has a coefficient dual vector $\boldsymbol{\genericdistribution} \in \mathbb{R}^\fedim_{\mathbf{M}^{-1}}$ with entries 
	\begin{equation*}
		\boldsymbol{\genericdistribution}_i=\genericdistribution(\febasis_i), \quad i=1,\dots,\fedim.
	\end{equation*} 
	For example, let $\delta_p$ be the Dirac distribution centered at a point $p\in\Omega$. If the finite element basis functions, $\febasis_i$, are continuous at $p$, then restricting the domain of $\delta_p$ to the space $V_h$ yields a linear functional in $V_h'$. The corresponding coefficient dual vector, $\boldsymbol{\delta}_p$, has entries 
	\begin{equation*}
		\left(\boldsymbol{\delta}_p\right)_i = \febasis_i(p), \quad i=1,\dots,\fedim.
	\end{equation*}
	
	\item[Action of an operator on a distribution:] Since $V_h$ is finite-dimensional, the restriction, $\genericdistribution_h \in V_h$, of a distribution, $\genericdistribution$, to the space $V_h$ has a a Riesz representation $\genericdistribution_h^* \in V_h$. Let $B_h$ be the integral kernel associated with an operator $\mathcal{B}_h:V_h \rightarrow V_h'$. For $v_h \in V_h$, we have
	\begin{align*}
		\langle \mathcal{B}_h, \genericdistribution_h \rangle(v) &= \int_\Omega v_h(y) \genericdistribution_h\left(B_h(y, \cdot)\right) dy \\
		&= \int_\Omega v_h(y) \left(\int_\Omega B_h(y,x) \genericdistribution_h^*(x) dx\right) dy \\
		&= \int_\Omega \int_\Omega v_h(y) B_h(y,x) \genericdistribution_h^*(x) dx dy = \left(\mathcal{B}_h \genericdistribution_h^*\right)(v).
	\end{align*}
	Going from the first to the second line we used the definition of the Riesz representation. The action of $\mathcal{B}_h$ on $\genericdistribution_h$ in the sense of distributions is therefore equal to the action of $\mathcal{B}_h$ on $\genericdistribution^*_h$ in the conventional sense. We have
	\begin{equation*}
		\sigma_h = \langle \mathcal{B}_h, \genericdistribution_h \rangle \quad \Leftrightarrow \quad \sigma_h = \mathcal{B}_h \genericdistribution_h^* \quad \Leftrightarrow \quad \boldsymbol{\sigma} = \mathbf{B} \mathbf{M}^{-1} \boldsymbol{\genericdistribution}.
	\end{equation*}
\end{description}


\subsection{Discretized mean and covariance estimation} 

\begin{algorithm2e}
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	{	
		\Input{Operator $\mathbf{A}$}
		\Output{$\boldsymbol{\spatialvol}\in \mathbb{R}^\fedim$, $\boldsymbol{\spatialmean}\in \mathbb{R}^{\gdim \times \fedim}$, and $\boldsymbol{\spatialcov} \in \mathbb{R}^{\gdim \times \gdim \times \fedim}$}
		
		Form vector $\mathbf{C} \in \mathbb{R}^\fedim_{\mathbf{M}}$ by either projecting or interpolating constant function $C(x)=1$ onto $V_h$
		
		$\boldsymbol{\spatialvol} = \left(\mathbf{M}^{-1}\mathbf{A}^T \mathbf{C}\right)^*$
		
		\For{$i=1,2,\dots,\gdim$}{
			Form vector $\mathbf{L}^i \in \mathbb{R}^\fedim_{\mathbf{M}}$ by either projecting or interpolating linear function $L^i(x) = x^i$ onto $V_h$
			
			$\mathbf{\spatialmean}^i = \left(\mathbf{M}^{-1}\mathbf{A}^T \mathbf{L}^i\right)^* / \boldsymbol{\spatialvol}$
		}
		\For{$i=1,2,\dots,\gdim$}{
			\For{$j=1,\dots,i$}{
				Form quadratic function $\mathbf{Q}^{ij} = x^i x^j$
				
				$\boldsymbol{\spatialcov}^{ij} = \left(\mathbf{M}^{-1}\mathbf{A}^T \mathbf{Q}^{ij}\right)^* / \boldsymbol{\spatialvol} - \boldsymbol{\spatialmean}^i\cdot \boldsymbol{\spatialmean}^j$
				
				$\boldsymbol{\spatialcov}^{ij} = \boldsymbol{\spatialcov}^{ji}$
				
			}
		}
		
	}
	\caption{Discretized version of Algorithm \ref{alg:varhpi_mean_cov}}
	\label{alg:varhpi_mean_cov_discrete}
\end{algorithm2e}

In computations, $\spatialvol$, $\spatialmean^{i}$, and $\spatialcov^{ij}$ are replaced with finite element approximations $\spatialvol_h, \spatialmean_h^i, \spatialcov_h^{ij}\in V_h$, respectively, which have coefficient vectors $\boldsymbol{\spatialvol}, \boldsymbol{\spatialmean}^{ij}, \boldsymbol{\spatialcov}^{ij}\in\mathbb{R}^\fedim_\mathbf{M}$, respectively. The functions $C$, $L^i$, and $Q^{ij}$ are replaced with either finite element projections onto $V_h$ (if the finite element basis does not satisfy the Kronecker property), or finite element interpolations onto $V_h$ (if the finite element basis satisfies the Kronecker property). In our numerical examples we use finite element basis functions that satisfy the Kronecker property, and we use finite element interpolations. The coefficient vectors corresponding to either the interpolations or projections are denoted $\mathbf{C}$, $\mathbf{L}^i$, and $\mathbf{Q}^{ij}$, respectively. The discretized versions of \eqref{eq:vol_mean_var_thm1}, \eqref{eq:vol_mean_var_thm2}, and \eqref{eq:vol_mean_var_thm3} are
\begin{align*}
	\boldsymbol{\spatialvol} &= \mathbf{M}^{-1}\mathbf{A}^T\mathbf{C} \\
	\boldsymbol{\spatialmean}^i &= \left(\mathbf{M}^{-1}\mathbf{A}^T \mathbf{L}^i\right) / \boldsymbol{\spatialvol}\\
	\boldsymbol{\spatialcov}^{ij} &= \left(\mathbf{M}^{-1}\mathbf{A}^T \mathbf{Q}^{ij}\right) / \boldsymbol{\spatialvol} - \boldsymbol{\spatialmean}^i\cdot \boldsymbol{\spatialmean}^j,
\end{align*}
respectively. Here $\mathbf{f} \cdot \mathbf{g}$ and $\mathbf{f} / \mathbf{g}$ denote the component-wise multiplication and division of vectors, respectively.


\subsection{Discretized sampling of impulse responses}
In computations, the Dirac comb coefficient dual vector for $\diraccomb_h^k$ is the vector $\boldsymbol{\diraccomb}^k \in \mathbb{R}^\fedim_{\mathbf{M}^{-1}}$ which has entries
\begin{equation*}
	\boldsymbol{\diraccomb}^k_i = \sum_{x_j \in \pointbatch_k} \febasis_i(x_j), \quad i=1,\dots,\fedim.
\end{equation*}
The Dirac comb impulse response coefficient vector for $\combresponse^h_k$ is the vector $\boldsymbol{\combresponse}_k \in \mathbb{R}^\fedim_\mathbf{M}$ given by
\begin{equation*}
	\boldsymbol{\combresponse}_k = \mathbf{M}^{-1} \mathbf{A}^T \mathbf{M}^{-1}\boldsymbol{\diraccomb}^k.
\end{equation*}


\section*{Acknowledgments}
We thank J.J. Alger, Longfei Gao, Mathew Hu, and Rami Nammour for helpful discussions.

\bibliographystyle{siamplain}
\bibliography{references}
\end{document}
